# Промпт для агента QWEN2.5: Исследование проблемы LightRAG query

## ЗАДАЧА

Исследовать документацию LightRAG и найти причину, почему метод `aquery()` возвращает пустой результат несмотря на успешно созданный граф.

---

## КОНТЕКСТ ПРОБЛЕМЫ

### 1. СТАТУС СИСТЕМЫ

✅ **Индексация завершена:** 423/427 документов (99.1%)  
✅ **Граф создан и сохранён:** 4689 узлов, 3812 связей (файл 4 MB)  
✅ **Векторные индексы существуют:**
- `vdb_entities.json` (29 MB)
- `vdb_relationships.json` (24 MB)  
- `vdb_chunks.json` (12 MB)

✅ **Модели загружены в VRAM:** qwen2.5:14b-instruct-q4_k_m + nomic-embed-text (11.5 GB)  
❌ **Поиск не работает:** `rag.aquery()` возвращает пустоту для ВСЕХ режимов (naive, local, global, hybrid)

---

### 2. КОД СЕРВЕРА (`lightrag_server.py`)

```python
# Инициализация через @asynccontextmanager lifespan
await rag.initialize_storages()  # При startup

# Запрос к графу
result = await rag.aquery(query, param=QueryParam(mode=mode))

# Проверка результата
if not result:
    return "Информация не найдена в базе знаний."
```

**Проблема:** Процесс занимает только **410 MB** памяти (граф 65 MB не загружен?)

---

### 3. КОНФИГУРАЦИЯ

- **WORKING_DIR:** `E:\AI_Librarian_Core\lightrag_cache`
- **LLM_MODEL:** `qwen2.5:14b-instruct-q4_k_m`
- **EMBEDDING_MODEL:** `nomic-embed-text`
- **Chunk size:** 700 символов
- **Ollama:** локальный Windows (порт 11434, НЕ Docker)

---

### 4. ФАЙЛЫ КЕША (все существуют и обновлены)

| Файл | Размер | Назначение |
|------|--------|------------|
| `graph_chunk_entity_relation.graphml` | 4 MB | Граф знаний |
| `kv_store_full_entities.json` | 0.26 MB | Список сущностей |
| `kv_store_full_relations.json` | 0.40 MB | Список связей |
| `vdb_entities.json` | 29 MB | Векторы сущностей |
| `vdb_relationships.json` | 24 MB | Векторы связей |
| `vdb_chunks.json` | 12 MB | Векторы чанков |

---

### 5. ТЕСТОВЫЕ ЗАПРОСЫ (все неуспешны)

| Запрос | Режим | Результат |
|--------|-------|-----------|
| "как разогнать видеокарту" | hybrid | "Информация не найдена" |
| "GPU memory clock settings" | hybrid | "Информация не найдена" |
| "RTX 5060 Ti" | naive/local/global/hybrid | "Информация не найдена" |

---

## ВОПРОСЫ ДЛЯ ИССЛЕДОВАНИЯ

### 1. Загрузка графа в память

- Загружается ли граф при вызове `initialize_storages()`?
- Нужен ли дополнительный метод `load_graph()` после `initialize_storages()`?
- Проверяется ли существование файла графа перед query?

### 2. Векторный поиск

- Правильно ли загружены векторные индексы `vdb_*.json`?
- Нужна ли явная инициализация vector store?

### 3. Параметры QueryParam

- Есть ли дополнительные параметры кроме `mode`?
- Нужно ли указывать `top_k`, `only_need_context` и др.?

### 4. Chunk size влияние

- Достаточно ли 700 символов для создания связей в графе?
- Может ли это объяснить "пустой" граф для поиска?

### 5. Примеры из документации

- Какие параметры используются в рабочих примерах?
- Как правильно вызывать `aquery()` для GraphRAG?

---

## ИСТОЧНИКИ ДЛЯ ПОИСКА

1. Официальная документация LightRAG (GitHub README, examples/)
2. Код библиотеки `lightrag-hku` (методы `LightRAG.aquery()`, `initialize_storages()`)
3. Issues на GitHub с тегами "empty result", "query not working", "graph not loaded"
4. Примеры использования LightRAG с Ollama + qwen моделями

---

## ОЖИДАЕМЫЙ РЕЗУЛЬТАТ

Подробный отчёт с:

1. **Причиной проблемы** (почему `aquery()` не видит граф)
2. **Примерами правильного кода** инициализации/запроса
3. **Конкретными изменениями** в `lightrag_server.py` для исправления
4. **Ссылками** на релевантную документацию

---

**Дата создания:** 24 ноября 2025  
**Система:** Windows 11, RTX 5060 Ti 16GB, LightRAG + Ollama  
**Цель:** Починить GraphRAG поиск без потери проиндексированных данных

=== ДОПОЛНИТЕЛЬНАЯ ТЕХНИЧЕСКАЯ ИНФОРМАЦИЯ ===

1. **Версия LightRAG:** lightrag-hku v1.4.9.8
2. **Хранилище графа:** Локальный диск E:\AI_Librarian_Core\lightrag_cache\
   - graph_chunk_entity_relation.graphml: 4.23 MB (4689 nodes, 3812 edges)
   - vdb_entities.json: 29 MB
   - vdb_relationships.json: 24 MB
   - vdb_chunks.json: 12 MB

3. **Backend:** Кастомный lightrag_server.py с FastAPI
   - Инициализация: await rag.initialize_storages() в lifespan startup
   - Процесс: PID 15408, **284 MB памяти** (КРИТИЧЕСКИ МАЛО!)

4. **Пример вызова aquery():**
   ```python
   result = await rag.aquery(query, param=QueryParam(mode=request.mode))
   if not result:
       result = "Информация не найдена в базе знаний."
   ```

5. **Тестовый запрос:**
   - Query: "MSI Afterburner" (mode: hybrid)
   - Response: "Информация не найдана в базе знаний."
   - Все режимы (naive/local/global/hybrid) возвращают пустоту

6. **КРИТИЧЕСКАЯ НАХОДКА:**
   - VRAM: 11.5 GB (модели загружены ✅)
   - Граф создан: 4.23 MB ✅
   - **НО**: Процесс использует 284 MB RAM вместо 1-2 GB
   - **ВЫВОД**: Граф НЕ загружается в память при запросах!

=== ВОПРОС К АГЕНТУ ===

Почему LightRAG создаёт граф в .graphml файле, но НЕ ЗАГРУЖАЕТ его в память процесса при вызове aquery()? 

Что нужно добавить в код lightrag_server.py, чтобы граф загружался в RAM и использовался для поиска?

