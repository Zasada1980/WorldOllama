Архитектурный проект внедрения локальной системы GraphRAG в экосистему Open WebUI на ограниченном оборудовании




1. Введение и постановка исследовательской задачи


Эволюция систем поиска и генерации ответов (Retrieval-Augmented Generation — RAG) достигла критической точки бифуркации. Традиционные методы, основанные на векторном поиске (Vector RAG), демонстрируют исключительную эффективность в задачах семантического сопоставления, однако сталкиваются с фундаментальными ограничениями при попытке интерпретации сложных структурных связей и многоходовых логических цепочек. В контексте пользовательского запроса, описывающего необходимость навигации между сущностями "Этаж 1" и "Этаж 9", проявляется классическая проблема "структурной слепоты" векторных моделей: семантически тексты описания этажей могут быть далеки друг от друга, но структурно они связаны жесткой ссылкой (гиперссылкой или указанием пути). Для решения подобных задач требуется переход к графовым методам (GraphRAG), которые моделируют данные не как набор изолированных векторов, а как сеть взаимосвязанных узлов и ребер.
Данный отчет представляет собой исчерпывающее техническое руководство и исследовательский анализ методов внедрения локального GraphRAG. Ключевым ограничением проекта является аппаратная база: одна видеокарта NVIDIA GPU с 16 ГБ видеопамяти (VRAM). Это ограничение накладывает строгие требования к архитектуре решения, исключая использование прожорливых серверных решений и требуя тщательной оптимизации процессов квантования, управления памятью и сериализации запросов. Основной фокус исследования направлен на интеграцию технологий Microsoft GraphRAG и LightRAG с локальными языковыми моделями (LLM) через Ollama и пользовательский интерфейс Open WebUI, с особым вниманием к проблеме индексации явных связей (Markdown-ссылок).


1.1 Анализ ограничений и выбор технологического стека


Работа на 16 ГБ VRAM (например, RTX 4080 или RTX 4060 Ti 16GB) создает специфический профиль производительности, который можно охарактеризовать как "бутылочное горлышко памяти".1 Стандартные реализации GraphRAG от Microsoft, разработанные для облачных сред Azure, предполагают массивную параллелизацию запросов к LLM (Map-Reduce), что на локальном оборудовании неминуемо приведет к ошибкам Out Of Memory (OOM).3
Для успешной реализации необходимо принять гибридную стратегию, сочетающую следующие компоненты:
1. Инференс-движок: Ollama. Выбор обусловлен поддержкой формата GGUF, позволяющего эффективно загружать квантованные модели (4-bit/8-bit), занимающие существенно меньше памяти, чем их FP16 аналоги.4
2. Языковые модели (LLM): Семейство Llama 3.1 8B или Mistral-Nemo 12B. Модели класса 70B исключены, так как даже в 4-битном квантовании они требуют более 35 ГБ VRAM, что превышает доступный лимит.2
3. Графовый движок (RAG):
   * Вариант А (Microsoft GraphRAG): Требует глубокой модификации конфигурационных файлов для отключения параллелизма.7
   * Вариант Б (LightRAG): Рекомендованное решение. Обладает меньшими накладными расходами, поддерживает инкрементальные обновления (что критично для локальной работы) и обеспечивает двойной уровень поиска (Low-level + High-level).9
4. Интерфейс: Open WebUI. Используется как фронтенд, подключаемый к графовому бэкенду через механизм Pipelines или API-совместимые прокси.11


1.2 Стратегическое значение обработки Markdown-связей


В запросе особо отмечена проблема связей типа "Этаж 1 -> Этаж 9". Это переводит задачу из плоскости чистого NLP (обработка естественного языка) в плоскость структурного анализа кода/документации. Стандартный GraphRAG извлекает сущности с помощью LLM, полагаясь на семантическое понимание текста. Однако LLM может пропустить техническую ссылку [Этаж 9](./floor9.md), если она не описана явным нарративом. Решение данной проблемы требует внедрения гибридного парсинга: использование регулярных выражений (Regex) для извлечения явных ребер графа из синтаксиса Markdown и их принудительная инъекция в граф знаний.13
________________


2. Теоретические основы и сравнительный анализ архитектур


Перед детальным разбором конфигурации необходимо определить, почему GraphRAG превосходит векторные методы в задачах навигации по документам, и в чем разница между подходами Microsoft и LightRAG в условиях ограниченных ресурсов.


2.1 Ограничения векторной парадигмы


Векторные базы данных (Vector DB) функционируют на принципе косинусного сходства. Документ "Инструкция по Этажу 1" превращается в вектор чисел. Запрос "Как пройти на Этаж 9" также превращается в вектор. Если в тексте "Этажа 1" нет слов, семантически близких к "пути на девятый этаж" (а есть только техническая ссылка), векторный поиск вернет документы, где упоминается "Этаж 9", но не тот документ, который ведет к нему. Связь теряется.15
GraphRAG решает это путем создания топологии. Документы или сущности становятся узлами (Nodes), а переходы — ребрами (Edges). LLM получает возможность "ходить" по графу, собирая контекст по цепочке связей, даже если начальный и конечный узлы семантически не похожи.


2.2 Сравнительная таблица архитектур для 16GB VRAM


Ниже приведен анализ применимости двух основных технологий для текущей задачи.


Характеристика
	Microsoft GraphRAG
	LightRAG (HKUDS)
	Рекомендация для 16GB VRAM
	Метод индексации
	Глобальная кластеризация (Leiden), иерархическое сжатие сообществ. Очень ресурсоемко.17
	Двухуровневый поиск (Low/High level), инкрементальная вставка.18
	LightRAG
	Потребление VRAM (Индекс)
	Высокое. Требует массивных контекстных окон для суммаризации сообществ.10
	Умеренное. Обрабатывает документы более локально.
	LightRAG
	Инкрементальное обновление
	Сложное/Отсутствует (часто требует перестройки индекса).19
	Поддерживается нативно ("Incremental updates").20
	LightRAG
	Интеграция с Ollama
	Через подмену API Base URL в settings.yaml.4
	Нативная поддержка через библиотеки Python.21
	LightRAG
	Качество глобального поиска
	Эталонное (Global Search). Идеально для вопросов "О чем весь массив данных?".7
	Высокое, использует гибридный режим (Mix Mode).22
	Microsoft GraphRAG
	Вставка явных связей (Markdown)
	Сложная, требует кастомных "Workflow" или инъекции в промпты.13
	Простая, есть API insert_custom_kg.23
	LightRAG
	Вывод: Хотя Microsoft GraphRAG является мощным инструментом, для задачи, требующей инструктивного выполнения на одной карте 16GB и специфической обработки ссылок Markdown, LightRAG представляет собой более гибкую и эффективную архитектуру.10 Тем не менее, отчет покроет настройку обоих решений.
________________


3. Оркестрация аппаратных ресурсов (16GB VRAM)


Управление памятью является критическим аспектом. На 16 ГБ VRAM невозможно одновременно держать в памяти тяжелую модель для генерации, модель для эмбеддинга и выполнять параллельную обработку графа.


3.1 Бюджетирование VRAM


Для успешного запуска необходимо распределить ресурсы следующим образом:


Компонент
	Модель / Процесс
	Занимаемая память (VRAM)
	Комментарий
	LLM (Веса)
	Llama 3.1 8B Instruct (Q4_K_M)
	~5.2 - 6.0 ГБ
	Квантование Q4 обязательно. FP16 займет ~16 ГБ и вызовет OOM.2
	Контекстное окно (KV Cache)
	8192 токенов
	~1.5 - 3.0 ГБ
	Размер кэша растет линейно с длиной контекста. Не ставьте 128k!
	Embedding Model
	Nomic-Embed-Text (v1.5)
	~0.3 - 0.7 ГБ
	Компактная модель, идеально подходит для RAG.4
	Резерв системы (OS)
	Windows/Linux Desktop
	~1.0 - 2.0 ГБ
	Зависит от окружения (мониторы, браузер).
	Буфер безопасности
	Пиковые нагрузки
	~2.0 - 4.0 ГБ
	Необходим для тензорных операций во время инференса.
	ИТОГО
	

	~10 - 15.7 ГБ
	На пределе, но работоспособно.
	

3.2 Стратегия сериализации


В облачных средах GraphRAG запускает десятки потоков индексации одновременно. На локальной машине это "смертельно" для производительности.
* Правило №1: Строгая сериализация. Параметр concurrent_requests (или аналог) должен быть установлен в значение 1.8 Это замедлит индексацию, но предотвратит крах драйвера GPU.
* Правило №2: Разделение фаз. Не запускайте индексацию (Indexing) одновременно с чатом (Inference). Это взаимоисключающие процессы для одной карты.
________________


4. Методология А: Внедрение Microsoft GraphRAG (Официальный репозиторий)


Данный раздел описывает настройку официального репозитория Microsoft для работы с Ollama.


4.1 Подготовка окружения


Необходимо использовать изолированную среду Python (Conda), чтобы избежать конфликтов версий, так как GraphRAG требователен к версиям библиотек.3


Bash




# Создание среды
conda create -n graphrag-local python=3.10
conda activate graphrag-local

# Установка GraphRAG
pip install graphrag

# Установка Ollama (если не установлен)
# Инструкции для Linux/Windows: [URL_REMOVED]



4.2 Подготовка моделей в Ollama


Запустите сервер Ollama и загрузите модели, оптимизированные для 16GB VRAM.


Bash




# Основная модель (Chat & Extraction)
ollama pull llama3.1:8b-instruct-q4_K_M

# Эмбеддинг модель (Vector Search)
ollama pull nomic-embed-text

Примечание: Использование q4_K_M (4-битное квантование) критически важно для экономии памяти. Модель nomic-embed-text выбрана из-за ее эффективности и поддержки длинного контекста (до 8k), что полезно для RAG.4


4.3 Инициализация и Конфигурация (settings.yaml)


Инициализируйте проект в рабочей директории:


Bash




python -m graphrag.index --init --root./my_rag_project

Это создаст файл settings.yaml. Его необходимо радикально изменить для локальной работы. Ниже приведена эталонная конфигурация для одной GPU 16GB.4
Файл: ./my_rag_project/settings.yaml


YAML




encoding_model: cl100k_base # Стандартный токенизатор (совместим с Llama через маппинг)
skip_workflows:

llm:
 api_key:ollama # Заглушка, Ollama не требует ключа
 type: openai_chat # GraphRAG использует клиент OpenAI, совместимый с Ollama
 model: llama3.1:8b-instruct-q4_K_M
 model_supports_json: true # Важно для извлечения сущностей в JSON
 api_base: [URL_REMOVED] # Адрес локального Ollama
 
 # ОПТИМИЗАЦИЯ ДЛЯ 16GB VRAM
 max_tokens: 4096 # Ограничение вывода. Не ставьте больше, риск OOM.
 request_timeout: 180.0
 api_type: openai
 
 # КРИТИЧЕСКИЕ НАСТРОЙКИ ПАРАЛЛЕЛИЗМА
 concurrent_requests: 1 # Только один запрос за раз!
 tokens_per_minute: 150000 # Искусственное замедление
 requests_per_minute: 10
 max_retries: 3
 
embeddings:
 async_mode: threaded # Threaded лучше для локального I/O
 llm:
   api_key: ollama
   type: openai_embedding
   model: nomic-embed-text
   api_base: [URL_REMOVED]
 
 # Настройки батчинга
 batch_size: 16 # Маленький размер пакета для экономии VRAM
 batch_max_tokens: 8191

chunks:
 size: 512 # Размер фрагмента текста. 512 оптимально для захвата контекста ссылок.
 overlap: 50
 group_by_columns: [id]

snapshots:
 graphml: true # Сохранять граф в формате GraphML для визуализации (Gephi) [25]

Анализ ключевых параметров:
* concurrent_requests: 1: Это самый важный параметр. По умолчанию GraphRAG пытается запустить 25+ потоков. На локальной карте это приведет к зависанию системы. Значение 1 выстраивает запросы в очередь.24
* model_supports_json: true: Llama 3.1 хорошо следует инструкциям JSON, что необходимо для этапа экстракции сущностей. Если поставить false, качество графа резко упадет.7


4.4 Индексация данных


Поместите ваши Markdown файлы в папку ./my_rag_project/input/. Затем запустите индексацию:


Bash




python -m graphrag.index --root./my_rag_project

Ожидаемое время: На RTX 4080 (16GB) индексация 100 файлов может занять 10-20 минут из-за последовательной обработки. На этом этапе нельзя пользоваться GPU для других задач.
________________


5. Методология Б: Внедрение LightRAG (Рекомендовано)


LightRAG (HKUDS) представляет собой более современную и легковесную альтернативу, специально разработанную для устранения недостатков Microsoft GraphRAG (дороговизна индексации, сложность обновлений). Для задачи с Markdown-связями LightRAG предпочтителен благодаря гибкому API вставки.9


5.1 Установка и Интеграция с Ollama


Установка производится через pip, с обязательным указанием поддержки API:


Bash




pip install "lightrag-hku[api]"
pip install ollama networkx # Дополнительные зависимости



5.2 Архитектура скрипта запуска (Server Deployment)


В отличие от MS GraphRAG, который работает как CLI-утилита, LightRAG лучше всего запускать как постоянный сервис Python. Ниже представлен код сервера, адаптированный для 16GB VRAM.
Файл: lightrag_server.py


Python




import os
from lightrag import LightRAG, QueryParam
from lightrag.llm.ollama import ollama_model_complete, ollama_embed

# Конфигурация путей
WORKING_DIR = "./lightrag_storage"
if not os.path.exists(WORKING_DIR):
   os.mkdir(WORKING_DIR)

# Инициализация LightRAG с настройками для 16GB VRAM
rag = LightRAG(
   working_dir=WORKING_DIR,
   
   # Настройки LLM (Ollama)
   llm_model_func=ollama_model_complete,
   llm_model_name="llama3.1:8b-instruct-q4_K_M",
   llm_model_max_async=1, # Сериализация запросов (аналог concurrent_requests: 1)
   llm_model_max_token_size=4096,
   
   # Настройки Embeddings
   embedding_func=ollama_embed,
   embedding_model_name="nomic-embed-text",
   embedding_batch_size=16, # Экономия памяти
   
   # Дополнительные параметры
   addon_params={
       "insert_batch_size": 10 # Обработка по 10 файлов за раз
   }
)

print("LightRAG инициализирован. Готов к работе.")

# Пример функции запроса (для тестирования)
def query_system(text, mode="hybrid"):
   return rag.query(text, param=QueryParam(mode=mode))

9
________________


6. Решение проблемы структурных связей (Markdown Links)


Центральная проблема запроса: "ИИ должен видеть связи между документами ('Этаж 1' связан с 'Этажом 9')". Стандартный GraphRAG извлекает сущности из текста. Ссылка [Текст](путь) часто игнорируется или воспринимается как шум.
Для решения этой задачи мы применим метод Explicit Edge Injection (Инъекция явных ребер). Мы напишем скрипт, который парсит Markdown файлы, находит ссылки, и добавляет их в граф знаний как "жесткие" связи.


6.1 Алгоритм гибридного индексирования


1. Парсинг: Скрипт сканирует директорию с .md файлами.
2. Regex-экстракция: Используя регулярные выражения, извлекает пары (Source_File, Target_File, Anchor_Text).
3. Формирование Custom KG: Создает структуру данных, понятную LightRAG.
4. Инъекция: Использует метод rag.insert_custom_kg() для добавления этих связей в граф.23


6.2 Реализация парсера (Python)


Ниже представлен готовый код модуля, который необходимо интегрировать в lightrag_server.py перед запуском индексации текста.
Файл: markdown_link_injector.py


Python




import os
import re
import networkx as nx

def extract_markdown_links(directory_path):
   """
   Сканирует папку и строит список ребер на основе Markdown-ссылок.
   Возвращает список словарей для LightRAG.
   """
   edges =
   # Regex для захвата ссылок формата(Link Path)
   # Поддерживает относительные пути и простые имена файлов
   link_pattern = re.compile(r'\[([^\]]+)\]\(([^)]+)\)')
   
   print(f"Сканирование директории: {directory_path}")
   
   for filename in os.listdir(directory_path):
       if filename.endswith(".md"):
           source_node = filename # Имя узла = имя файла (можно усложнить)
           file_path = os.path.join(directory_path, filename)
           
           with open(file_path, 'r', encoding='utf-8') as f:
               content = f.read()
               matches = link_pattern.findall(content)
               
               for anchor_text, target_path in matches:
                   # Очистка пути до имени файла (упрощенная логика)
                   target_node = os.path.basename(target_path)
                   
                   # Фильтрация внешних ссылок (http)
                   if target_node.startswith("http"):
                       continue
                       
                   # Создание явного ребра
                   edge_data = {
                       "src_id": source_node,
                       "tgt_id": target_node,
                       "description": f"Структурная навигационная связь: Из '{source_node}' можно перейти в '{target_node}'. Описание перехода: '{anchor_text}'",
                       "keywords": "navigation, link, structure, path",
                       "weight": 10.0, # Высокий вес, чтобы перебить семантическую несхожесть
                       "source_id": "markdown_structure_parser"
                   }
                   edges.append(edge_data)
                   
   print(f"Обнаружено {len(edges)} структурных связей.")
   return edges

def inject_into_lightrag(rag_instance, edges):
   """
   Вставляет подготовленные ребра в LightRAG
   """
   custom_kg = {
       "entities":, # LightRAG создаст сущности автоматически из ребер, если их нет
       "relationships": edges,
       "chunks":
   }
   
   print("Инъекция явного графа в LightRAG...")
   rag_instance.insert_custom_kg(custom_kg)
   print("Инъекция завершена.")

13
Как это работает:
Когда LLM ищет путь от "Этажа 1", она сначала обращается к графу. Благодаря весу 10.0 (стандартные веса обычно около 1.0), ребро "Markdown Link" становится приоритетным маршрутом. Описание ребра "Из Этажа 1 можно перейти в Этаж 9" дает модели четкую инструкцию, которую она включает в ответ.22
________________


7. Интеграция с Open WebUI: Pipelines и API


Финальный этап — подключение настроенного бэкенда к интерфейсу Open WebUI. Мы рассмотрим два метода: через прокси-сервер (проще) и через Pipelines (гибче).


7.1 Метод А: Использование API Wrapper (GraphRAG4OpenWebUI)


Для Microsoft GraphRAG существует готовый проект GraphRAG4OpenWebUI, который поднимает сервер, совместимый с форматом OpenAI.30
1. Установка:
Bash
git clone [URL_REMOVED]
cd GraphRAG4OpenWebUI
pip install -r requirements.txt

2. Настройка: В .env файле укажите путь к вашему проекту GraphRAG и адрес Ollama.
3. Запуск: python main.py --host 0.0.0.0 --port 8012
4. Подключение в Open WebUI:
   * Перейти в Admin Panel -> Connections.
   * Добавить новое соединение типа OpenAI.
   * URL: [URL_REMOVED]
   * API Key: EMPTY.
   * Теперь в списке моделей появится graphrag-local.


7.2 Метод Б: Open WebUI Pipelines (Для LightRAG и кастомной логики)


Механизм Pipelines позволяет писать Python-скрипты внутри Open WebUI для обработки запросов. Это идеальный вариант для интеграции LightRAG, так как позволяет напрямую вызывать наш Python-код.11
Сценарий интеграции:
   1. Установите Pipelines (Docker контейнер или локально).
   2. Создайте файл пайплайна lightrag_pipeline.py:


Python




"""
title: LightRAG Integration Pipeline
author: Local Researcher
description: Интеграция локального LightRAG с Open WebUI
"""

import requests
from pydantic import BaseModel

class Pipeline:
   class Valves(BaseModel):
       # Настройки, доступные из UI
       lightrag_server_url: str = "[URL_REMOVED]"

   def __init__(self):
       self.name = "LightRAG Search"
       self.valves = self.Valves()

   async def on_startup(self):
       print(f"Pipeline {self.name} started")

   async def pipe(self, user_message: str, model_id: str, messages: list, body: dict):
       # Отправляем запрос на наш сервер LightRAG (который мы настроили в разделе 5)
       # Предполагается, что сервер LightRAG обернут в простой REST API (FastAPI)
       
       # Индикатор работы для пользователя
       print(f"Querying LightRAG for: {user_message}")
       
       try:
           # Формируем запрос к API LightRAG
           response = requests.post(
               f"{self.valves.lightrag_server_url}/query",
               json={
                   "query": user_message,
                   "mode": "hybrid" # Используем гибридный поиск
               },
               timeout=120
           )
           response.raise_for_status()
           result = response.json()
           
           # Возвращаем ответ в чат
           return result.get("response", "Ошибка: пустой ответ от LightRAG")
           
       except Exception as e:
           return f"Error querying LightRAG: {str(e)}"

31
Этот метод дает максимальный контроль. Вы можете добавить логику переключения режимов поиска (Local/Global) прямо в чате, используя префиксы (например, /global Что такое проект Х?).
________________


8. Оптимизация производительности и устранение неполадок


При работе на одном GPU возможны задержки.


8.1 Проблема "Холодного старта"


При первом запросе Ollama загружает модель в VRAM. Это занимает 5-10 секунд.
   * Решение: В настройках Ollama установите keep_alive на длительное время (например, -1 для бесконечности), чтобы модель не выгружалась из памяти.3


8.2 Тайм-ауты в Open WebUI


GraphRAG работает медленно (15-60 секунд на сложный запрос), так как делает несколько вызовов LLM. Open WebUI может разорвать соединение по тайм-ауту.
   * Решение: В переменных окружения Open WebUI увеличьте AIOHTTP_CLIENT_TIMEOUT или аналогичный параметр до 300 (5 минут).11


8.3 Мониторинг VRAM


Используйте команду nvidia-smi -l 1 в консоли для мониторинга потребления памяти. Если потребление достигает 15.8 ГБ, процесс упадет.
   * Решение: Уменьшите num_ctx (контекстное окно) в настройках модели Ollama (Modelfile) с 8192 до 4096.
________________


9. Заключение


Реализация локального GraphRAG на одной видеокарте 16GB VRAM является сложной, но выполнимой инженерной задачей. Ключ к успеху лежит не в наращивании мощностей, а в оптимизации архитектуры:
   1. Отказ от параллелизма: Принудительная сериализация запросов (concurrent_requests: 1) спасает от переполнения памяти.
   2. Выбор LightRAG: Эта архитектура значительно эффективнее утилизирует ресурсы по сравнению с тяжеловесным решением от Microsoft, предлагая при этом сравнимое качество поиска.
   3. Гибридная индексация: Проблема навигации "Этаж 1 -> Этаж 9" решается не магией LLM, а детерминированным парсингом Markdown-ссылок и их прямой инъекцией в граф знаний.
Представленная архитектура обеспечивает полный суверенитет данных, отсутствие затрат на API и высокую точность ответов на сложные структурные вопросы, соответствующие критериям исследования.
Источники
   1. Context Kills VRAM: How to Run LLMs on consumer GPUs | by Lyx | Medium, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   2. Best model for RAG on 16 GB VRAM : r/LocalLLaMA - Reddit, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   3. Install GraphRAG Locally: vLLM & Ollama Setup Guide - Chitika, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   4. GraphRAG Local Setup via Ollama: Pitfalls Prevention Guide - Chi-Sheng Liu, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   5. GraphRAG local setup via vLLM and Ollama : A detailed integration guide. - Medium, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   6. Choosing the Right GPU for LLMs on Ollama - Database Mart, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   7. Detailed Configuration - GraphRAG - Microsoft Open Source, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   8. settings.yaml.example · Raj95/RAG-App at c3d0aba123ce067039679a9f43116e50513e643c - Hugging Face, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   9. [EMNLP2025] "LightRAG: Simple and Fast Retrieval-Augmented Generation" - GitHub, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   10. LightRAG: A Better Approach to Graph-Enhanced Retrieval-Augmented Generation | by Courtlin Holt-Nguyen | Oct, 2025 | Medium, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   11. open-webui/pipelines: Pipelines: Versatile, UI-Agnostic OpenAI-Compatible Plugin Framework - GitHub, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   12. open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...) - GitHub, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   13. I built graph enhanced RAG, and graph visualizations : r/Rag - Reddit, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   14. Extract Markdown Links with RegEx: A Beginner's Guide - YouTube, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   15. Navigating the Nuances of GraphRAG vs. RAG : r/mongodb - Reddit, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   16. Understanding GraphRAG vs. LightRAG: A Comparative Analysis for Enhanced Knowledge Retrieval - Maarga Systems, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   17. Building a Graph RAG System: A Step-by-Step Approach - MachineLearningMastery.com, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   18. LightRAG: Simple and Fast Alternative to GraphRAG for Legal Doc Analysis - Learn OpenCV, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   19. How to Handle Incremental Updates to Indexed Data? · microsoft graphrag · Discussion #511 - GitHub, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   20. RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge - arXiv, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   21. Hands-on Experience with LightRAG | by Alain Airom (Ayrom) | Oct, 2025 | Medium, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   22. Under the Covers With LightRAG: Retrieval - Graph Database & Analytics - Neo4j, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   23. LarFii/LightRAG-hku: "LightRAG: Simple and Fast Retrieval-Augmented Generation" - GitHub, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   24. Microsoft GraphRAG and Ollama: Code Your Way to Smarter Question Answering, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   25. дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   26. meowterspace42/github-ai-project-docs · Datasets at Hugging Face, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   27. How to extract markdown links with a regex? - python - Stack Overflow, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   28. AcademicRAG: Knowledge Graph Enhanced Retrieval-Augmented Generation for Academic Resource Discovery - kth .diva, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   29. GraphRAG4OpenWebUI integrates Microsoft's GraphRAG technology into Open WebUI, providing a versatile information retrieval API. It combines local, global, and web searches for advanced Q&A systems and search engines. This tool simplifies graph-based retrieval integration in open web environments. - GitHub, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   30. Integrating LangGraph Agents into Open WebUI | by Davit Martirosyan | Oct, 2025 - Medium, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]
   31. Custom RAG Filter pipeline hang forever · open-webui pipelines · Discussion #215 - GitHub, дата последнего обращения: ноября 22, 2025, [URL_REMOVED]