Анализ Технологий Прозрачного Checkpoint/Restore (C/R) для Гетерогенных GPU-Контейнеров




Часть 1. Технологический Стек Гомогенного C/R: От CRIU до CRIUgpu


Целью данного раздела является установление технического базового уровня. Анализируются технологии, которые по состоянию на 2025 год позволяют реализовать прозрачный Checkpoint/Restore (C/R) в гомогенной среде (т.е. миграцию с NVIDIA на NVIDIA или с AMD на AMD), а также их фундаментальные ограничения.


1.1. CRIU: Фундамент и его Недостаточность для GPU


CRIU (Checkpoint/Restore In Userspace) является де-факто стандартом в экосистеме Linux для C/R на уровне процессов и контейнеров.2 Эта технология глубоко интегрирована в современные инструменты оркестрации, включая Docker, Podman и Kubernetes.2
Механизм CRIU заключается в "замораживании" запущенного процесса и сохранении на диск всех ресурсов, управляемых ядром Linux. Это включает в себя анонимную память (RAM), состояние потоков, файловые дескрипторы, сетевые сокеты и другие ресурсы, о которых знает ядро.4
Ключевая проблема, однако, заключается в том, что CRIU по своей природе неспособен управлять состоянием внешних аппаратных устройств.5 Состояние графического процессора (GPU) — которое включает в себя данные в видеопамяти (VRAM), контексты CUDA, внутренние очереди вычислений и состояние планировщика — не является стандартным ресурсом, управляемым ядром Linux.4 Следовательно, стандартное применение CRIU к процессу, использующему GPU, приведет к успешному "снимку" CPU-части, но при восстановлении этот процесс "проснется" и обнаружит, что его GPU-контекст и все гигабайты данных в VRAM (например, веса модели) бесследно исчезли.6
Этот разрыв между зрелостью CRIU (проект существует много лет 2) и недавним (2024-2025 гг.) появлением C/R для GPU 6 доказывает, что прорыв стал возможен только после прямого вмешательства вендоров GPU. Реализация C/R на уровне ОС была невозможна до тех пор, пока NVIDIA и AMD не предоставили необходимые API на уровне драйверов и специализированные плагины.9 Это создает фундаментальную стратегическую зависимость от дорожных карт и доброй воли конкретных вендоров оборудования.


1.2. Решение NVIDIA: cuda-checkpoint и его Критические Ограничения


NVIDIA представила свой компонент решения в виде утилиты cuda-checkpoint, которая требует драйвера версии 550 или новее.4 Важно понимать, что cuda-checkpoint не заменяет CRIU, а интегрируется с ним.4
Процесс C/R с использованием этого стека выглядит следующим образом: cuda-checkpoint берет на себя управление исключительно GPU-специфичной частью. Он приостанавливает всю работу CUDA, ожидает завершения отправленных заданий, копирует полное содержимое VRAM и состояние CUDA-контекстов в системную (хост) память.4 Только после этого cuda-checkpoint передает управление CRIU, который безопасно "замораживает" основной CPU-процесс, теперь уже содержащий дамп VRAM в своей памяти. При восстановлении процесс обратный: CRIU восстанавливает CPU-процесс, cuda-checkpoint видит дамп GPU-состояния, выделяет ресурсы на новом GPU и копирует данные обратно в VRAM.5
Хотя это и является значительным шагом вперед, официальная документация NVIDIA 4 и углубленный анализ 4 выявляют критические для современных AI-задач ограничения:
1. Отсутствие поддержки UVM (Unified Virtual Memory).
2. Отсутствие поддержки IPC (Inter-Process Communication) memory.
3. Поддержка только архитектуры $x64$.4
4. Отсутствие поддержки миграции GPU (перенос на другой GPU).4
Эти ограничения не являются мелкими недочетами; они представляют собой абсолютные "шоустопперы" для большинства серьезных HPC и AI-задач. UVM активно используется для обучения моделей, которые не помещаются в VRAM (oversubscription). Отсутствие поддержки UVM означает, что cuda-checkpoint работает только для моделей, которые полностью находятся в VRAM. Еще более критично отсутствие поддержки IPC. IPC является технологической основой для multi-GPU обучения (например, через NVLink), позволяя GPU напрямую обмениваться данными. Отсутствие поддержки IPC означает, что cuda-checkpoint в его текущем виде пригоден только для однопроцессных, одно-GPU задач.
Это делает текущее решение NVIDIA скорее "доказательством концепции" (proof of concept), чем промышленным инструментом для SOTA AI. Заявление о том, что эти ограничения "будут устранены в будущих версиях драйверов" 4, представляет собой высокий технический риск для любой платформы, строящейся на этой технологии сегодня.


1.3. Решение AMD: Плагин amdgpu_plugin.so (ROCm)


У AMD существует аналогичный, но архитектурно иной подход для C/R в экосистеме ROCm. Он реализован в виде плагина amdgpu_plugin.so для CRIU.9 В отличие от NVIDIA, которая выпустила отдельную утилиту, AMD встроила C/R-логику непосредственно в свой драйвер ядра (AMDKFD).
Механизм AMD опирается на специфичные для KFD (Kernel Fusion Driver) вызовы $ioctl$ 12, такие как CRIU_PAUSE, CRIU_PROCESS_INFO, CRIU_DUMPER и CRIU_RESTORER. Эти вызовы предоставляют CRIU (работающему в userspace) стандартизированный API для безопасного запроса у ядра таких действий, как приостановка очередей GPU, выгрузка VRAM и сохранение полного состояния KFD.5
Сравнение подходов NVIDIA и AMD демонстрирует разную философию: AMD выбрала путь глубокой интеграции C/R в ядро, предоставив явный API через $ioctl$. NVIDIA, напротив, выпустила внешнюю утилиту. Эта фундаментальная разница в архитектуре (KFD $ioctl$ против cuda-checkpoint) создает первый и, по-видимому, непреодолимый барьер для гетерогенной миграции. Дамп состояния, созданный cuda-checkpoint, описывает внутреннее состояние железа NVIDIA и контекстов CUDA; дамп, созданный amdgpu_plugin.so, описывает состояние железа AMD и контекстов KFD. Эти дампы бинарно и семантически несовместимы.


1.4. SOTA (2025): CRIUgpu — Связующее Звено для Прозрачного C/R


Проект CRIUgpu, представленный в феврале 2025 года (arXiv:2502.16631), является ключевым элементом, связывающим все воедино.5 Важно понимать, что CRIUgpu — это не третья конкурирующая технология, а интеграционная платформа ("SOTA-клей"), которая объединяет основной CRIU с вендор-специфичными плагинами.6
По сути, CRIUgpu — это фреймворк плагинов для CRIU.10 Он предоставляет стандартные "хуки" (hooks), которые в нужный момент вызывают либо cuda-checkpoint для NVIDIA 6, либо amdgpu_plugin.so для AMD.5 Главная ценность CRIUgpu заключается в создании единого, унифицированного снимка CPU+GPU.6 Это позволяет полностью прозрачно "заморозить" контейнер с GPU-нагрузкой, не требуя никаких изменений в коде самого приложения.
CRIUgpu устраняет главный недостаток старых C/R-решений, которые полагались на перехват API-вызовов ($LD_PRELOAD$). Такой перехват был хрупким, не работал со статически слинкованными библиотеками (стандарт для CUDA) и приводил к огромным накладным расходам.5 CRIUgpu, опираясь на нативные драйверные API, имеет нулевые накладные расходы во время штатной работы приложения.5
Таким образом, на сегодняшний день (середина 2025 г.) технология для прозрачного гомогенного C/R (NVIDIA $\rightarrow$ NVIDIA) технически осуществима. Ее имя — CRIUgpu. Однако ее практическая польза для SOTA AI-задач сильно ограничена текущими недоработками cuda-checkpoint (отсутствие поддержки UVM/IPC). Мы имеем прозрачный C/R, но пока только для простых (single-GPU, full-VRAM) задач.


Часть 2. Конкурентный Ландшафт: Деконструкция "Прозрачности" (Анализ L3)


Цель данного раздела — определить, реализует ли кто-либо из L3-конкурентов (io.net, RunPod, TensorOpera) настоящий прозрачный C/R (аналогичный CRIUgpu), или они используют иные, менее продвинутые механизмы для достижения отказоустойчивости.


2.1. Ключевая Дихотомия: Stateful Compute vs. Stateful Storage


Анализ показывает, что ни один из исследованных L3-конкурентов не реализует C/R для вычислений (Stateful Compute). То есть, они не сохраняют состояние VRAM, CUDA-контексты или стек вызовов GPU. Вместо этого их решения полностью сфокусированы на хранилище (Stateful Storage).
Это создает маркетинговую иллюзию. Конкуренты активно используют термины "fault tolerance" ("отказоустойчивость") 16 и "automated failover" ("автоматическое восстановление после сбоя") 18, создавая у пользователя впечатление, что платформа магическим образом справится со сбоем GPU-задачи.
В действительности, как показывает углубленное изучение документации, пользователь (т.е. разработчик AI/ML) должен сам написать в своем приложении код для сохранения (сериализации) и загрузки (десериализации) чекпоинтов.16 Все, что делают платформы — это автоматизируют перезапуск пода и переподключение персистентного диска к этому новому поду. Вся сложность C/R остается на пользователе. Инновация "Прозрачный C/R" полностью устраняет эту необходимость, что является колоссальным и фундаментальным конкурентным преимуществом.


2.2. Ray (используется io.net)


Платформа io.net использует Ray в качестве фреймворка для распределенных вычислений.17 Отказоустойчивость в Ray 20 основана на двух механизмах:
1. Автоматический перезапуск: Ray автоматически обнаруживает и перезапускает отказавшие воркеры (процессы или ноды).16
2. Пользовательские чекпоинты: Ray ожидает, что пользователь в своем коде (в так называемой trainable function) явно реализует логику сохранения и загрузки чекпоинтов. Для этого Ray предоставляет API, такое как tune.save_checkpoint() и tune.get_checkpoint().16
Вывод: Ray (и, следовательно, io.net) не предоставляет прозрачный C/R. Он перекладывает 100% ответственности за сохранение состояния внутри приложения на пользователя.16


2.3. RunPod


RunPod предлагает функцию "Network Volumes" (Сетевые тома).23 Это, по сути, персистентные сетевые диски, которые можно отключать и подключать к разным подам (контейнерам).24
Документация RunPod 26 кристально ясно описывает сценарий сбоя: когда вы останавливаете под (или он "крашится"), вы «освобождаете этот конкретный GPU». Ваше хранилище (volume storage) остается на месте, но GPU (и вся его VRAM) теряется безвозвратно. При попытке перезапуска пода, если тот же самый физический GPU уже занят другим пользователем, вы получаете ошибку "Zero GPU Pods".26
Вывод: RunPod — это чистый "Stateful Storage". Он не сохраняет VRAM. Пользователь должен вручную остановить старый под, запустить новый (который, возможно, окажется на другой физической машине), подключить к нему тот же "Network Volume", а затем самостоятельно (своим скриптом) загрузить веса модели и данные с этого диска обратно в VRAM. Это не C/R, это ручное восстановление.


2.4. TensorOpera (ранее FEDML)


TensorOpera (ранее FEDML) предлагает "MLOps-платформу" 27 с маркетинговыми лозунгами "zero-code" ("без кода") 29 и "automated failover" ("автоматическое восстановление").18
Анализ их подхода показывает, что это оркестрация на уровне пайплайна. Их Workflow API 33 управляет шагами "Download Model and Checkpoints" ("Скачать модель и чекпоинты") и "Execute Job" ("Выполнить Задачу"). Это означает, что TensorOpera управляет чекпоинтами модели (т.е. файлами весов на диске), а не чекпоинтами процесса выполнения. Если задача падает, оркестратор просто перезапускает ее с последнего сохраненного моделью чекпоинта.
Вывод: TensorOpera, как и Ray, не выполняет C/R состояния выполнения (VRAM, CUDA-контексты, стек вызовов). Он лишь автоматизирует MLOps-пайплайн, который зависит от того, что само приложение (модель) умеет корректно сохранять чекпоинты на диск.


2.5. Матрица Зрелости Отказоустойчивости L3-Конкурентов


Данная таблица визуально деконструирует маркетинговые заявления конкурентов и демонстрирует уникальность "Прозрачного C/R".


Платформа
	Заявленный Механизм
	Фактический Механизм
	Сохраняемое Состояние
	Уровень Прозрачности
	Ответственность Пользователя
	Ray (io.net)
	"Fault Tolerance", "Automatic recovery" 16
	Перезапуск воркеров + API для чекпоинтов
	Только Диск (пользователем)
	Нет (уровень приложения)
	Высокая: Явная реализация C/R-логики в коде.16
	RunPod
	"Network Volumes" 23
	Stateful Storage (Персистентный диск)
	Только Диск
	Нет (уровень I/O)
	Высокая: Ручной перезапуск пода, ручная загрузка данных с диска в VRAM.26
	TensorOpera
	"Automated Failover", "Zero-code LLM training" 18
	Оркестрация MLOps-пайплайна
	Только Диск (чекпоинты модели)
	Нет (уровень оркестрации)
	Средняя: Необходимо, чтобы приложение/модель поддерживало C/R-логику, которой управляет оркестратор.33
	Предлагаемая Инновация (CRIUgpu)
	"Прозрачный C/R"
	C/R на уровне ОС (Процесс + GPU-контекст)
	RAM + VRAM + Диск
	Полная (уровень ОС)
	Нулевая: Приложение не требует модификаций.5
	

Часть 3. Проблема Гетерогенности: Разрывы в поддержке и Альтернативные Пути


Целью данного раздела является оценка осуществимости гетерогенной миграции (ядра предложенной инновации), исследование "слепых пятен" в поддержке (Intel) и анализ "параллельных технологий" (vGPU).


3.1. Почему CRIUgpu не решает Задачу Гетерогенной Миграции (NVIDIA $\rightarrow$ AMD)


Технология CRIUgpu, описанная в Части 1, поддерживает и CUDA-приложения, и ROCm-приложения.5 Однако это не означает, что она может мигрировать задачи между ними.
Как было установлено в 1.2 и 1.3, CRIUgpu вызывает вендор-специфичные инструменты для создания снимка состояния GPU. cuda-checkpoint 4 создает бинарный дамп состояния аппаратуры NVIDIA и контекстов CUDA.5 Плагин AMD amdgpu_plugin.so 12 создает дамп состояния аппаратуры AMD и контекстов KFD.5
Эти дампы несовместимы на бинарном уровне. Не существует способа, которым плагин AMD amdgpu_plugin.so мог бы интерпретировать "снимок" CUDA-контекста и "развернуть" его на железе AMD. Таким образом, CRIUgpu решает задачу гомогенного C/R (NVIDIA $\rightarrow$ NVIDIA или AMD $\rightarrow$ AMD), но гетерогенная миграция с помощью этого стека технологически невозможна.


3.2. "Слепое Пятно" Экосистемы: Отсутствие Поддержки Intel XPU


При поиске решений для C/R на GPU от Intel (XPU) 34 наблюдается полное отсутствие поддержки на уровне операционной системы.
* В обсуждениях сообщества CRIU прямо говорится: "Looks like cricket is for Nvidia, CRIU already supports AMD, but my target GPU is Intel :)" ("Похоже, cricket для Nvidia, CRIU уже поддерживает AMD, но моя целевая GPU — Intel :)").37
* Ключевая академическая работа по CRIUgpu (arXiv:2502.16631) упоминает только CUDA и ROCm.5
* Все текущие усилия Intel по поддержке GPU-вычислений 38 сосредоточены на уровне приложения — фреймворке oneAPI и его интеграции, например, в torch.xpu.38
Отсутствие C/R-плагина для Intel XPU означает, что гетерогенная платформа, включающая Intel, не может использовать прозрачный C/R-механизм на базе CRIU. Intel делает ставку на абстракцию уровнем выше (oneAPI), что возвращает нас к модели "ответственности на уровне приложения", как у Ray (см. Часть 2). Это "слепое пятно" делает идею C/R-платформы (NVIDIA/AMD/Intel) нереализуемой с помощью существующего стека CRIU/CRIUgpu.


3.3. Параллельная Технология: VM Live Migration с NVIDIA vGPU


Существует совершенно иной, более зрелый подход к C/R — миграция всей виртуальной машины (VM) с использованием технологии NVIDIA vGPU (виртуальный GPU).42 Этот механизм давно и стабильно поддерживается гипервизорами, такими как VMware vSphere (через vMotion) 45 и Citrix.47
Механизм vGPU позволяет "нарезать" один физический GPU на несколько виртуальных (vGPU), которые предоставляются гостевым VM.43 Технология vGPU Live Migration 42 позволяет "бесшовно" перенести всю VM с одного физического хоста на другой без остановки, включая полное состояние VRAM, привязанное к vGPU.
Этот подход, однако, также строго гомогенный. Документация Proxmox, например, четко указывает, что миграция возможна только при условии, что на исходном и целевом хосте "идентичное оборудование и поддержка драйверов".51 Мигрировать VM с vGPU NVIDIA на хост с GPU AMD невозможно.


3.4. Устранение Противоречий: Поддержка vGPU Live Migration в KVM (Proxmox)


При исследовании поддержки vGPU Live Migration на гипервизоре KVM (основа многих Linux-облаков) были обнаружены противоречивые данные. Старая база знаний NVIDIA 52 категорически заявляет: "1 Linux VMs are not supported with any vGPU live migration features" ("Linux VM не поддерживаются никакими функциями vGPU live migration").
Однако это утверждение прямо опровергается более новыми источниками:
1. Proxmox: Proxmox VE 8.4 (релиз 2024 г.) официально "включает полную поддержку live-миграции VM, использующих mediated passthrough devices, таких как NVIDIA vGPU".51 Это подтверждается официальными представителями Proxmox на форумах.53
2. NVIDIA (Новые данные): Блог NVIDIA, анонсирующий vGPU 18.0 (март 2025 г.), объявляет о поддержке Proxmox VE.54 Официальная таблица функций vGPU 47 явно указывает на поддержку Live Migration для "Red Hat Enterprise Linux with KVM". Релиз-ноты vGPU 18.0 также упоминают "Support for... hypervisors based on Linux with KVM".55
3. Сообщество: Несмотря на официальную поддержку, пользователи на "ванильных" KVM-гипервизорах (например, Ubuntu 20.04) все еще сталкиваются с ошибками libvirt.56
Спор о KVM разрешен: поддержка vGPU Live Migration в KVM существует. Однако она является недавним добавлением (2024-2025 гг.) и, по-видимому, стабильно работает только в коммерческих KVM-дистрибутивах (RHEL, Proxmox), а не в "ванильных" сборках.
Это дает альтернативный технологический стек (VM+vGPU) для достижения гомогенного прозрачного C/R. Стратегический выбор стоит между:
* CRIUgpu: Легковесные контейнеры, но незрелые C/R-плагины с критическими ограничениями (UVM/IPC).
* vGPU Live Migration: Тяжелые VM, но зрелая, аппаратно-поддерживаемая миграция VRAM.
Ни один из этих путей не ведет к гетерогенной миграции.


Часть 4. "Святой Грааль": SOTA-Исследование Гетерогенной Миграции (HetGPU)


Целью данного раздела является анализ технологии, которая теоретически решает задачу гетерогенной миграции, найденную в Категории 4 поисковых запросов.


4.1. Идентификация SOTA: arXiv:2506.15993 ("HetGPU")


Академическая статья "HetGPU: The pursuit of making binary compatibility towards GPUs" (arXiv:2506.15993, опубликована в июне 2025 г.) является единственным найденным решением, которое напрямую адресует проблему гетерогенной миграции GPU-задач.57
Важно, что архитектура HetGPU с самого начала проектировалась для поддержки гетерогенной среды, включая NVIDIA, AMD, Intel и даже новые архитектуры, такие как Tenstorrent.57


4.2. Ключевая Архитектура: Концепция "JVM для GPU"


HetGPU предлагает радикально иной подход, чем CRIUgpu. Вместо того чтобы пытаться сохранить аппаратно-специфичное состояние (что, как мы выяснили, невозможно для гетерогенной миграции), HetGPU полностью абстрагируется от него.
Механизм HetGPU состоит из двух частей:
1. Компилятор: Компилятор HetGPU принимает исходный код (например, CUDA) и компилирует его не в бинарный код NVIDIA (PTX) или AMD (GCN), а в архитектурно-агностичное промежуточное представление (architecture-agnostic GPU intermediate representation, или $IR$).57
2. Runtime (Среда Выполнения): Этот $IR$-файл выполняется на специальном runtime-слое.57 Эта среда выполнения динамически транслирует (JIT-компилирует) $IR$ в нативный код для любого GPU, доступного на данном узле — будь то NVIDIA, AMD или Intel.60
Эта архитектура по своей сути является "Java Virtual Machine" 60 или ".NET" 57 для GPU. Приложение компилируется один раз в "байт-код" ($IR$) и может выполняться на любой машине, где установлена среда выполнения HetGPU.


4.3. "Святой Грааль": C/R на Уровне Промежуточного Представления (IR)


"Святой Грааль" — возможность гетерогенной миграции — находится именно в этой архитектуре. Статья HetGPU 57 явно описывает: "state capture/reload mechanism for live GPU migration" (механизм захвата/перезагрузки состояния для живой миграции GPU).
Этот механизм решает проблему гетерогенности следующим образом:
1. В отличие от CRIUgpu, который захватывает физическое состояние (дамп VRAM и регистров CUDA/KFD) 4, HetGPU-runtime захватывает логическое, абстрактное состояние выполнения внутри $IR$-виртуальной машины.
2. Это абстрактное состояние (например, указатели в $IR$-памяти, состояние $IR$-потоков, счетчики $IR$-инструкций) по определению не зависит от вендора.
3. Следовательно, HetGPU-runtime на узле NVIDIA может "захватить" это $IR$-состояние.
4. Этот снимок (snapshot) можно передать по сети на узел с GPU AMD.
5. HetGPU-runtime на узле AMD получает этот снимок и "перезагружает" его.57 Среда выполнения на AMD понимает это абстрактное состояние и динамически транслирует его в свое нативное аппаратное состояние AMD.
Это единственная идентифицированная архитектура, которая делает гетерогенную миграцию (NVIDIA $\rightarrow$ AMD $\rightarrow$ Intel) теоретически возможной.


4.4. Оценка Зрелости: Академический Прототип


Необходимо трезво оценивать зрелость данной технологии. Статья датирована июнем 2025 г. 57 и описывает "prototype implementation" ("прототип реализации").57
Это не готовый к промышленному внедрению продукт, а SOTA-исследование. HetGPU решает невероятно сложные R&D-задачи: "differing SIMT vs. MIMD execution... varied instruction sets, scheduling and memory model discrepancies" ("различия в выполнении SIMT против MIMD... разные наборы инструкций, расхождения в моделях планирования и памяти").57
Создание производительной и стабильной среды выполнения, которая компилирует CUDA в $IR$, а затем транслирует $IR$ в нативный код для 3+ вендоров 60, — это R&D-задача колоссальной сложности, сравнимая по масштабу с разработкой самой CUDA или Microsoft.NET.


Часть 5. Стратегический Анализ и Рекомендации


Данный раздел синтезирует все четыре категории исследований в единый набор стратегических выводов и практических рекомендаций.


5.1. Синтез Ответов на Ключевые Вопросы


1. (Категория 1) Существуют ли технологии C/R для VRAM?
   * Да. SOTA-технология (2025 г.) называется CRIUgpu.5 Она полностью прозрачна для приложений 6, но критически зависит от вендор-специфичных плагинов.10
   * Критический недостаток: Плагин NVIDIA cuda-checkpoint не поддерживает UVM и IPC 4, что делает его непригодным для SOTA AI/HPC.
2. (Категория 2) Возможна ли гетерогенная миграция (NVIDIA $\rightarrow$ AMD/Intel)?
   * Нет, не с помощью стека CRIUgpu или vGPU. Дампы аппаратного состояния несовместимы (см. 3.1).
   * Теоретически да, но это требует совершенно иной архитектуры — HetGPU 57, которая абстрагирует C/R на уровень промежуточного представления ($IR$) (см. 4.3).
   * Intel XPU — "слепое пятно", C/R-технологии на уровне ОС для них отсутствуют (см. 3.2).
3. (Категория 3) Реализовал ли это кто-то из конкурентов (io.net, RunPod)?
   * Категорически нет. Конкуренты (Ray, RunPod, TensorOpera) реализуют только Stateful Storage (хранение на диске) (см. 2.1). Они не сохраняют VRAM.
   * Они перекладывают всю ответственность за C/R-логику на приложение пользователя.16
4. (Категория 4) Найден ли "Святой Грааль"?
   * Да. Он найден в статье arXiv:2506.15993 ("HetGPU").57 Это единственный известный путь к реализации инновации, объединяющей прозрачный C/R и гетерогенность.


5.2. Стратегический Выбор: Два Пути Развития


Анализ показывает, что предложенная инновация "Прозрачный C/R" состоит из двух ортогональных частей: (1) Прозрачность и (2) Гетерогенность. Существующие технологии (CRIUgpu) решают только первую задачу, SOTA-исследования (HetGPU) — обе. Это ставит руководство перед стратегической развилкой.
Путь A: "Прагматичный" (Гомогенный C/R на базе CRIUgpu)
* Реализация: Создать платформу на базе CRIUgpu 5 для гомогенных кластеров (пулы NVIDIA-only, пулы AMD-only).
* Плюсы: Самый быстрый путь к рынку. Предлагает истинную прозрачность C/R, что уже является уникальным торговым предложением, на голову превосходящим Ray и RunPod (см. 2.5).
* Минусы:
   1. Немедленная блокировка критическими ограничениями NVIDIA (отсутствие UVM/IPC) 4, что снизит ценность для high-end клиентов.
   2. Не реализует гетерогенность (см. 3.1).
   3. Полная стратегическая зависимость от дорожной карты NVIDIA/AMD по улучшению их C/R-плагинов.
Путь B: "Стратегический" (Гетерогенный C/R на базе HetGPU)
* Реализация: Принять SOTA-статью HetGPU 57 как архитектурный R&D-план. Начать разработку собственной "JVM для GPU".
* Плюсы:
   1. Это единственный путь к реализации полной инновации (Прозрачность + Гетерогенность).
   2. Создает абсолютный технологический "ров" (moat), который конкурентам (Ray, RunPod) будет почти невозможно преодолеть.
   3. Устраняет зависимость от вендор-специфичных C/R-плагинов (C/R происходит на уровне $IR$).
* Минусы:
   1. Экстремальная сложность R&D. Это задача на много лет и десятки инженеров-компиляторщиков (см. 4.4).
   2. Риск производительности: динамическая трансляция из $IR$ в нативный код 60 может уступать "голому" CUDA.


5.3. Рекомендации


1. Немедленно Прекратить Рассмотрение L3-Конкурентов как Угрозы. Анализ (Часть 2) однозначно показывает, что их "отказоустойчивость" — это маркетинг. Они не решают проблему прозрачного C/R и не являются конкурентами для предложенной инновации. Ваша инновация, даже в гомогенном виде (Путь A), не имеет аналогов на L3-рынке.
2. Признать, что CRIUgpu — Ловушка (для Гетерогенности). CRIUgpu 5 — это полезный инструмент для гомогенных пулов, но он является стратегическим тупиком для вашей гетерогенной стратегии. Он навсегда привяжет вас к гомогенным пулам и их специфичным аппаратным ограничениям.4
3. Сформировать R&D-Команду "HetGPU". Ваша настоящая инновация — это HetGPU.57 Необходимо немедленно начать R&D-исследование этой архитектуры. Это рискованная, но единственная ставка, которая может привести к созданию по-настоящему уникальной гетерогенной облачной платформы.
4. Гибридная Стратегия (Рекомендовано):
   * Краткосрочно (6-12 мес): Реализовать Путь A (CRIUgpu) в качестве тактического продукта. Это даст немедленное преимущество над Ray/RunPod и позволит отладить C/R-процессы. Ограничения (UVM/IPC) следует принять как известный временный риск.
   * Долгосрочно (1-3 года): Параллельно финансировать R&D-команду по Пути B (HetGPU).60 Цель этой команды — заменить гомогенный бэкенд CRIUgpu на ваш собственный гетерогенный бэкенд на базе $IR$. Это позволит вам в конечном итоге реализовать "Святой Грааль" — прозрачную, живую миграцию AI-задач с NVIDIA на AMD и Intel.
Источники
1. Troubleshooting common issues with GitHub Copilot - GitHub Docs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
2. CRIU, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
3. Using CRIU for checkpointing docker containers - General, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
4. Checkpointing CUDA Applications with CRIU | NVIDIA Technical Blog, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
5. CRIUgpu: Transparent Checkpointing of GPU-Accelerated Workloads - arXiv, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
6. GPU Container Checkpoint/Restore with CRIUgpu: Zero-Downtime Live Migration for ML Workloads - DevZero, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
7. Memory Snapshots: Checkpoint/Restore for Sub-second Startup | Modal Blog, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
8. Articles - CRIU, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
9. A parallel path for GPU restore in CRIU - LWN.net, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
10. [Literature Review] CRIUgpu: Transparent Checkpointing of GPU-Accelerated Workloads, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
11. Using CUDA's checkpoint/restore API to reduce cold boot time by 12x - Reddit, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
12. Fast Checkpoint Restore for AMD GPUs with CRIU - Indico, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
13. LPC - Fast Checkpoint Restore for GPUs - Linux Plumbers, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
14. [2502.16631] CRIUgpu: Transparent Checkpointing of GPU-Accelerated Workloads - arXiv, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
15. arXiv:2502.16631v1 [cs.DC] 23 Feb 2025, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
16. Handling Failures and Node Preemption - Ray Docs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
17. Ray for Fault-Tolerant Distributed LLM Fine-Tuning - Ghost, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
18. FEDML Launch - Run Any GenAI Jobs on Globally Distributed GPU Cloud: Pre-training, Fine-tuning, Federated Learning, and Beyond - TensorOpera AI Blog, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
19. Scalable Model Deployment and Serving on TensorOpera AI, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
20. Fault tolerance — Ray 2.51.1 - Ray Docs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
21. Ray jobs on Amazon SageMaker HyperPod: scalable and resilient distributed AI, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
22. How to Enable Fault Tolerance in Ray Tune, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
23. Network volumes - Runpod Documentation, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
24. Four Reasons To Set Up A Network Volume in the Runpod Secure Cloud, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
25. Run Automatic1111 on Runpod: The Easiest Way to Use Stable Diffusion A1111 in the Cloud, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
26. Zero GPU Pods on restart - Runpod Documentation, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
27. Tutorial on TensorOpera AI Platform, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
28. Getting Started | TensorOpera® Documentation, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
29. Zero-code Serverless LLM Training on TensorOpera AI, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
30. How to use TensorOpera GenAI Studio, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
31. FEDML Nexus AI Studio: an all-new zero-code LLM builder, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
32. Fast and Scalable AI Agents with Groq LPU and FEDML Nexus AI - TensorOpera AI Blog, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
33. Workflow for Compound Training Jobs | TensorOpera® Documentation, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
34. Tasks Overview Analysis and Extended Timeline for CPU/GPU Kernel... - Intel, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
35. Schedule GPUs - Kubernetes, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
36. Intel Xpu training support · Issue #20964 · ultralytics/ultralytics - GitHub, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
37. Stateless GPU workloads · Issue #2326 · checkpoint-restore/criu - GitHub, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
38. Getting Started on Intel GPU — PyTorch 2.9 documentation, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
39. Configuring GPU Device - Intel, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
40. Introducing the Intel® Extension for PyTorch* for GPUs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
41. Releases — Intel® Extension for PyTorch* 2.8.10+xpu documentation, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
42. Live Migration for GPU-Accelerated Virtual Machines - NVIDIA, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
43. Virtual GPU Software User Guide - NVIDIA Docs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
44. Virtual GPU Software User Guide - NVIDIA Docs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
45. Using vMotion to Migrate vGPU Virtual Machines - TechDocs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
46. Using GPUs with Virtual Machines on vSphere – Part 3: Installing the NVIDIA Virtual GPU Technology - VMware Blogs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
47. vGPU Features - NVIDIA Docs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
48. Preparing for NVIDIA GRID vGPU Capabilities for Full-Clone VMs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
49. Migrating Live a vGPU-enabled VM Within the Cluster - Nutanix Support Portal, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
50. About manual live migration | Compute Engine - Google Cloud Documentation, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
51. Proxmox VE 8.4: Live Migration, vGPU, and Backup API Power-Up - Techwrix, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
52. Comprehensive Knowledge Base on vGPU Features Across Hypervisors - NVIDIA Docs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
53. vGPU Live Migration - Proxmox Support Forum, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
54. NVIDIA Virtual GPU 18.0 Enables VDI for AI on Every Virtualized Platform, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
55. Live Migration with GPU-P and Server 2025 : r/HyperV - Reddit, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
56. vGPU Live Migration on Ubuntu+KVM - General Discussion - NVIDIA Developer Forums, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
57. HetGPU: The pursuit of making binary compatibility towards GPUs - arXiv, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
58. Hetgpu: The Pursuit of Making Binary Compatibility Towards Gpus | PDF | Graphics Processing Unit | Computer Engineering - Scribd, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
59. HetGPU: The pursuit of making binary compatibility towards GPUs | AI Research Paper Details - AIModels.fyi, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
60. HetGPU: The pursuit of making binary compatibility towards GPUs - arXiv, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
61. HetGPU: The pursuit of making binary compatibility towards GPUs - ResearchGate, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]