Анализ жизнеспособности "Пути B": производительность, экономика и оркестрация миграции vGPU




1.0 Сводное заключение и стратегическая оценка "Пути B"


Представленный анализ подтверждает, что "Путь B", использующий "живую" миграцию (live migration) виртуальных машин (ВМ) с NVIDIA vGPU на платформе KVM/Proxmox, технически реализуем в 2025 году. Однако его жизнеспособность сопряжена с высокими экономическими издержками и значительными операционными рисками, обусловленными зависимостью от проприетарных технологий и неочевидной поддержкой программного обеспечения.
Стратегический вердикт заключается в том, что жизнеспособность проекта зависит не от технической возможности, а от стратегического компромисса: либо принять высокую совокупную стоимость владения (TCO), связанную с обязательным лицензированием NVIDIA, либо строить бизнес на технически работающем, но официально не поддерживаемом стеке.
Ключевые выводы данного отчета:
1. Техническая осуществимость: "Живая" миграция vGPU не является стандартной функцией KVM. Это проприетарная технология, эксклюзивно предоставляемая NVIDIA.1 Анализ рынка в 2025 году не выявил жизнеспособных, готовых к производству альтернатив с открытым исходным кодом или от конкурентов (Intel, AMD), которые бы обеспечивали именно живую миграцию на KVM.4
2. Экономический барьер: Стоимость лицензирования является подтвержденным и значительным экономическим барьером. Функция "живой" миграции для ИИ-вычислений требует подписки NVIDIA AI Enterprise (NVAIE), стоимость которой по прейскуранту составляет $4,500 за GPU в год.7 Эта стоимость может сделать бизнес-модель, основанную на гибком предоставлении ресурсов, неконкурентоспособной.
3. План для "Лаунчера": Задача по созданию оркестратора ("Лаунчера") выполнима. Обнаружен точный "проект" (blueprint) необходимого стека ПО, основанный на работе сообщества OpenStack.8 Этот стек определяет конкретные минимальные версии libvirt, QEMU и Linux kernel, необходимые для успешной миграции опосредованных устройств (mdev), которыми являются vGPU.
4. Главный риск ("Серая зона"): Обнаружено критическое противоречие в поддержке Proxmox. Хотя NVIDIA vGPU 18.0 (март 2025 г.) официально добавила поддержку Proxmox VE 9, и сотрудники Proxmox 10, и официальная wiki-документация Proxmox 11 явно заявляют, что NVIDIA AI Enterprise (NVAIE) — лицензия, необходимая для вычислений (AI/ML), — "в настоящее время НЕ поддерживается официально".
Это расхождение в поддержке создает "серую зону": NVIDIA, вероятно, добавила поддержку Proxmox для захвата рынка VDI (виртуальных рабочих столов) 3, но ее флагманский продукт для ИИ-вычислений 2 остается официально несовместимым с Proxmox. Если заказчик будет использовать Proxmox + NVAIE, это может работать 10, но в случае сбоя миграции (что, как показывает практика, вероятно 12), и NVIDIA, и Proxmox откажут в поддержке.


2.0 Анализ производительности "Незаметного перехвата": реальное время простоя (Freeze Time)


Цель "незаметного перехвата" (простой в сотнях миллисекунд) сталкивается с фундаментальными техническими проблемами при миграции больших объемов VRAM (64-128 ГБ) под высокой вычислительной нагрузкой.


2.1 Деконструкция "Времени заморозки": VRAM и Скорость "Грязных Страниц" (Dirty Page Rate)


Стандартный механизм "живой" миграции KVM (pre-copy) предназначен для миграции RAM, а не VRAM. Он работает путем многократного копирования измененных ("грязных") страниц памяти с исходного хоста на целевой. Миграция завершается ("замораживает" ВМ) только тогда, когда скорость изменения страниц (dirty rate) становится достаточно низкой, чтобы финальная копия могла быть передана быстрее, чем ВМ успеет снова изменить память.13
При высокой вычислительной нагрузке (high compute workload) эта модель не работает для GPU:
* Проблема конвергенции: ВМ с 32 ГБ RAM на Proxmox, имеющая высокую "dirty rate" (299.1 MiB/s), уже демонстрирует проблемы с конвергенцией. В этом случае KVM был вынужден автоматически увеличить целевое время простоя (downtime), что привело к реальному простою в 531 мс.14
* Масштаб VRAM: Задачи ИИ-обучения на H100 (с 80 ГБ VRAM) или A100 (80 ГБ) будут генерировать "dirty rate" VRAM, измеряемый, вероятно, в гигабайтах в секунду, а не мегабайтах.
* Аппаратные ограничения: В отличие от CPU, стандартные GPU (commodity GPU) исторически не имеют аппаратных механизмов (например, "dirty bit" в таблице страниц) для отслеживания измененных страниц VRAM.15
Без специализированного решения, которое понимает семантику VRAM, наивный подход KVM к миграции vGPU обречен на провал. Передача данных не сойдется, и "время заморозки" составит многие секунды, а не миллисекунды, что полностью исключает "незаметный перехват".


2.2 Собственный путь (NVIDIA): Оценка vGPU Live Migration на Proxmox 8.x (H100/A100)


NVIDIA решает проблему "грязных страниц" VRAM с помощью своего проприетарного драйвера (vGPU Manager) 2, который управляет выгрузкой и загрузкой состояния GPU. NVIDIA обещает "минимальное время простоя".1
Однако реальные отчеты пользователей с Proxmox 8.x показывают, что эта технология крайне хрупкая:
* Пример сбоя: Пользователь Proxmox 8.2 (современная установка) сообщил о катастрофическом сбое при попытке "живой" миграции ВМ с vGPU между двумя узлами, использующими разные версии ядра Linux (6.8 и 6.5). В результате "ВМ Debian зависли, а ВМ Windows упали".12
* Пример успеха: Другой пользователь смог заставить миграцию работать, но это потребовало поиска и включения "пропущенной опции сборки в драйвере ядра NVIDIA vfio".16
"Живая" миграция vGPU — это, по сути, "горячая" выгрузка всего состояния VRAM и регистров из одного физического GPU и загрузка в другой. Этот процесс требует, чтобы драйвер хоста NVIDIA (vGPU Manager) на исходном и целевом узлах был не просто той же версии, а абсолютно идентичной сборки, загруженной в ядро с идентичными параметрами.16 Расхождение в версиях ядра 12 привело к загрузке несовместимых модулей, что вызвало сбой GPU на целевом хосте и крах ВМ.
Таким образом, ключевым показателем является не только "время заморозки", но и вероятность успеха миграции. "Лаунчер" заказчика должен будет реализовать строгую проверку предварительных условий (идентичность версий ядра, драйверов NVIDIA и pve-manager) на обоих хостах перед запуском миграции.


2.3 Открытый путь (gMig): Анализ академического решения (119 мс) и его применимости


Существует академический проект с открытым исходным кодом под названием gMig, который был разработан для решения фундаментальной проблемы отсутствия аппаратного отслеживания "грязных страниц" VRAM.15
* Механизм: gMig реализует технику "Software Dirty Page" (Программные "грязные" страницы) с использованием хэширования. Он хэширует все страницы VRAM, передает их, затем на последующих итерациях повторно хэширует и передает только те страницы, чьи хэши изменились.15
* Производительность: Заявленное "время заморозки" (downtime) составляет в среднем 119 мс на Linux.15 Это идеально соответствует цели "незаметного перехвата".
Несмотря на впечатляющие результаты, gMig не является готовым к развертыванию решением для H100/A100:
1. Устаревание: Публикации датированы 2018-2019 гг. 15 и тестировались на GPU того времени.
2. Проблема архитектуры: Механизм gMig включает "Динамическое переотображение графических адресов", что требует парсинга и манипулирования командами GPU.15 Архитектуры команд H100 (Hopper) и A100 (Ampere) кардинально отличаются от GPU 2018 года.
3. Накладные расходы: Хэширование 80-128 ГБ VRAM для выявления "грязных" страниц само по себе является вычислительно интенсивной задачей, которая будет потреблять ресурсы CPU хоста.
gMig — это концептуальный проект. Заказчик не может "использовать gMig"; он может лишь попытаться профинансировать R&D-проект по воссозданию gMig для современных GPU, что является многолетней задачей с высоким риском.


3.0 Экономический барьер: лицензирование NVIDIA и жизнеспособные альтернативы


Стоимость лицензирования NVIDIA является основным фактором, определяющим экономическую жизнеспособность "Пути B".


3.1 Анализ "убийцы бизнес-модели": Де-рискинг лицензии NVIDIA AI Enterprise (NVAIE)


Функции "Живая миграция" (Live Migration) и Suspend/Resume являются продвинутыми возможностями управления, которые лицензируются исключительно через программный пакет NVIDIA AI Enterprise (NVAIE) (ранее известный как Virtual Compute Server или vCS).2
Так как проект заказчика сфокусирован на ИИ-вычислениях (AI/ML), а не на VDI, ему потребуется лицензия "NVIDIA vGPU for Compute", которая доступна только в составе NVAIE.2
Официальная цена (List Price) на 2025-2026 гг. за подписку NVAIE (включая поддержку) составляет $4,500 за GPU в год.7 Сторонние ритейлеры, такие как Dell, подтверждают этот порядок цен.19


3.2 Модель затрат на 2025-2026 гг. и совокупная стоимость владения (TCO)


Прямые затраты на лицензирование NVAIE являются ключевым компонентом TCO.
Таблица 1: Модель ценообразования NVIDIA AI Enterprise (NVAIE) (2025-2026 гг.)
(Источник: 7)
Тип лицензии
	Срок
	List Pricing (Прейскурант)
	EDU and Inception Pricing³
	Subscription (Includes support²)
	1 год
	$4,500 / GPU
	$1,125 / GPU
	Subscription (Includes support²)
	3 года
	$13,500 / GPU
	$3,375 / GPU
	Subscription (Includes support²)
	5 лет¹
	$18,000 / GPU
	$4,500 / GPU
	Perpetual (вкл. 5 лет поддержки²)
	-
	$22,500 / GPU
	$5,625 / GPU
	¹Включено с некоторыми GPU (H100, H200). ²Стандартная поддержка. ³Вероятно, неприменимо к бизнес-модели CSP.


3.3 Исследование безлицензионных альтернатив ("Обходных путей")


Анализ показывает, что в 2025 году не существует зрелых, безлицензионных альтернатив, отвечающих всем требованиям (KVM, Open Source, H100/A100, Live Migration).
Таблица 2: Сравнительный анализ альтернатив "живой" миграции GPU (2025-2026 гг.)


Решение
	Платформа
	Лицензия
	Поддержка H100/A100
	"Живая" миграция
	NVIDIA vGPU (NVAIE)
	KVM/Proxmox 9
	$4,500/год/GPU 7
	Да 2
	Да (проприетарная) 1
	Intel GVT-g
	KVM/Proxmox
	Нет (Open Source)
	Нет (устарело)
	Нет (устарело) 22
	Intel SR-IOV (Xe/Flex)
	KVM/Proxmox 5
	Нет (Open Source)
	Нет (только Intel Xe)
	НЕТ (критический недостаток) 5
	AMD MxGPU
	KVM/Proxmox 4
	Нет (Open Source) 23
	Нет (только Pro)
	НЕТ (только старые GPU) 4
	gMig (Open Source)
	KVM 15
	Нет (Open Source)
	Нет (требуется R&D)
	Да (119 мс, академическая) 15
	

3.3.1 Анализ альтернатив (Intel, AMD)


* Intel GVT-g: (Запрошено заказчиком) Категорическое "Нет". Технология мертва (deprecated). Официальный репозиторий Intel архивирован с "известными проблемами безопасности".22 Это нежизнеспособный путь.
* Intel SR-IOV (Преемник GVT-g): Это современное, открытое решение Intel 5, которое отлично работает на Proxmox для разделения GPU.24 Однако оно не поддерживает "живую" миграцию. Документация RedHat (2025 г.) прямо заявляет: "живая" миграция ВМ с vGPU в настоящее время "возможна только с GPU NVIDIA".6 Этот "обходной путь" — тупик.
* AMD MxGPU: Ситуация еще хуже. Официальная поддержка AMD MxGPU на Proxmox ограничена очень старыми картами 2016 года (FirePro S7150). Новые карты (PRO W7900) не поддерживают MxGPU.4


3.3.2 Анализ альтернатив (Open Source проекты)


* Project HAMi (k8s-vGPU): Этот проект 25 не является решением для KVM/Proxmox. Это планировщик для Kubernetes, который обеспечивает разделение GPU для контейнеров. Он работает путем перехвата API CUDA в user-space 28 и не виртуализирует устройство на уровне гипервизора (mdev). Он не имеет отношения к "живой" миграции ВМ.
* vgpu_unlock: Это "хак" для разблокировки vGPU на потребительских картах (RTX 30-й серии).29 Он не обеспечивает "живую" миграцию и не является решением для ЦОД.
Вывод однозначен: NVIDIA имеет эффективную монополию на функцию "живой" миграции vGPU в средах KVM/Proxmox.


3.3.3 Риск "Серой зоны": Противоречие vGPU 18.0 и NVAIE на Proxmox


Как было отмечено в разделе 1.0, это самый значительный операционный риск.
* Факт 1: NVIDIA vGPU 18.0 (март 2025 г.) официально поддерживает Proxmox VE.9
* Факт 2: Официальная Wiki Proxmox 11 и сотрудник Proxmox 10 явно заявляют, что NVIDIA AI Enterprise (NVAIE) — лицензия, необходимая для вычислений (AI/ML) — "в настоящее время НЕ поддерживается официально".
Это ставит заказчика перед выбором из трех неидеальных вариантов:
1. Вариант А (Официальный): Отказаться от Proxmox. Использовать RHEL/Ubuntu KVM или VMware, которые официально поддерживаются NVAIE.
2. Вариант Б ("Серая зона"): Использовать Proxmox + NVAIE. Это должно работать 10, но без какой-либо поддержки.
3. Вариант В (R&D): Отказаться от NVAIE и попытаться профинансировать R&D-проект по воссозданию gMig.15
Наиболее прагматичным выглядит "Вариант Б", если заказчик готов создать внутреннюю команду экспертов по KVM/NVIDIA/Proxmox для самостоятельной поддержки и устранения неполадок.


4.0 Проект "Лаунчера": оркестрация и API-управление миграцией vGPU


Для успешной работы "Лаунчера" требуется стек компонентов, способный корректно обрабатывать миграцию опосредованных устройств (mdev).


4.1 Уровень управления Proxmox: Триггеры API и автоматизация


Proxmox предоставляет API (вероятно, PVE::API2), который используется для управления кластером.12 Пользователи уже применяют его для CI/CD-тестирования миграций с помощью Ansible или Python.
Однако прямой вызов API Proxmox для миграции ВМ с vGPU (mdev) может потерпеть неудачу. Пользователи KVM/libvirt сообщают 30, что стандартная команда virsh migrate (которую, вероятно, и вызывает Proxmox) завершается с ошибкой domain has assigned non-USB host devices. Это происходит потому, что старые версии libvirt не умеют обрабатывать миграцию состояния mdev.


4.2 Низкоуровневое управление KVM и эталонная архитектура OpenStack


На самом низком уровне миграция KVM/QEMU запускается либо через монитор QEMU 31, либо через libvirt.32 vGPU подключается к QEMU через параметр -device.33
Проблема ошибки mdev 30 уже решена сообществом OpenStack. Документация OpenStack Nova (релиз 2024.1 Caracal) подтверждает, что "Nova теперь поддерживает 'живую' миграцию vGPU".8
Эта документация предоставляет точный "проект" (blueprint) стека компонентов, необходимых для того, чтобы libvirt мог обрабатывать миграцию mdev.
Таблица 3: Эталонный стек компонентов KVM для "живой" миграции vGPU (2025 г.)
(Источник: 8)


Компонент
	Минимальная требуемая версия
	Источник/Обоснование
	Linux Kernel
	5.18.0
	OpenStack Nova 8
	Libvirt
	8.6.0
	OpenStack Nova 8
	QEMU
	8.1.0
	OpenStack Nova 8
	NVIDIA Host Driver
	NVAIE-совместимый (например, 550.x)
	10
	Proxmox VE
	8.x (для поддержки vGPU 18.0)
	9
	Современные версии Proxmox 8.x (например, 8.2 с ядром 6.8 12) уже соответствуют или превышают эти требования к ядру, что делает эту архитектуру реализуемой.


4.3 Стратегия интеграции для "Лаунчера" Заказчика


"Лаунчер" не должен слепо вызывать API миграции. Он должен действовать как интеллектуальный оркестратор:
1. Шаг 1 (Валидация среды): "Лаунчер" перед миграцией должен по API опросить исходный и целевой узлы Proxmox и убедиться, что версии kernel, qemu-kvm, libvirt-daemon и nvidia-vgpu-mgr 34 абсолютно идентичны и соответствуют минимальным требованиям из Таблицы 3.
2. Шаг 2 (Запуск): "Лаунчер" должен использовать Proxmox API 12 или virsh 32 для запуска миграции.
3. Шаг 3 (Мониторинг): "Лаунчер" должен отслеживать состояние миграции (например, через info migrate в QEMU-мониторе 31).
Из существующих оркестраторов с открытым исходным кодом OpenStack 8 демонстрирует наиболее зрелую поддержку этой функции, в то время как OpenNebula, хотя и поддерживает vGPU 35, по-видимому, не имеет надежной автоматической "живой" миграции vGPU при сбое хоста.38


5.0 Итоговые выводы и стратегические рекомендации по "Пути B"


"Путь B" является узкой тропой, требующей значительных финансовых вложений в проприетарное ПО NVIDIA и глубокой внутренней технической экспертизы для управления хрупким, официально не поддерживаемым стеком.
* Рекомендация 1 (Экономика): Немедленно смоделировать совокупную стоимость владения (TCO), исходя из стоимости лицензии NVAIE в $4,500/GPU/год.7 Это "налог" NVIDIA на функцию "живой" миграции. Если эта цифра делает бизнес-модель нерентабельной, проект "Путь B" (в его нынешнем виде) должен быть остановлен.
* Рекомендация 2 (Альтернативы): Прекратить тратить ресурсы на поиск безлицензионных альтернатив "живой" миграции. Данный анализ подтверждает, что Intel 6, AMD 4 и существующие проекты с открытым исходным кодом 5 не предоставляют готового к производству решения для миграции современных GPU (H100/A100) на KVM в 2025 году. NVIDIA имеет эффективную монополию на эту функцию.
* Рекомендация 3 (Технический стек): Принять "проект" OpenStack Caracal 8 в качестве внутреннего стандарта для вашего кластера Proxmox. "Лаунчер" должен принудительно устанавливать и поддерживать libvirt 8.6.0+, QEMU 8.1.0+ и Kernel 5.18.0+ на всех хостах GPU.
* Рекомендация 4 (Управление рисками "Серой зоны"): Принять стратегическое решение о риске "серой зоны".10 При использовании Proxmox + NVAIE необходимо выделить бюджет на создание внутренней команды L3-поддержки по KVM/QEMU, поскольку вы не сможете рассчитывать на официальную поддержку от Proxmox или NVIDIA для этого конкретного (и критически важного) сочетания.
* Рекомендация 5 (Производительность): Отказаться от цели "119 мс" 15 на данном этапе. Реалистичная цель — стабильная, успешная миграция с простоем ~500-1000 мс (аналогично миграции RAM 14). "Незаметный перехват" (119 мс), вероятно, потребует R&D-усилий. "Лаунчер" должен быть спроектирован для миграции ВМ в периоды низкой нагрузки или с допущением заметного (но короткого) "подвисания" для пользователя.
Источники
1. Live Migration for GPU-Accelerated Virtual Machines - NVIDIA, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
2. NVIDIA vGPU for Compute — NVIDIA AI Enterprise, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
3. Live Migration for GPU-Accelerated Virtual Machines - NVIDIA, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
4. The MxGPU solution on Proxmox VE 8.4 to share an AMD Radeon™ PRO W7900 Dual Slot graphics card, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
5. Intel i915 sr-iov mode for Flex 170 + Updated for Proxmox 9 PVE Kernel 6.14.8, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
6. Chapter 12. Migrating virtual machines - Red Hat Documentation, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
7. Pricing — NVIDIA Enterprise Licensing Guide - NVIDIA Docs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
8. Attaching virtual GPU devices to guests — nova 32.1.0.dev54 ..., дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
9. NVIDIA Virtual GPU 18.0 Enables VDI for AI on Every Virtualized ..., дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
10. NVIDIA Virtual Compute Server - subscription license (3 years) - 1 ..., дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
11. NVIDIA vGPU on Proxmox VE - Proxmox VE, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
12. Proxmox Wiki about Status? Live Migration? VGPU Support? Overview, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
13. Virtualization and Migration with GPGPUs - ITEC, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
14. VM migration speed question - Proxmox Support Forum, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
15. gMig: Efficient vGPU Live Migration with Overlapped Software-based Dirty Page Verification - NUS Computing, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
16. Nvidia vGPU mdev and live migration - Proxmox Support Forum, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
17. gMig: Efficient vGPU Live Migration with Overlapped Software-Based Dirty Page Verification | Request PDF - ResearchGate, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
18. gMig: Efficient GPU Live Migration Optimized by Software Dirty Page for Full Virtualization | Request PDF - ResearchGate, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
19. NVIDIA Enterprise (NVAIE and NVIDIA Omniverse Enterprise)Subscription per GPU 1 Year Includes Standard 8x5 Support - Dell, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
20. NVIDIA AI Enterprise Subscription per GPU 5 Years Includes Standard 8x5 Support - Dell, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
21. What are the best practices for deploying NVIDIA Tesla GPUs in a, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
22. Intel GVT-g - ArchWiki, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
23. ok but what about literally every other MxGPU-capable GPU · Issue #1 - GitHub, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
24. Intel Gen 12th Iris Xe vGPU on Proxmox - Reddit, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
25. Project-HAMi/volcano-vgpu-device-plugin - GitHub, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
26. Project-HAMi/HAMi: Heterogeneous AI Computing Virtualization Middleware(Project under CNCF) - GitHub, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
27. HAMi: Open, Device Virtualization, VGPU, Heterogeneous AI Computing, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
28. [Seeking Advice] CNCF Sandbox project HAMi – Why aren't more global users adopting our open-source fine-grained GPU sharing solution? : r/kubernetes - Reddit, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
29. NVIDIA's New vGPU Solution Cracked: RTX 30-Series & 40-Series Gaming GPUs Now Support vGPU : r/Proxmox - Reddit, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
30. vGPU Live Migration on Ubuntu+KVM - General Discussion - NVIDIA Developer Forums, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
31. Migration - KVM, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
32. 15.5. Live KVM Migration with virsh | Virtualization Deployment and Administration Guide | Red Hat Enterprise Linux, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
33. Virtual GPU Software User Guide - NVIDIA Docs, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
34. GPU virtualisation with QEMU/KVM - Ubuntu Server documentation, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
35. NVIDIA vGPU & MIG | - docs - OpenNebula, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
36. Maximizing GPU Power with OpenNebula: Passthrough and vGPU, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
37. Support to vGPU - OpenNebula Community Forum, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
38. How to Enable Automatic Live Migration on Host Failure in OpenNebula? - Product Support, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]