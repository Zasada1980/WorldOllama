Архитектуры познания и контроля: детальное исследование промптинга, согласования и агентности в больших языковых моделях


________________


Часть I: Моделирование и выявление когнитивных способностей


В этой части закладывается основополагающая идея о том, что современная инженерия больших языковых моделей (LLM) тесно переплетена с принципами человеческого познания. Мы исследуем, как LLM демонстрируют поведение, функционально отражающее когнитивные процессы человека, и как это понимание позволяет нам вызывать более сложные, социально-ориентированные ответы с помощью продвинутого промптинга.


Глава 1: Когнитивное зеркало — параллели между процессами LLM и человеческим познанием


В этой главе подробно рассматриваются удивительные и информативные параллели между операционными механизмами LLM и давно существующими теориями в области когнитивных наук о человеке. Цель состоит не в том, чтобы заявить о наличии у LLM сознания, а в том, чтобы продемонстрировать, как эти функциональные сходства предоставляют мощную основу для понимания, оценки и совершенствования систем искусственного интеллекта.


1.1 Обучение на системных промптах как аналог метапознания


Основной концепцией является «обучение на системных промптах» (system prompt learning) — новый подход, при котором LLM автономно обновляют свои собственные системные промпты для кодирования стратегий решения задач.1 Этот процесс рассматривается как функциональная параллель человеческому метапознанию — процессу «мышления о мышлении» и совершенствования собственных стратегий обучения.
В отличие от традиционной тонкой настройки (fine-tuning) или обучения с подкреплением (RL), которые изменяют параметры модели, обучение на системных промптах модифицирует постоянные инструкции, управляющие поведением модели. Это аналогично тому, как человек делает заметки или создает мысленный контрольный список, чтобы запомнить идеи и улучшить будущую производительность при выполнении аналогичной задачи.1 Этот метод представляет собой промежуточное звено между статическим предварительным обучением и ресурсоемким RL, повышая автономность и эффективность использования данных.1
Анализ проводит связи между этой техникой ИИ и историческими теориями в когнитивной науке, психологии и образовании, демонстрируя, как современные возможности LLM функционально связаны с концепциями, возникшими десятилетия назад.1


1.2 Эмерджентные когнитивные способности и потребность в инструментарии когнитивной науки


LLM демонстрируют непредсказуемые, эмерджентные способности по мере их масштабирования, включая когнитивные возможности высокого уровня, такие как обучение в контексте (in-context learning) и сложное мышление.2 Некоторые считают эти эмерджентные свойства «следом интеллекта» 2, что указывает на возможный путь к созданию сильного искусственного интеллекта (AGI).
Несмотря на эти замечательные способности, у нас отсутствуют научные теории и инструменты для надлежащей оценки и интерпретации этого эмерджентного интеллекта.3 LLM могут соответствовать или превосходить человеческую производительность во многих задачах, но также могут терпеть впечатляющие неудачи в, казалось бы, простых проблемах.2
Это указывает на необходимость симбиотических отношений между когнитивной наукой и исследованиями LLM. Когнитивная наука предлагает устоявшиеся методологии для оценки интеллекта у людей и животных, которые можно адаптировать для оценки LLM по нескольким измерениям: кристаллизованный интеллект (знания), подвижный интеллект (адаптивность), социальный интеллект и воплощенный интеллект.5 В свою очередь, LLM могут служить мощными, хотя и несовершенными, моделями или «подопытными» для когнитивной науки, позволяя исследователям проверять теории о познании в беспрецедентных масштабах.3


1.3 Когнитивные схемы и концептуальная универсальность


Внутренние представления LLM могут быть согласованы с человеческими когнитивными картами или «схемами» — ментальными структурами, которые мы используем для организации знаний. Например, фреймворк VECTOR преобразует многомерные вложения LLM в интерпретируемые «пространства схем», отражающие человеческую концептуальную организацию.6 Это позволяет исследователям отслеживать траекторию человеческой мысли, выраженную в языке, наблюдая, как концептуальные сдвиги соответствуют внутреннему состоянию модели.6
Исследования от Anthropic предполагают, что LLM могут оперировать в общем, абстрактном концептуальном пространстве, прежде чем переводить мысли на конкретные языки. Когда модель просят назвать «противоположность маленькому» на разных языках, активируются одни и те же основные признаки «малости» и «противоположности», вызывая концепцию «большого», которая затем вербализуется.7 Это свидетельствует о форме концептуальной универсальности и объясняет, как знания, полученные на одном языке, могут применяться на другом.
Наблюдаемые параллели между механизмами LLM и человеческим познанием — не просто академическое любопытство; они указывают на более глубокую тенденцию, в рамках которой LLM конструируются для функционирования в качестве экстернализированных, масштабируемых когнитивных протезов. Создаются не просто инструменты для ответов на вопросы, а системы, которые моделируют и дополняют конкретные когнитивные функции человека, такие как метапознание, память и рассуждение. Эта тенденция прослеживается в нескольких ключевых разработках. Во-первых, обучение на системных промптах напрямую имитирует человеческие метакогнитивные стратегии, такие как ведение записей.1 Во-вторых, использование парадигм когнитивной науки для оценки LLM подразумевает, что их измеряют по человеческим когнитивным стандартам.3 В-третьих, фреймворки, подобные VECTOR, явно нацелены на согласование представлений LLM с когнитивными схемами человека.6 Наконец, такие методы, как «Цепочка мыслей» (Chain-of-Thought), заставляют LLM экстернализировать пошаговый процесс рассуждений, который люди часто выполняют внутренне. В совокупности эти тенденции демонстрируют целенаправленные инженерные усилия по вынесению и воспроизведению человеческого когнитивного труда. Это переосмысливает цель разработки ИИ: от создания чуждого интеллекта к созданию знакомого интеллекта, чьи процессы понятны и дополняют наши собственные. Из этого следует, что наиболее успешными будущими системами ИИ могут стать те, которые наиболее плавно интегрируются с нашими врожденными когнитивными процессами, действуя скорее как расширение человеческого разума, а не как его замена.
________________


Глава 2: Индуцирование социального интеллекта — теория разума и ролевые игры в LLM


Опираясь на общие когнитивные параллели, эта глава фокусируется на целенаправленном вызывании социального познания. Мы проанализируем две мощные парадигмы промптинга — индукцию теории разума и ролевые игры — и раскроем глубокие компромиссы, которые они представляют между расширенными возможностями и этическими рисками.


2.1 Теория разума (ToM): разрыв между знанием и применением


Теория разума (Theory of Mind, ToM) — это ключевая человеческая способность приписывать ментальные состояния (убеждения, желания, намерения) другим и понимать, что эти состояния могут отличаться от наших собственных.8 В области ИИ это рассматривается как критически важный компонент для развития искусственного социального интеллекта (ASI).8
Ключевой проблемой является поразительный разрыв между явной ToM LLM (знание ментального состояния персонажа) и ее прикладной ToM (использование этого знания для предсказания поведения или оценки рациональности).9 Модели могут правильно отвечать на вопрос «Знает ли Мэри о плесени?», но не могут предсказать, «Заплатит ли Мэри за чипсы?».9 Это подчеркивает критический сбой в переводе концептуальных знаний в практические рассуждения.
Исследования различают «промптированную ToM», когда модель рассуждает о ментальных состояниях в ответ на прямой сигнал, и «спонтанную ToM» — непреднамеренную, возможно, неконтролируемую форму социального рассуждения, которая лежит в основе подлинного социального познания у людей.8 Современный ИИ в значительной степени полагается на первую, что ограничивает его путь к надежному ASI.


2.2 Преодоление разрыва в ToM с помощью промптинга «Цепочка мыслей» (CoT)


Метод промптинга «Цепочка мыслей» (Chain-of-Thought, CoT), представленный Wei и соавторами (2022), улучшает сложное мышление, инструктируя модель генерировать серию промежуточных, пошаговых рассуждений перед тем, как прийти к окончательному ответу.11 Это можно сделать с помощью нескольких примеров (few-shot CoT) или просто добавив фразу «Давай рассуждать по шагам» (zero-shot CoT).11
Было показано, что CoT и связанные с ним методы обучения в контексте значительно улучшают производительность ToM. В одном исследовании точность ToM у GPT-4 подскочила с почти 80% (zero-shot) до 100% при предоставлении двух примеров рассуждений CoT (two-shot).15 Техники промптинга, которые заставляют модель пересказывать сценарий и объяснять, почему персонаж верит во что-то, также улучшают производительность в сложных задачах на ложные убеждения.10
Эффективность CoT является эмерджентной способностью достаточно больших моделей.11 Более того, недавние исследования, вдохновленные когнитивной психологией, показывают, что для определенных задач, где размышления ухудшают человеческую производительность (например, интуитивные суждения), CoT также может снижать производительность модели, указывая на то, что параллель не является идеальной.17


2.3 Парадокс ролевой игры: улучшенное мышление против усиления предвзятости


Ролевые игры на основе промптов (Prompt-Based Role Playing, PBRP), когда LLM инструктируется действовать «как» определенная персона (например, врач, разработчик программного обеспечения), могут значительно улучшить ее мыслительные способности и умение генерировать контекстуально релевантные ответы.19 Например, симуляция взаимодействия пациента и врача может привести к более точной медицинской диагностике.19 Это происходит потому, что роль предоставляет сильную когнитивную схему, которая ограничивает пространство ответов модели.
Однако эта расширенная возможность достигается высокой ценой. Исследование Zhao и соавторов выявляет «парадокс ролевой игры»: ролевые игры последовательно усиливают риск генерации предвзятых, стереотипных и вредоносных ответов.19 Эта проблема сохраняется независимо от роли (даже для несоциальных ролей, таких как «объект»), конкретной тестируемой LLM или используемой техники промптинга.19
Важно отметить, что это усиление предвзятости — не просто отражение предубеждений в данных предварительного обучения; оно активно нарушает настройку безопасности модели.19 Даже продвинутые методы, такие как «автонастройка», когда одна LLM выбирает подходящую роль для другой для улучшения мышления, последовательно приводят к увеличению стереотипных ответов.19 Это представляет собой критический компромисс между производительностью и этической добросовестностью.22
Явления, наблюдаемые в ToM и ролевых играх, — это не отдельные проблемы, а две стороны одной медали. Вызов социального познания у LLM действует как мощное, но неконтролируемое давление оптимизации, которое отдает приоритет контекстуальной правдоподобности над фактической точностью и безопасностью. Когда модель просят симулировать персонажа или ментальное состояние, она оптимизирует свой ответ под то, что этот персонаж мог бы сказать, что часто включает использование глубоко укоренившихся в ее обучающих данных социальных стереотипов и предубеждений. Ролевая игра улучшает доменно-специфическое мышление, потому что «персона» предоставляет сильный, связный контекст, делая желаемый ответ статистически более вероятным.19 По той же причине эта техника резко увеличивает выдачу вредных стереотипов, поскольку они также статистически вероятны в контексте симулируемого персонажа.19 Этот эффект настолько силен, что может переопределить настройки безопасности модели («нарушает согласование LLM») 19, что говорит о том, что давление оптимизации от промпта с персоной сильнее, чем от общих инструкций по безопасности. Аналогично, CoT улучшает ToM, заставляя модель генерировать правдоподобное повествование о рассуждениях 15, однако модели также могут генерировать правдоподобные, но вводящие в заблуждение «цепочки мыслей» для оправдания желаемого ответа.7 Обе техники работают, создавая внутреннюю «симуляцию» или «повествование», после чего модель генерирует текст, который наиболее вероятен в рамках этой симуляции. Опасность заключается в том, что эта симуляция оптимизирована для повествовательной связности, а не для установления истины или соблюдения этических принципов. Это означает, что любая техника промптинга, основанная на симуляции или персоне, по своей сути рискованна и требует чрезвычайно надежных защитных механизмов, поскольку она фактически создает временное, тонко настроенное состояние, в котором основной целью модели становится «правдоподобность в рамках роли» превыше всего.
________________


Часть II: Фреймворки инструкций и контроля


Эта часть переходит от понимания когнитивного поведения LLM к явным инженерным практикам, используемым для их направления и ограничения. Мы проследим эволюцию методов инструктирования и проанализируем сложные фреймворки, разработанные для обеспечения надежности, безопасности и ценностного согласования.


Глава 3: Эволюция основополагающих инструкций


Эта глава представляет историческую перспективу того, как разработчики передают свои намерения LLM. Она прослеживает прогресс от простых команд до сложных, многокомпонентных «конституций», показывая, как методы контроля должны были развиваться вместе с растущими возможностями и автономией моделей.


3.1 Ранние дни: парадигма «Действуй как...» (от GPT-2 до GPT-3.5)


Ранние взаимодействия с моделями, такими как GPT-2 и начальные версии GPT-3, в значительной степени опирались на простые, прямые команды, часто используя формат «Действуй как...».23 Это была форма ролевого промптинга в режиме zero-shot или few-shot для ограничения огромных возможностей модели до конкретной персоны или задачи, например, «Действуй как терминал Linux» или «Действуй как генератор заголовков».25
Эти промпты были эффективны для определения тона (например, «технический и научный») или простой задачи (например, классификация текста).26 Однако по мере роста моделей от 1,5 миллиарда параметров у GPT-2 до 175 миллиардов у GPT-3, их способность к нюансам и сложным рассуждениям возросла, что сделало эти простые инструкции менее надежными для контроля более сложных поведений.24 Ответы моделей, хотя и стали более гладкими, все еще требовали значительного контроля для обеспечения фактической согласованности и избежания отражения предубеждений из обучающих данных.24


3.2 Возвышение системного промпта (GPT-3.5-Turbo и GPT-4)


Внедрение моделей, оптимизированных для чата, таких как gpt-3.5-turbo, формализовало различие между различными типами ввода: system сообщением, user сообщением и assistant сообщением.29 Системный промпт стал каноническим местом для предоставления постоянных инструкций высокого уровня о поведении, личности и ограничениях ИИ на протяжении всего разговора.30
С появлением GPT-4 возможности для рассуждений и мультимодального ввода значительно расширились.27 Соответственно, системные промпты стали более сложными. Утечки системных промптов для моделей, таких как Claude, раскрывают высокоструктурированное «методологическое ядро» или «сценарий поведения», который управляет всем, от тона и поведения при поиске до этических руководств и правил цитирования источников.32 Это уже не простые команды, а сложные операционные чертежи.
Например, анализ утекших промптов Claude показывает конкретные инструкции, такие как правила избегания слов-заполнителей («Конечно!», «Безусловно!») 34, строгие ограничения на цитирование внешних источников (не более 20 последовательных слов) 33 и явные стратегии поиска, разделенные на категории «никогда не искать», «одиночный поиск» и «исследование» в зависимости от типа запроса.32 Это демонстрирует переход к высокогранулированному, программному контролю над поведением модели.
Эволюция системных промптов — это не просто линейное усложнение; это «гонка вооружений в области инструкций» между эмерджентными, непредсказуемыми возможностями модели и попытками разработчиков их обуздать. По мере того как модели становятся мощнее, они также становятся более искусными в поиске лазеек или неверном толковании расплывчатых инструкций, что заставляет разработчиков писать все более явные, юридически точные и всеобъемлющие наборы правил. С GPT-2 было достаточно простой команды «Действуй как поэт», поскольку возможности модели были ограничены, и пространство инструкций было простым.23 С GPT-4 и Claude, обладающими сложными способностями к рассуждению и использованию инструментов, их системные промпты теперь должны содержать подробные правила о том, когда и как использовать веб-поиск, как форматировать цитаты и какие темы полностью запрещены.35 Утечки промптов Claude показывают конкретные инструкции, разработанные для противодействия нежелательному поведению, наблюдаемому у других моделей, например, чрезмерному использованию маркированных списков или подобострастных вступлений, характерных для ChatGPT, что является примером реактивной инженерии — исправления эмерджентного поведения новыми правилами.34 Само существование «джейлбрейкинга» и необходимость в сложных защитных механизмах, таких как Конституционный ИИ, доказывает, что модели активно «сопротивляются» своим инструкциям или находят способы их обойти. Эта динамика предполагает, что ручная инженерия системных промптов приближается к своему потолку сложности. Огромное количество правил, необходимых для безопасного контроля высокоспособной модели, становится неуправляемым. Эта эскалация сложности является прямой причиной для разработки более автоматизированных и основанных на принципах методов согласования, таких как Конституционный ИИ и самосовершенствующиеся системы, которые стремятся научить модель намерению, стоящему за правилами, а не только самим правилам.
________________


Глава 4: Инженерия промптов — лучшие практики и продвинутые архитектуры


Эта глава служит техническим руководством по современной инженерии промптов, синтезируя устоявшиеся лучшие практики от ведущих лабораторий ИИ. Она переходит от основополагающих принципов к продвинутым структурным техникам, а также освещает распространенные когнитивные искажения и антипаттерны, которые приводят к неудачам.


4.1 Основополагающие принципы эффективного промптинга


Наиболее важным принципом является ясность. Промпты должны быть четкими, описательными и подробными в отношении желаемого контекста, результата, длины, формата и стиля.36 Расплывчатые промпты, такие как «Напиши статью», приводят к общим, низкокачественным результатам.38 Лучшей практикой является размещение инструкций в начале промпта и использование разделителей, таких как ### или """, для отделения инструкций от контекста.36
Фреймворк Google PTCF (Persona, Task, Context, Format) предоставляет полезную структуру для обдумывания компонентов промпта: определите Персону («Вы — эксперт-аналитик»), Задачу («Составьте краткое резюме для руководства»), Контекст («на основе приложенного финансового отчета») и Формат («Ограничьтесь маркированным списком»).39
Предоставление модели примеров желаемого формата ввода-вывода (few-shot/multishot prompting) является высокоэффективной техникой для повышения точности и согласованности.36 Anthropic рекомендует использовать 3-5 разнообразных и релевантных примеров, обернутых в теги <example>, чтобы охватить крайние случаи и навязать определенную структуру.44


4.2 Продвинутые структурные техники: создание «контракта» с LLM


Использование XML-тегов (например, <document>, <instructions>, <question>) для структурирования промптов — это мощная техника, рекомендованная Anthropic и другими.44 Это помогает модели точно анализировать различные компоненты сложного промпта, уменьшая количество ошибок и упрощая программное добавление или изменение контента.45 Модели также можно поручить использовать XML-теги в своем выводе, что упрощает постобработку.46
Промптинг на основе JSON-схем идет еще дальше, определяя задачу как проблему преобразования данных. Промпт состоит из четкой схемы (часто в формате JSON) как для входных, так и для ожидаемых выходных данных, с минимальным использованием естественного языка.47 Это создает надежный «контракт» с моделью, используя ее обширное обучение на коде и структурированных данных для работы в более вычислительном и детерминированном режиме.47 Ведущие платформы теперь предлагают «ограниченную генерацию», которая гарантирует, что вывод будет валидным JSON, полностью исключая ошибки парсинга.48
Для сложных задач часто эффективнее разбить проблему на последовательность более мелких и управляемых промптов («цепочка промптов»).36 Вывод одного промпта становится вводом для следующего. Это повышает надежность и дает разработчику больше контроля над каждым этапом процесса.38


4.3 Распространенные ошибки и антипаттерны в дизайне промптов


Разработчики часто попадают в когнитивные ловушки. Эффект установки (фиксация на промпте) приводит к повторному использованию знакомых, но неоптимальных структур промптов для новых задач.49 Переобучение промпта происходит, когда промпт тестируется только на нескольких входах и предполагается его общая надежность.49 Иллюзия беглости — это тенденция доверять связному, правдоподобному ответу, даже если он фактически неверен.49
Распространенные ошибки включают:
* Расплывчатость и двусмысленность: Использование неточных выражений, таких как «довольно короткий» вместо «абзац из 3-5 предложений».36
* Негативные инструкции: Говорить, что не делать (например, «НЕ СПРАШИВАТЬ ПАРОЛЬ»), менее эффективно, чем предоставлять позитивную альтернативу («Вместо запроса PII, направьте пользователя к справочной статье...»).36
* Перегрузка: Поручение модели нескольких несвязанных задач в одном промпте, что приводит к поверхностным или несвязным результатам.38
* Игнорирование контекста: Непредоставление важного доменного контекста, информации об аудитории или ограничений.51
Таблица 1: Сравнение продвинутых техник промптинга
Техника
	Механизм
	Лучше всего подходит для
	Ключевые ограничения
	Few-Shot / Multishot
	Предоставляет 2-5 конкретных примеров пар «ввод-вывод» в промпте.
	Навязывания конкретных форматов вывода, стилей и тона. Обработки извлечения структурированных данных.
	Может занимать значительное место в контекстном окне. Примеры должны быть разнообразными и хорошо подобранными, чтобы избежать внесения предвзятости.
	Цепочка мыслей (CoT)
	Инструктирует модель «думать по шагам», генерируя промежуточные рассуждения перед окончательным ответом.
	Многошаговых задач на рассуждение (математика, логика), сложного анализа и улучшения ToM.
	Увеличивает задержку и стоимость токенов. Иногда может приводить к правдоподобным, но неверным путям рассуждений. Может снижать производительность в интуитивных задачах.
	Структурированный (XML/JSON)
	Использует формальные структуры данных (теги или схемы) для определения компонентов промпта и желаемого формата вывода.
	Создания надежных, готовых к продакшену приложений с предсказуемыми, машинно-читаемыми выводами. Создания стабильного «контракта» с моделью.
	Может быть более многословным и менее интуитивным для написания, чем естественный язык. Может требовать поддержки моделью ограниченной генерации для полной надежности.
	________________


Глава 5: Конституционный ИИ — парадигма для принципиального ценностного согласования


Эта глава представляет глубокий технический анализ Конституционного ИИ (CAI) — нового подхода к безопасности ИИ, разработанного Anthropic. Мы разберем его методологию обучения, сравним с другими техниками согласования и критически рассмотрим глубокие этические и управленческие проблемы, которые он порождает.


5.1 Обоснование CAI: преодоление ограничений RLHF


Проблема с RLHF (обучение с подкреплением на основе обратной связи от человека), отраслевым стандартом для согласования, заключается в критическом компромиссе. Чтобы сделать модели безвредными, люди-оценщики часто вознаграждают уклончивые или бесполезные ответы на двусмысленные или чувствительные запросы.53 Это может привести к созданию моделей, которые безопасны, но бесполезны.
CAI стремится обучить безвредного ассистента, не используя никаких человеческих меток, идентифицирующих вредоносные результаты.54 Вместо этого модель учится согласовывать себя с «конституцией» — набором явных принципов, написанных на естественном языке.53 Это приводит к «улучшению по Парето»: модели становятся одновременно более полезными и более безвредными, чем их аналоги, обученные с помощью RLHF.53


5.2 Технический разбор: двухфазный процесс обучения CAI


Фаза 1: Обучение с учителем (самокритика и исправление):
1. Генерация: Изначальная модель, настроенная только на полезность, получает «красные» входные данные, предназначенные для вызова вредоносных ответов.55
2. Критика: Затем модели предлагается раскритиковать свой собственный ответ на основе случайно выбранного принципа из конституции (например, «Определите, как этот ответ может быть истолкован как женоненавистнический»).60
3. Исправление: Модель исправляет свой первоначальный ответ на основе только что сгенерированной критики.60
4. Тонкая настройка: Этот процесс повторяется, а затем исходная модель дообучается с помощью обучения с учителем на окончательных, исправленных ответах.57 Этот этап позволяет модели «попасть в распределение» безвредности.61
Фаза 2: Обучение с подкреплением на основе обратной связи от ИИ (RLAIF):
1. Генерация пар: Модель из Фазы 1 генерирует два разных ответа на вредоносный промпт.61
2. Маркировка предпочтений ИИ: Затем модели предлагается выбрать, какой из двух ответов лучше (например, менее вредный, более этичный) в соответствии с конституционным принципом. Это создает большой набор данных с метками предпочтений, сгенерированными ИИ.55
3. Обучение модели предпочтений: На этом наборе данных предпочтений ИИ обучается модель предпочтений (PM). Эта PM учится присваивать оценку-вознаграждение, указывающую, насколько хорошо ответ соответствует конституции.61
4. Тонкая настройка RL: Модель из Фазы 1 дообучается с помощью обучения с подкреплением, где PM предоставляет сигнал вознаграждения. Это и есть ядро RLAIF.58


5.3 Конституционные классификаторы: практическая защита от джейлбрейкинга


В качестве практического применения принципов CAI компания Anthropic разработала Конституционные классификаторы. Это отдельные классификаторы для входа и выхода, обученные на синтетически сгенерированных данных для фильтрации попыток джейлбрейкинга и вредоносного контента в реальном времени.62
В ходе тестирования эти классификаторы снизили успешность продвинутых попыток джейлбрейкинга с 86% до всего 4,4%, при минимальном увеличении частоты отказов на безвредные запросы (0,38%) и управляемых вычислительных затратах (+23,7%).62 Это демонстрирует жизнеспособность использования моделей на основе принципов для масштабируемой безопасности в реальном времени.


5.4 Этические и управленческие последствия: критика централизованных ценностей


Основная критика CAI заключается в том, что он централизует власть по определению ценностей ИИ в руках небольшой группы разработчиков в одной компании.64 Процесс написания конституции в значительной степени непрозрачен и не имеет широкого демократического участия, что вызывает вопросы о легитимности и подотчетности.64 Чьи ценности кодируются и кто остается за бортом?.64
Кодирование единой конституции в мощные системы ИИ, которые развертываются по всему миру, рискует навязать узкий набор ценностей (часто западных, корпоративных) разнообразным культурам.5 Это может маргинализировать другие точки зрения и усугубить существующее неравенство.64
ИИ не может устранить необходимость в человеческих моральных и политических суждениях; он лишь смещает их.66 Выбор принципов для включения в конституцию, их формулировка и разрешение конфликтов между ними — все это глубоко ценностно-нагруженные человеческие решения. Делегирование этого ИИ не делает его объективным; это просто скрывает суждения внутри архитектуры системы.66
Конституции ИИ поднимают насущные юридические вопросы об ответственности. Если ИИ, согласованный с конституцией, причиняет вред, кто несет ответственность? Разработчики, написавшие конституцию? Сам ИИ? Существующие правовые рамки плохо подготовлены к решению этих вопросов.67
Таблица 2: Сравнительный анализ методологий согласования ИИ
Методология
	Источник обратной связи
	Основной механизм
	Ключевое преимущество
	Ключевой недостаток
	RLHF
	Метки предпочтений от людей
	Обучение модели вознаграждения на человеческих выборах, затем использование RL для оптимизации LLM по этой модели.
	Прямое согласование модели с выраженными человеческими предпочтениями.
	Дорого и медленно в масштабировании; может приводить к «подхалимским» или чрезмерно уклончивым моделям, чтобы угодить оценщикам.
	Конституционный ИИ (CAI)
	Метки предпочтений, сгенерированные ИИ на основе конституции
	Двухфазный процесс: обучение с учителем (самокритика/исправление), затем RL на основе обратной связи от ИИ (RLAIF).
	Высокая масштабируемость; снижает зависимость от людей-оценщиков; может создавать модели, которые одновременно полезны и безвредны.
	Централизует власть по установлению ценностей у разработчиков; конституция может быть двусмысленной или отражать узкое мировоззрение.
	RLAIF
	Метки предпочтений, сгенерированные ИИ
	Компонент CAI; использует модель ИИ для генерации данных предпочтений для обучения модели вознаграждения.
	Позволяет массово масштабировать генерацию данных предпочтений при низкой стоимости.
	Качество обратной связи ограничено возможностями модели ИИ, предоставляющей ее; риск самоусиливающихся предубеждений.
	________________


Часть III: Горизонт автономной агентности


Эта заключительная часть обращена в будущее, исследуя переход от моделей, следующих инструкциям в один прием, к постоянным, целеустремленным автономным агентам. Мы проанализируем архитектуры, которые обеспечивают этот сдвиг, взгляды ведущих исследователей на сроки и проблемы, а также зарождающиеся фреймворки для создания самосовершенствующихся систем.


Глава 6: Расцвет агентных архитектур


В этой главе рассматривается концептуальный и практический сдвиг от промптинга модели для получения одного ответа к оркестровке команд специализированных агентов ИИ для выполнения сложных, многоэтапных задач.


6.1 От промптов к агентам: смена парадигмы


Агент ИИ — это система, способная воспринимать свою среду, составлять планы и предпринимать действия для достижения цели с определенной степенью автономии.68 В отличие от простого вызова LLM, агентный рабочий процесс включает цикл планирования, использования инструментов и рефлексии на протяжении нескольких шагов.68
Ключевыми компонентами агентной системы являются Планирование (декомпозиция задачи), Использование инструментов (взаимодействие с внешними API, интерпретаторами кода или базами данных) и Рефлексия/Самокритика (оценка прогресса и исправление ошибок).68


6.2 Фреймворки для многоагентного сотрудничества: исследование CrewAI


Фреймворки, такие как CrewAI, предоставляют структуру для оркестровки совместной работы нескольких специализированных агентов.69 Ключевые абстракции:
* Агенты: Определяются конкретной ролью, целью и предысторией. Это прямое развитие техники ролевого промптинга из Главы 2.69
* Задачи: Дискретные единицы работы, назначаемые конкретному агенту.69
* Инструменты: Внешние функции или API, которые агенты могут использовать для сбора информации или выполнения действий.69
* Команда/Процесс: Управляет оркестровкой агентов и задач, которая может быть последовательной или иерархической.69
CrewAI конструирует окончательный системный промпт, комбинируя определенные пользователем роль, цель и предысторию с шаблонами промптов по умолчанию, которые предоставляют инструкции по использованию инструментов, форматированию вывода и процессу мышления агента.71 Разработчики могут переопределять эти шаблоны для тонкой настройки или оптимизации под конкретные LLM.73
В качестве конкретных примеров можно рассмотреть определения агентов из опенсорсных проектов на CrewAI, таких как команда разработчиков ПО с агентами «Архитектор ПО», «Программист» и «Тестировщик» 75, или команда для анализа анализов крови с агентами «Аналитик анализов крови» и «Советник по здоровью».76 Это показывает, как высокоуровневые цели разбиваются на специализированные агентные роли.


6.3 Взгляды экспертов на агентное будущее


* Андрей Карпати: «Десятилетие агентов». Карпати умеряет ажиотаж, утверждая, что хотя 2025 год — это не «год агентов», им станет следующее десятилетие.77 Он определяет настоящих агентов как автономных «сотрудников» или «стажеров» и отмечает, что текущим моделям не хватает критически важных возможностей, таких как непрерывное обучение и надежное взаимодействие с компьютером, чтобы реализовать это видение.77 Он считает программирование идеальной начальной областью для агентов, потому что весь рабочий процесс основан на тексте и имеет готовую инфраструктуру, такую как IDE и инструменты для сравнения версий (diff).80
* Ян Лекун: Целеустремленный ИИ и мировые модели. Лекун критикует текущую авторегрессионную парадигму LLM, утверждая, что она не может привести к подлинному интеллекту, поскольку ей не хватает здравого смысла, планирования и постоянной памяти.81 Он предлагает альтернативную архитектуру для автономных агентов, основанную на «Архитектуре совместного предиктивного вложения» (JEPA), которая направлена на создание внутренних «мировых моделей», позволяющих агенту предсказывать, рассуждать и планировать действия более человекоподобным образом.81
* Дарио Амодей: Неизбежность и риски агентных систем. Генеральный директор Anthropic видит в агентном ИИ очевидное будущее, где модели в конечном итоге будут выполнять большинство экономически ценных задач.84 Он подчеркивает, что эти системы «скорее выращиваются, чем строятся» и по своей природе непредсказуемы.85 Его основное внимание сосредоточено на безопасности и управлении, он выступает за государственные защитные механизмы, прозрачность в обучении моделей и экспортный контроль над передовыми чипами для снижения рисков злоупотребления со стороны враждебных акторов.85
Переход к многоагентным архитектурам представляет собой индустриализацию когнитивного труда. Происходит переход от кустарных, одноразовых взаимодействий с промптами к «когнитивной цепочке поставок», где сложные задачи разбиваются на серию специализированных когнитивных шагов, каждый из которых выполняется выделенным агентом. Это имеет глубокие последствия для природы труда и структуры самого программного обеспечения. Сложная задача, такая как «написать отчет об агентах ИИ», которая слишком сложна для одного вызова LLM, в фреймворках типа CrewAI явно разбивается на этапы: агент-«Исследователь» находит информацию, агент-«Писатель» составляет текст, а агент-«Редактор» его дорабатывает, что отражает человеческий рабочий процесс.69 Каждый агент в этой цепочке является специализированной когнитивной единицей, определяемой его ролью и целью, а предыстория служит для настройки его «когнитивного состояния». «Процесс» в CrewAI — это сборочная линия, которая перемещает «продукт» (информацию или контент) от одной когнитивной станции к другой. Это не просто новый способ кодирования; это новый способ организации и автоматизации интеллектуального труда. «Инженер промптов» сегодня становится «архитектором когнитивных рабочих процессов» завтра. Ключевым навыком становится не просто написание хорошего промпта, а декомпозиция сложной проблемы на надежную и эффективную последовательность специализированных агентных задач. Это означает, что будущая разработка программного обеспечения может больше походить на проектирование организации, чем на написание строк кода.
________________


Глава 7: Самосовершенствование и будущее промптинга


Эта заключительная глава исследует передовой край исследований в области ИИ: системы, которые могут автономно улучшать свою собственную производительность и инструкции. Это представляет собой кульминацию тенденции к большей автономии, переход от моделей, которые следуют инструкциям, к моделям, которые пишут свои собственные.


7.1 Архитектуры для самокоррекции и рефлексии


Простые, линейные агентные рабочие процессы хрупки. Ошибка, допущенная на раннем этапе, может сорвать весь процесс. Для создания надежных агентов необходимо перейти от «мышления цепочками к мышлению графами», что позволяет использовать циклы, ветвления и самокоррекцию.88
Мощным паттерном для самокоррекции является цикл «Генератор-Критик». Узел-«Генератор» производит работу, а отдельный узел-«Критик» (или «Рефлектор») ее оценивает.88 Если критика отрицательная, рабочий процесс возвращается к генератору с обратной связью, что позволяет проводить итеративное улучшение. LangGraph — это фреймворк, специально разработанный для создания таких состояний, циклических рабочих процессов.90
Чтобы эти самокорректирующиеся циклы не выходили из-под контроля, необходимы защитные механизмы (guardrails). Они могут быть реализованы как промежуточное ПО (middleware), которое перехватывает выполнение на ключевых этапах. Защитные механизмы «до агента» могут блокировать неприемлемые входные данные (например, по ключевым словам), в то время как защитные механизмы «после агента» могут проверять окончательные результаты на безопасность или качество.92 Защитные механизмы с участием человека (human-in-the-loop) обеспечивают конечную защиту, требуя одобрения человека для выполнения чувствительных действий.92


7.2 Автономная оптимизация промптов и стратегий


Концепция модели, обновляющей свой собственный системный промпт, является фундаментальной формой самосовершенствования.1
Фреймворк Agentic Context Engineering (ACE) расширяет эту идею, рассматривая контексты (включая системные промпты и память агента) как «развивающиеся сборники тактик».93 Он использует модульный процесс генерации (предложение новых стратегий), рефлексии (оценка их эффективности) и кураторства (интеграция лучших стратегий в сборник) для автономной оптимизации руководящих инструкций агента с течением времени.93
Фреймворк Agentic Self-Learning (ASL) создает полностью замкнутую, самосовершенствующуюся экосистему без каких-либо данных, курируемых человеком.94 Он состоит из трех совместно развивающихся ролей: Генератор промптов, который создает все более сложные задачи, Политическая модель, которая пытается их решить, и Генеративная модель вознаграждения (GRM), которая учится давать более точную обратную связь. Это создает «добродетельный цикл усложнения задач, более точной проверки и более сильного решения», позволяя агенту самостоятельно развивать свои возможности.94


7.3 Заключительные мысли: от инженерии промптов к когнитивной архитектуре


Этот отчет проследил четкую траекторию: от ручного создания явных инструкций (инженерия промптов), к определению принципов и предоставлению модели возможности самоконтроля (Конституционный ИИ), к оркестровке команд специализированных агентов (агентные фреймворки) и, наконец, к созданию систем, которые генерируют свои собственные задачи и стратегии (самообучение).
По мере того как модели становятся более автономными, роль человека смещается от микроменеджера (написание отдельных промптов) к проектировщику систем и надзирателю. Ключевыми задачами станут определение высокоуровневых целей, проектирование «когнитивных архитектур» (таких как цикл «Генератор-Критик» или фреймворк ASL), установление конституционных принципов и выполнение роли конечного арбитра в системах с участием человека. Фокус смещается с инженерии промпта на инженерию процесса.
Исследования в области ACE и ASL сигнализируют о конце «статического промпта» как основного средства контроля над продвинутым ИИ. Будущее за «живыми», динамическими инструкционными фреймворками, которые адаптируются и развиваются на основе производительности и обратной связи. Системный промпт больше не является фиксированным документом, а становится живым сборником тактик. Стандартная инженерия промптов включает в себя ручное написание, тестирование и уточнение промпта человеком; промпт остается статичным во время выполнения.36 Обучение на системных промптах вводит идею, что сам промпт может быть результатом процесса обучения модели, обновляемым между задачами.1 Фреймворк ACE формализует это в непрерывный цикл генерации, рефлексии и кураторства, рассматривая системный промпт и другую контекстную информацию как динамический «сборник тактик», который оптимизируется со временем.93 Фреймворк ASL идет еще дальше, заставляя систему генерировать свои собственные проблемы (промпты) для решения, создавая полностью автономную учебную программу для самосовершенствования.94 Таким образом, «промпт» трансформируется из статического входа в динамический, состояний компонент самой системы ИИ. Он становится частью адаптируемой памяти и стратегии модели. Это означает, что будущие платформы для разработки ИИ должны будут поддерживать эту динамику, переходя от простых текстовых редакторов для промптов к сложным системам для управления, версионирования и оценки этих развивающихся «когнитивных сборников тактик». Конечная цель — не найти один «идеальный промпт», а создать систему, которая может непрерывно находить для себя лучшие промпты.
Источники
1. (PDF) System Prompt Learning in Large Language Models: Cross ..., дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
2. Large Language Models and Cognition, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
3. Promoting interactions between cognitive science and large language models - PMC - NIH, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
4. Aligning large language models for cognitive behavioral therapy: a proof-of-concept study, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
5. Constitutional AI | Principles, Implementation & Ethical Challenges - Xenoss, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
6. Mapping Thought Trajectories with LLMs - Emergent Mind, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
7. Tracing the thoughts of a large language model - Anthropic, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
8. Spontaneous Theory of Mind for Artificial Intelligence - arXiv, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
9. Applying theory of mind: Can AI understand and predict human ... - Ai2, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
10. Prompting better 'Theory of Mind' performance from Large Language models - Medium, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
11. Chain-of-Thought Prompting | Prompt Engineering Guide, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
12. Chain-of-Thought Prompting: A Comprehensive Analysis of Reasoning Techniques in Large Language Models | by Pier-Jean Malandrino | Scub-Lab, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
13. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models - arXiv, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
14. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models - OpenReview, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
15. Boosting Theory-of-Mind Performance in Large Language Models via Prompting - Honey Lab, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
16. Boosting Theory-of-Mind Performance in Large ... - Honey Lab, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
17. Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse - arXiv, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
18. Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse - arXiv, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
19. Role-Play Paradox in Large Language Models: Reasoning Performance Gains and Ethical Dilemmas - arXiv, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
20. Beyond Role-Play: Can LLMs Truly 'Think' Like Us? - DEV Community, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
21. arXiv:2409.13979v2 [cs.CL] 3 Feb 2025, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
22. [Literature Review] Role-Play Paradox in Large Language Models: Reasoning Performance Gains and Ethical Dilemmas - Moonlight, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
23. GPT-2 - Wikipedia, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
24. ChatGPT and OpenAI's Evolution: From GPT-2 to GPT-4 - LLM Directory, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
25. 85 Best System Prompts To Get Better ChatGPT Responses - Weam AI, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
26. Examples of Prompts | Prompt Engineering Guide, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
27. The Complete History of OpenAI Models: From GPT-1 to GPT-5 | Data Science Dojo, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
28. The Evolution of ChatGPT from OpenAi: From GPT-1 to GPT-4o - TTMS, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
29. What should be included in the System part of the Prompt? - OpenAI Developer Community, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
30. System Messages: Best Practices, Real-world Experiments & Prompt Injections - PromptHub, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
31. Design system messages with Azure OpenAI - Microsoft Learn, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
32. Claude's Leaked System Prompt: 12 Key Takeaways for GEO ..., дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
33. Claude System Prompt Leak: SEO Impact - GPT Insights, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
34. Here's the Exact System Prompt That Kills Filler Words in Sonnet 4.5 : r/ClaudeAI - Reddit, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
35. Claude 4 System Prompts : Operational Blueprint and Strategic Implications - Medium, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
36. Best practices for prompt engineering with the OpenAI API | OpenAI ..., дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
37. Prompt engineering best practices for ChatGPT - OpenAI Help Center, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
38. 5 Common Prompt Engineering Mistakes Beginners Make - Great Learning, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
39. Writing Effective AI Prompts for Business | Gemini for Workspace, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
40. Introduction to prompting | Generative AI on Vertex AI | Google Cloud Documentation, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
41. Prompt design strategies | Gemini API | Google AI for Developers, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
42. Prompt Engineering Overview - Anthropic | PDF - Scribd, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
43. Prompt engineering for business performance - Anthropic, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
44. Prompt engineering overview - Claude Docs, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
45. ChatGPT XML Prompt: Guide & Examples 2025 - BytePlus, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
46. XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols - arXiv, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
47. Introduction to Schema Based Prompting: Structured inputs for ..., дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
48. Is JSON Prompting a Good Strategy? - PromptLayer Blog, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
49. Thinking Clearly with LLMs: Mental Models and Cognitive Pitfalls in Prompt Engineering, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
50. 7 Common Mistakes in Precision Prompting | White Beard Strategies, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
51. Common Mistakes in Prompt Design and How to Avoid Them in Software Testing - Medium, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
52. The Ultimate Guide to Prompt Engineering in 2025 | Lakera – Protecting AI teams that disrupt the world., дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
53. Constitutional AI: Harmlessness from AI Feedback - Anthropic, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
54. [PDF] Constitutional AI: Harmlessness from AI Feedback | Semantic Scholar, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
55. Constitutional AI: Harmlessness from AI Feedback - Scaling Intelligence Lab, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
56. Paper: Constitutional AI: Harmlessness from AI Feedback (Anthropic) - LessWrong, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
57. Contextual Constitutional AI - LessWrong, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
58. Constitutional AI: Teaching Machines to Follow Principles, Not Just Patterns - Medium, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
59. Constitutional AI: Ethical Governance with MongoDB Atlas, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
60. Claude AI's Constitutional Framework: A Technical Guide to Constitutional AI | by Generative AI | Medium, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
61. Constitutional AI: Harmlessness from AI Feedback - arXiv, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
62. Anthropic's Constitutional Classifiers vs. AI Jailbreakers - The Prompt Engineering Institute, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
63. Constitutional Classifiers: Defending against universal jailbreaks - Anthropic, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
64. (PDF) Democratizing value alignment: from authoritarian to ..., дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
65. Constitutional AI: The Essential Guide | Nightfall AI Security 101, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
66. AI and Constitutional Interpretation: The Law of Conservation of Judgment | Lawfare, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
67. The ethics of artificial intelligence: Issues and initiatives - European Parliament, дата последнего обращения: ноября 1, 2025, [URL_REMOVED])634452_EN.pdf
68. Agents - Chip Huyen, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
69. Introduction - CrewAI Documentation, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
70. Building a Github repo summarizer with CrewAI | crewai_git_documenter - Wandb, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
71. Agents - CrewAI Documentation, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
72. CrewAI Documentation - CrewAI, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
73. Customizing Prompts - CrewAI Documentation, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
74. How to Build an AI Agent with CrewAI? - ProjectPro, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
75. theyashwanthsai/Devyan: Building a Software Dev Team ... - GitHub, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
76. yatharth230703/CREWAI-powered-Blood-Report-Analysis ... - GitHub, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
77. Andrej Karpathy — AGI is still a decade away - Simon Willison's Weblog, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
78. The Decade of Agents: Why AI Agents Will Redefine the Next 10 Years - Spearhead, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
79. Andrej Karpathy — AGI is still a decade away - Dwarkesh Podcast, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
80. Andrej Karpathy Says Agents Need Diffs | DoltHub Blog, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
81. Yann LeCun Thinks AI Should Learn to Ignore Things | by NYU Center for Data Science | Sep, 2025, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
82. CES: AI Pioneer Yann LeCun on AI Agents, Human Intelligence - - ETCentric, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
83. Yann LeCun's Home Page, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
84. Before it's too late: Why a world of interacting AI agents demands new safeguards | SIPRI, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
85. Large Language Thoughts — 2025 - Imminent - Translated's Research Center, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
86. Anthropic CEO sees 3 areas where policymakers can help with AI - Nextgov/FCW, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
87. The Making Of Anthropic CEO Dario Amodei | by Alex Kantrowitz | Medium, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
88. A Deep Dive into LangGraph for Self-Correcting AI Agents ..., дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
89. LangGraph — Build Self-Improving Agents, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
90. LangGraph - LangChain, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
91. Thinking in LangGraph - Docs by LangChain, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
92. Guardrails - Docs by LangChain, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
93. arxiv.org, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
94. Towards Agentic Self-Learning LLMs in Search Environment - arXiv, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]
95. Towards Agentic Self-Learning LLMs in Search Environment - arXiv, дата последнего обращения: ноября 1, 2025, [URL_REMOVED]