Комплексный технический аудит системы HetGPU (Путь C-1): Оценка зрелости, механики интеграции и ограничений производительности




Раздел 1: Анализ зрелости проекта, рисков и практического развертывания (Аудит Категории 1)




1.1. Идентификация проекта и ключевой риск: Статус «Heavy Development»


Анализируемый проект, HetGPU, размещен в репозитории Multi-V-VM/hetGPU.1 Фундаментальной характеристикой этого проекта, определяющей все последующие оценки рисков, является его статус. Файл README.md репозитория содержит явное и недвусмысленное предупреждение: проект находится в «активной разработке» (heavy development), и предоставленные инструкции «могут не работать».2
Этот статус переводит HetGPU из категории готовых к внедрению COTS (Commercial Off-The-Shelf) продуктов в категорию исследовательского прототипа. Любое решение о его интеграции должно приниматься с полным пониманием того, что это потребует значительных внутренних инженерных ресурсов, выделенных на стабилизацию, исправление ошибок и, вероятно, создание и поддержку долгосрочного внутреннего форка. Фактически, это не «внедрение» стороннего решения, а «совместная разрабока» или «доведение до промышленной эксплуатации».
Дальнейший анализ репозитория показывает, что HetGPU является форком проекта vosen/ZLUDA.2 ZLUDA, в свою очередь, известен как «drop-in replacement for CUDA on non-NVIDIA GPU» (прямая замена CUDA для GPU не от NVIDIA), позволяющая запускать немодифицированные приложения CUDA на альтернативном оборудовании с почти нативной производительностью.2
Это различие в происхождении критически важно. HetGPU не был написан с нуля. Он, по-видимому, унаследовал от ZLUDA зрелый механизм перехвата API (API-hooking) и трансляции вызовов CUDA. Новизна HetGPU, описанная в сопроводительной статье (arXiv:2506.15993), заключается в фундаментальном расширении этой модели. HetGPU — это не просто прокси API; это полноценная система, включающая компилятор, архитектурно-агностическое промежуточное представление GPU (GPU Intermediate Representation, IR) и среду выполнения (runtime).1
Цель этой новой архитектуры двояка:
1. Обеспечить бинарную совместимость (а не только API) с различными архитектурами, включая NVIDIA, AMD, Intel и Tenstorrent.1
2. Обеспечить «сериализацию состояния для live migration» (живой миграции).1
Таким образом, проект HetGPU состоит из двух компонентов с разной степенью зрелости: (1) Вероятно, более стабильный слой перехвата API, унаследованный от ZLUDA, и (2) Новый, экспериментальный и высокорискованный слой, состоящий из компилятора в IR и механизма захвата состояния. Все риски и вся уникальная ценность «Пути C-1» сосредоточены во втором компоненте.


1.2. Анализ лицензирования: Благоприятный режим для корпоративного внедрения


Анализ лицензионных файлов в репозитории Multi-V-VM/hetGPU показывает, что проект распространяется под двойной лицензией: Apache 2.0 или MIT.2 Лицензия на саму научную статью, размещенную на arXiv.org (бессрочная неэксклюзивная лицензия) 1, не имеет отношения к лицензированию исходного кода.
Выбор лицензий Apache 2.0 и MIT является исключительно благоприятным для корпоративного внедрения в закрытую проприетарную платформу («Оркестратор»). Обе лицензии являются пермиссивными (разрешительными). Они позволяют:
* Создавать производные работы (в том числе внутренний, стабилизированный форк).
* Модифицировать код для нужд интеграции с «Оркестратором».
* Распространять интегрированный продукт в коммерческих целях без каких-либо требований по раскрытию исходного кода (в отличие от копилефт-лицензий, таких как GPL).
С юридической точки зрения, лицензирование HetGPU не создает препятствий для его использования в качестве стратегического компонента.


1.3. Практическое руководство по развертыванию: Анализ инструкций по сборке и запуску




1.3.1. Зависимости и среда сборки


Практические инструкции по сборке 2 выявляют сложную и нетривиальную архитектуру.
Во-первых, основной командой сборки является cargo build --release.2 Это однозначно идентифицирует ядро системы HetGPU (среду выполнения, JIT-компилятор) как написанное на языке Rust.
Во-вторых, список зависимостей для сборки является крайне неортодоксальным и требовательным. Он включает одновременную установку на сборочную машину:
1. oneapi (включая level-zero и level-zero-dev от Intel)
2. cuda (включая nvidia-cuda-toolkit) 2
Это требование подразумевает, что среда выполнения HetGPU (написанная на Rust) во время своей работы динамически связывается с бэкенд-компиляторами и библиотеками всех вендоров, которые она намерена поддерживать. Когда HetGPU перехватывает бинарный код CUDA, он не просто транслирует вызовы API; он JIT-компилирует (Just-In-Time) этот код 5 через свое промежуточное представление (hetIR 6) в нативный код целевого устройства. Для этого ему требуются нативные компиляторы (Intel Level Zero для GPU Intel, компилятор NVIDIA для NVIDIA и т.д.) в качестве бэкенда.
Это имеет серьезные последствия для развертывания. «Лаунчер» («Оркестратор») должен будет управлять этим сложным и хрупким стеком зависимостей в гостевых образах, где будет работать HetGPU.


1.3.2. Известные проблемы сборки


Документация README.md проактивно предупреждает о потенциальной ошибке компоновки: /usr/bin/ld: cannot find -lLLVMTarget.2
Предоставляемое решение (ручной запуск ninja LLVMTarget в директории сборки llvm_hetGPU) 2 подтверждает два факта:
1. Система HetGPU использует LLVM в качестве основы для своей цепочки компиляции. Это согласуется с концепцией промежуточного представления (IR).
2. Проект, по-видимому, зависит от кастомной или, по крайней мере, не входящей в стандартную сборку части LLVM (LLVMTarget).
Это создает дополнительный риск при обслуживании. Обновление системной версии LLVM, скорее всего, нарушит сборку HetGPU. Проект жестко привязан к конкретной, возможно, модифицированной версии инфраструктуры LLVM, что усложняет его долгосрочную поддержку.


1.3.3. Механизм запуска (Quick Start)


Механизм запуска HetGPU на Linux является классическим примером «перехвата API» (API hooking) на уровне динамического загрузчика.2
Команда запуска: LD_LIBRARY_PATH=<hetGPU_DIRECTORY> <APPLICATION> <APPLICATIONS_ARGUMENTS>.2
Этот метод заставляет динамический загрузчик Linux (ld.so) искать разделяемые библиотеки (файлы .so) сначала в директории HetGPU, и только потом в системных путях.
Это подтверждается инструкциями post-build, которые требуют от пользователя создать символические ссылки:
* ln -s libnvcuda.so target/release/libcuda.so
* ln -s libnvcuda.so target/release/libcuda.so.1
* ln -s libnvml.so target/release/libnvidia-ml.so 2
Процесс работы выглядит следующим образом:
1. Приложение, скомпилированное с CUDA, пытается загрузить libcuda.so.
2. Переменная LD_LIBRARY_PATH заставляет его вместо системной библиотеки NVIDIA загрузить библиотеку HetGPU (которая притворяется libcuda.so).
3. С этого момента HetGPU перехватывает все вызовы CUDA API (например, cuInit, cuModuleLoadData, cuLaunchKernel).
4. Когда приложение запускает ядро, HetGPU получает бинарный код (PTX или SASS), декомпилирует или транслирует его в свое внутреннее промежуточноное представление (hetIR), а затем JIT-компилирует 5 этот IR в нативный код для фактически установленного в системе GPU (будь то AMD, Intel или даже NVIDIA).
Для Windows используется аналогичный, но специфичный для ОС механизм инъекции через исполняемый файл-обертку: <hetGPU_DIRECTORY>\hetGPU_with.exe -- <APPLICATION>.2 Поддержка MacOS отсутствует.


1.4. Оценка активности и внедрения (Критические пробелы в данных)


Запрос требовал аудита активности сообщества, планов развития («roadmap», «future development 2026») и известных случаев внедрения («community adoption», «known users»).
Анализ всех предоставленных материалов 2 не выявил абсолютно никакой информации по этим пунктам.
* Отсутствует файл ROADMAP.md или его эквивалент.
* Отсутствуют ссылки на форумы, списки рассылки или публичные обсуждения (кроме упоминания Discord в README.md проекта-предшественника ZLUDA 2, которое может не относиться к форку HetGPU).
* Отсутствует список известных пользователей, партнеров или контрибьюторов, не входящих в первоначальную группу авторов.
Этот информационный вакуум является столь же значимым, сколь и явные предупреждения в README.md. Он подтверждает, что HetGPU, скорее всего, является чисто академическим исследовательским проектом, созданным для сопровождения научной публикации (arXiv:2506.15993).1
Проект не имеет видимого сообщества или промышленной поддержки. Это означает, что организация, внедряющая его («Оркестратор»), станет не просто «пользователем», а, по сути, единственным промышленным пользователем. Весь груз по поддержке, устранению неисправностей и развитию функциональности ляжет на внутреннюю R&D команду.


Раздел 2: Архитектура интеграции «Оркестратора»: Демистификация механизма «Заморозки» (Аудит Категории 2)




2.1. Опровержение гипотезы о хуках гипервизора (KVM/Libvirt)


Первоначальная гипотеза, заложенная в запросе, предполагала, что HetGPU интегрируется на уровне гипервизора. Ожидалось найти API, такое как hetgpuCheckpoint, или механизмы интеграции, такие как гостевой агент QEMU или хуки libvirt, которые бы «Лаунчер» мог вызывать для инициирования миграции.
Целевой поиск по репозиторию и документации HetGPU 2 не дал никаких результатов по этим ключевым словам. В коде и документации Multi-V-VM/hetGPU отсутствуют какие-либо упоминания KVM, QEMU, libvirt или гостевых агентов, связанных с миграцией.
Это приводит к фундаментальному выводу: HetGPU не является решением для виртуализации на уровне гипервизора (I/O passthrough или mediated device). Механизм миграции HetGPU реализован полностью внутри гостевой операционной системы и оперирует на уровне приложения/среды выполнения. Он ничего не знает о KVM или о том, что он работает в виртуальной машине.


2.2. Истинный механизм: Кооперативное создание контрольных точек (Cooperative Checkpointing)


Поскольку интеграция на уровне гипервизора исключена, истинный механизм должен быть реализован на уровне среды выполнения. Статья (arXiv:2506.15993) предоставляет точное описание этого механизма, который является формой кооперативного создания контрольных точек (cooperative checkpointing).
Команда «ЗАМОРОЗИТЬ» от «Оркестратора» — это не прямой синхронный вызов API, а асинхронная установка флага.
Статья в 7 описывает этот процесс: среда выполнения HetGPU (на CPU в гостевой ОС) устанавливает pause_flag (флаг паузы) в участке памяти, видимом для GPU. HetGPU при компиляции или трассировке GPU-ядра внедряет в него код, который периодически проверяет этот флаг.
Это и есть искомый «триггер». Для интеграции «Оркестратора» это означает следующее:
1. «Оркестратору» все еще нужен канал связи с гостевой ОС (например, стандартный QEMU guest agent, virtio-serial или другой RPC-механизм).
2. Задача этого канала — не выполнить сложную операцию чекпойнта, а выполнить единственную простую операцию: передать команду среде выполнения HetGPU, работающей в госте, чтобы та установила pause_flag.
3. Дальнейший процесс «заморозки» полностью автономен и выполняется кооперативно кодом HetGPU и самим GPU-ядром.


2.3. Специфика реализации для NVIDIA: Динамическая бинарная трассировка через NVBit


На GPU NVIDIA механизм реализации этого кооперативного чекпойнта особенно сложен и элегантен. HetGPU использует для этого NVBit.7
NVBit — это официальный фреймворк динамической бинарной трассировки от NVIDIA (NVIDIA Dynamic Binary Instrumentation Framework).8 Он позволяет на лету внедрять код в выполняющиеся GPU-ядра.
HetGPU использует NVBit для «внедрения кода в барьерные точки» (barrier points).7 Барьерная точка — это, как правило, операция синхронизации, например __syncthreads(), где все потоки (варпы) в блоке должны дождаться друг друга.
Процесс «Заморозки» на NVIDIA, согласно 7, выглядит следующим образом:
1. «Оркестратор» через свой канал связи приказывает HetGPU-runtime в госте установить pause_flag.
2. GPU-ядро продолжает выполняться до ближайшей __syncthreads() или аналогичной барьерной точки.
3. В этот момент управление перехватывает внедренный NVBit-код, который выполняет проверку: if (pause_flag).
4. Если флаг установлен, GPU-потоки (варпы) начинают процесс самосохранения. Они «копируют регистры в глобальную память». API NVBit предоставляет для этого необходимые интроспективные возможности, позволяя «получить количество регистров и их значения для варпа».7
5. После завершения копирования состояния потоки «сигнализируют хосту» (вероятно, HetGPU-runtime на CPU) о том, что чекпойнт завершен.
6. Затем потоки «переходят к концу» (jump to end), завершая выполнение ядра.
Это чрезвычайно мощный механизм, позволяющий делать чекпойнты внутри выполняющегося ядра. Однако он также является хрупким, поскольку полностью зависит от проприетарной, потенциально нестабильной технологии NVBit. Любое обновление драйвера NVIDIA или API NVBit (например, в будущей версии R580) может нарушить работу всей системы миграции.


2.4. Специфика реализации для AMD/Intel


Для GPU AMD и Intel, где отсутствует эквивалент NVBit, HetGPU использует другой, более статический подход.
Вместо динамической трассировки, HetGPU «компилирует ядро с условным переходом на барьерах, который проверяет значение буфера».7
В этом случае проверка pause_flag внедряется в само ядро на этапе JIT-компиляции (когда HetGPU транслирует свой IR в нативный код AMD GCN/RDNA или Intel Xe).
Этот подход, вероятно, более надежен, поскольку он не зависит от внешних проприетарных инструментов трассировки. Он является полностью самодостаточным в рамках компилятора и среды выполнения HetGPU. Недостатком является то, что он менее гибкий — проверки вкомпилированы статически и не могут быть добавлены или изменены на лету, как в случае с NVBit.


Раздел 3: Производительность, совместимость и технические ограничения (Аудит Категории 3)




3.1. Совместимость с целевой архитектурой: NVIDIA Hopper (H100) и NVBit


Ключевым вопросом для внедрения является совместимость с целевой архитектурой заказчика, а именно с NVIDIA Hopper.
Анализ статьи arXiv:2506.15993 1 дает однозначный положительный ответ. Тесты производительности и миграции, проведенные авторами HetGPU, явно использовали NVIDIA H100 (архитектура Hopper, 96 ГБ) в качестве одной из основных платформ.
Более того, использование NVBit, которое является ядром механизма миграции на NVIDIA, также подтверждено для архитектуры Hopper.8
Это, возможно, самый важный положительный вывод данного аудита. Он снимает главный риск: целевая архитектура (Hopper) и основной механизм (NVBit-хук) были протестированы и подтверждены авторами HetGPU.


3.2. Накладные расходы на вычисления: HetGPU против нативного CUDA


Оценка накладных расходов на производительность является решающей для определения экономической целесообразности HetGPU. Хотя 6 указывает на "недоступность" этой информации, более полные данные из раздела 6 статьи, процитированные в 5, предоставляют точные цифры.
Тесты проводились на NVIDIA H100, AMD Radeon RX 9070 XT и Intel A750.7
Тест: Умножение матриц (1024x1024) на H100 5:
* Нативный CUDA (без использования тензорных ядер): 3.8 TFLOPs
* HetGPU: 3.5 TFLOPs
* Накладные расходы: < 8%
Тест: Редукция (1 миллион элементов) на NVIDIA (H100) 5:
* Нативный CUDA: 0.16 мс
* HetGPU: 0.17 мс
* Накладные расходы: ~6.25%
Тесты на AMD (Radeon RX 9070 XT) 5:
* Умножение матриц: Нативный HIP (3.5 TFLOPs) vs HetGPU (3.3 TFLOPs) — Накладные расходы: < 6%
* Редукция: Нативный HIP (0.18 мс) vs HetGPU (0.20 мс) — Накладные расходы: ~11.1%
Эти результаты являются исключительно важными. Они демонстрируют, что накладные расходы на вычисления, вызванные двойной трансляцией (CUDA -> hetIR -> JIT -> Native), в ключевых тестах на H100 минимальны и составляют менее 10%.
Это доказывает, что компилятор HetGPU и его промежуточное представление (hetIR) высокоэффективны и не приводят к значительным потерям производительности для самих вычислений. Это делает HetGPU жизнеспособным решением, а не просто академическим экспериментом.
Ниже представлена сводная таблица для наглядности.
Таблица 3.2.1: Сравнительный анализ производительности HetGPU и нативных API 5
Ворклод
	Платформа
	Нативный API (Производительность)
	HetGPU (Производительность)
	Накладные расходы (%)
	Умножение матриц (1024x1024)
	NVIDIA H100
	3.8 TFLOPs (CUDA, без TC)
	3.5 TFLOPs
	< 8%
	Редукция (1M)
	NVIDIA H100
	0.16 мс (CUDA)
	0.17 мс
	6.25%
	Умножение матриц (1024x1024)
	AMD 9070 XT
	3.5 TFLOPs (HIP)
	3.3 TFLOPs
	< 6%
	Редукция (1M)
	AMD 9070 XT
	0.18 мс (HIP)
	0.20 мс
	11.1%
	

3.3. Производительность Live Migration: Задержки (Downtime)


В 1 описан конкретный тест миграции: матричное умножение переносилось с NVIDIA H100 на AMD 9070 XT.
* Чекпойнт (Заморозка): Занял 0.5 секунды для копирования 2 ГБ данных (матриц) с H100 на хост через PCIe.
* Восстановление (Разморозка): Заняло 0.6 секунды для инициализации AMD GPU и загрузки 2 ГБ данных на него.
Общее время «простоя» (downtime), во время которого вычисления не производились, составило 1.1 секунды (0.5 + 0.6) для 2 ГБ состояния GPU.
Этот показатель имеет решающее значение. Задержка миграции прямо пропорциональна объему мигрируемого состояния. NVIDIA H100 имеет 96 ГБ VRAM.7 Если приложение (например, большая языковая модель) использует всю доступную память, линейная экстраполяция дает оценочное время простоя:
$(96 \text{ ГБ} / 2 \text{ ГБ}) \times 1.1 \text{ с} = 48 \times 1.1 \text{ с} = 52.8 \text{ секунды}$
Время простоя почти в одну минуту, хотя и приемлемо для перебалансировки длительных HPC-задач или для восстановления после сбоев, неприемлемо для сервисов, требующих низкой задержки (low-latency inference) или высокой доступности (high-availability).
Авторы отмечают, что это время можно сократить, используя pre-copy (фоновое копирование) или будущие технологии, такие как CXL 1, но в текущей реализации время простоя является линейной функцией от объема данных.


3.4. Известные и неизвестные технические ограничения




3.4.1. Критическое известное ограничение: Гранулярность миграции


В данных существует серьезное и потенциально критическое противоречие.
* С одной стороны, механизм NVBit 7 позволяет выполнять чекпойнт внутри ядра на барьерных точках.
* С другой стороны, в описании теста миграции 1 авторы явно заявляют: «Мы в настоящее время мигрируем на границах ядра (kernel boundaries)».
Это, вероятно, самое серьезное ограничение всей системы. Миграция на «границах ядра» означает, что HetGPU может сохранить состояние только в промежутке между вызовами cuLaunchKernel.
Если приложение запускает один монолитный GPU-кернел, который выполняется два часа (типичный сценарий для обучения ML-моделей), HetGPU не сможет мигрировать эту задачу до тех пор, пока ядро не завершится. Это полностью обесценивает функцию «живой миграции» для таких рабочих нагрузок.
Наиболее вероятное объяснение этого расхождения: механизм внутриядерного чекпойнта 7 — это теоретическая возможность или экспериментальная функция, а единственный стабильно работающий в настоящее время механизм 1 — это захват состояния между запусками ядер. Проверка этого предположения должна стать приоритетом №1 для внутренней R&D команды.


3.4.2. Критические неизвестные ограничения: Неподдерживаемые функции CUDA


6 подтверждает, что в доступных материалах отсутствует какая-либо информация о поддержке, производительности или ограничениях, связанных с:
* UVM (Unified Virtual Memory): (например, cudaMallocManaged())
* CUDA IPC (Inter-Process Communication): (например, cudaIpcGetMemHandle())
Это критический пробел в данных. Современные фреймворки для глубокого обучения и AI-ворклоды сильно зависят от UVM для упрощения управления памятью и от IPC для многопроцессной (и много-GPU) работы.
Архитектура HetGPU, основанная на абстракции памяти 1, и его происхождение от ZLUDA (который мог реализовывать только базовый API cudaMalloc/cudaMemcpy) делают весьма вероятным то, что эти низкоуровневые, тесно интегрированные с драйвером NVIDIA функции, не поддерживаются. Если HetGPU не поддерживает UVM, он будет несовместим с подавляющим большинством современных AI-приложений.


3.4.3. Другие известные проблемы: Отсутствие поддержки Тензорных Ядер (Tensor Cores)


Как было отмечено в разделе 3.2, бенчмарк умножения матриц на H100 5 сравнивал HetGPU (3.5 TFLOPs) с нативным CUDA без тензорных ядер (3.8 TFLOPs). При этом нативная библиотека cublas, использующая тензорные ядра, на том же H100 достигала 9 TFLOPs.
Вывод очевиден: компилятор HetGPU (CUDA -> hetIR -> JIT) не умеет генерировать код, использующий тензорные ядра.
Для рабочих нагрузок машинного обучения (ML) и искусственного интеллекта (AI) это катастрофическое ограничение производительности. Потеря <10% производительности из-за JIT-компиляции (как в 3.2) является приемлемой; потеря >60% TFLOPs (3.5 против 9) из-за невозможности использовать специализированное оборудование (Tensor Cores) делает решение неконкурентоспособным для ML-тренировок.


Раздел 4: Анализ конкурентной среды (Альтернативы C-1)




4.1. HetGPU как уникальный подход (IR + State Serialization)


HetGPU («Путь C-1») представляет собой уникальную в своем роде систему. Его архитектура, объединяющая компилятор, промежуточное представление (IR) и среду выполнения, специально разработана для решения двух задач одновременно:
1. Бинарная совместимость между гетерогенными GPU (NVIDIA, AMD, Intel, Tenstorrent).
2. Сериализация состояния и живая миграция GPU-ворклодов.1
На данный момент в открытых источниках нет данных о зрелых коммерческих или open-source конкурентах, которые бы пытались решить обе эти проблемы единым подходом.


4.2. Сравнение со смежными технологиями (Не конкуренты)


* MLIR (Multi-Level Intermediate Representation): MLIR 3 часто упоминается в контексте гетерогенных вычислений. Однако MLIR — это инфраструктура для создания компиляторов, а не готовое решение. Это набор инструментов, в то время как HetGPU — это (хоть и экспериментальный) законченный продукт. HetGPU мог бы использовать MLIR в качестве основы для своего hetIR, но MLIR сам по себе не является его конкурентом.
* eGPU: Этот термин упоминается в 4 и 4, но в контексте расширения eBPF для наблюдаемости (observability) GPU, а не для миграции или совместимости. Эта технология не имеет отношения к целям HetGPU.
* ZLUDA: Как обсуждалось в 1.1, HetGPU — это форк и расширение ZLUDA.2 ZLUDA является его предшественником, а не конкурентом.


4.3. Сравнение с устаревшими подходами (API Remoting)


Более старые подходы к виртуализации GPU, такие как rCUDA и gVirtuS 10, представляют собой фундаментально иной класс решений.
Эти системы работают по принципу «API forwarding» (перенаправление/проброс API). Они перехватывают вызовы CUDA API на CPU (в гостевой ОС или на клиентской машине) и пересылают их по сети на удаленный сервер, на котором должен быть физически установлен нативный GPU NVIDIA.
rCUDA и gVirtuS не решают проблему гетерогенности. Они не позволяют запускать CUDA-код на AMD или Intel. Они решают только проблему удаленного доступа.
Этот анализ подтверждает стратегическую правильность решения о прекращении исследований «Серого Пути» (который, вероятно, был основан на подходе API remoting) и фокусировке на «Пути C-1» (HetGPU). Только HetGPU с его подходом (IR + JIT) решает фундаментальную проблему бинарной совместимости с гетерогенным оборудованием.


Раздел 5: Стратегические выводы и рекомендации по внедрению




5.1. Рекомендация по снижению рисков: Немедленный форк и внутреннее тестирование


Основание: Статус «Heavy Development» 2, полное отсутствие публичной дорожной карты, сообщества и известных внедрений (Раздел 1.4), а также сложная, хрупкая процедура сборки.2
Рекомендация: HetGPU не может рассматриваться как стабильная внешняя зависимость. Команда «Оркестратора» должна немедленно создать приватный форк репозитория Multi-V-VM/hetGPU. Вся дальнейшая разработка и интеграция должны вестись исключительно на базе этого форка. Необходимо выделить инженерные ресурсы (R&D) на стабилизацию этого форка, исправление ошибок и, возможно, реализацию недостающей функциональности. Проект следует рассматривать как внутренний компонент, требующий поддержки, а не как внешний COTS-продукт.


5.2. Предлагаемый чертеж архитектуры интеграции «Оркестратора»


Основание: Анализ показал, что механизм миграции — это кооперативный чекпойнт на уровне приложения (Раздел 2.2), а не хук гипервизора.2 Триггером является асинхронный pause_flag, проверяемый через NVBit 7 или статически скомпилированный код.
Рекомендуемая архитектура интеграции:
1. «Оркестратор» (на хосте) принимает решение о миграции.
2. «Оркестратор» использует существующий канал связи с гостевой ОС (например, virtio-serial или QEMU Guest Agent) для отправки простой команды «Инициировать заморозку».
3. Легковесный агент (который необходимо разработать), запущенный в гостевой ОС, получает эту команду.
4. Агент взаимодействует со средой выполнения HetGPU (также работающей в госте) и приказывает ей установить pause_flag в памяти.
5. Среда выполнения HetGPU (на CPU) устанавливает флаг. GPU-ядро (через NVBit или JIT-код) обнаруживает этот флаг при следующем барьере.7
6. GPU-ядро кооперативно сохраняет свое состояние (например, регистры в глобальную память) и завершается.
7. Среда выполнения HetGPU детектирует успешное сохранение состояния и сигнализирует об этом гостевому агенту.
8. Гостевой агент сообщает «Оркестратору» (по обратному каналу), что гость «готов к миграции».
9. Только после получения этого подтверждения «Оркестратор» инициирует стандартную KVM-миграцию (которая перенесет дамп памяти CPU и VRAM, включая сохраненное состояние GPU).


5.3. План немедленных действий: Устранение критических пробелов в данных


Основание: В ходе аудита были выявлены три критических пробела в данных и ограничениях, каждое из которых может являться блокером для всего проекта.
Рекомендуемый план R&D-тестирования (в порядке приоритета):
1. Блокер 1 (Совместимость с AI): Поддержка UVM и IPC.
   * Тест: Попытаться скомпилировать и запустить под управлением HetGPU (на H100) базовые приложения CUDA SDK, использующие cudaMallocManaged() (UVM) и cudaIpcGetMemHandle() (IPC).
   * Обоснование: 6 подтверждает отсутствие этой информации. Если эти тесты не пройдут, HetGPU несовместим с большинством современных AI-ворклодов, и проект нежизнеспособен в текущем виде.
2. Блокер 2 (Производительность ML): Поддержка Тензорных Ядер (Tensor Cores).
   * Тест: Запустить стандартный бенчмарк (например, cublasSgemm) под HetGPU и сравнить его производительность с нативным cublas.
   * Обоснование: 5 доказывает, что тензорные ядра не используются. Этот тест должен подтвердить это и оценить точный масштаб потерь. Если потери производительности (например, >60%) подтвердятся, HetGPU непригоден для ML-тренировок.
3. Блокер 3 (Функциональность миграции): Гранулярность миграции.
   * Тест: Написать GPU-ядро с бесконечным циклом (while(1)), содержащим барьер (__syncthreads()). Запустить его под HetGPU. Попытаться инициировать миграцию (путем установки pause_flag).
   * Обоснование: Это разрешит фундаментальное противоречие между 7 (чекпойнт на барьерах) и 1 (чекпойнт на границах ядра). Если ядро не приостановится (и чекпойнт не сработает), это докажет, что миграция возможна только между запусками ядер. Это резко снижает ценность функции «live migration» для длительных HPC-задач.
Источники
1. HetGPU: The pursuit of making binary compatibility towards GPUs - arXiv, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
2. Multi-V-VM/hetGPU: PTX on XPUs - GitHub, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
3. Cross-Vendor GPU Programming: Extending CUDA Beyond NVIDIA - ResearchGate, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
4. eGPU: Extending eBPF Programmability and Observability to GPUs | Request PDF, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
5. Hetgpu: The Pursuit of Making Binary Compatibility Towards Gpus | PDF | Graphics Processing Unit | Computer Engineering - Scribd, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
6. HetGPU: The pursuit of making binary compatibility towards ... - arXiv, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
7. HetGPU: The pursuit of making binary compatibility towards GPUs - arXiv, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
8. ThreadFuser: A SIMT Analysis Framework for MIMD Programs - Purdue Engineering, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
9. KPerfIR: Towards a Open and Compiler-centric Ecosystem for GPU Kernel Performance Tooling on Modern AI Workloads - USENIX, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
10. On the benefits of the remote GPU virtualization mechanism: The rCUDA case | Request PDF - ResearchGate, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
11. On implementation of GPU virtualization using PCI pass-through - ResearchGate, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
12. Multi-Tier GPU Virtualization for Deep Learning in Cloud-Edge Systems - ResearchGate, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]
13. Smash: A Compression Benchmark with AI Datasets from Remote, дата последнего обращения: ноября 7, 2025, [URL_REMOVED]