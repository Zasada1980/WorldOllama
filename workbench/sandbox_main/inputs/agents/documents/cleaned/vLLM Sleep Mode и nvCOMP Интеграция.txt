Технический анализ и руководство по интеграции: vLLM "Sleep Mode" и GPU-сжатие nvCOMP




1. Введение: Стратегическое Обоснование "Sleep Mode" для MVP-инференса


Для эффективного развертывания нескольких моделей логических выводов (LLM) на ограниченном GPU-оборудовании требуются решения, выходящие за рамки традиционного статического распределения ресурсов. Проблема обслуживания нескольких моделей (multi-model serving) часто сводится к неэффективному компромиссу: либо постоянное хранение всех моделей в VRAM (что требует 2x-Nx GPU-памяти и часто невозможно), либо перезагрузка моделей по требованию, что влечет за собой неприемлемые задержки "холодного старта" от 30 до 100+ секунд для каждой смены модели.1
Инновационная функция "Sleep Mode" (Режим Сна) в vLLM предлагает третий путь: возможность "гибернации" моделей за считанные секунды и их быстрого "пробуждения".1 Это обеспечивает эффективность загрузки по требованию со скоростью, близкой к постоянно работающему сервису.
Этот технический отчет предоставляет исчерпывающий анализ, API-спецификации и руководство по интеграции для двух ключевых технологий:
1. vLLM "Sleep Mode": Детальное описание API, внутренние механизмы работы, уровни гибернации (L1/L2) и критические ограничения.
2. NVIDIA nvCOMP: Обзор SOTA-библиотеки для GPU-ускоренного сжатия чекпоинтов, включая интеграцию с PyTorch и будущие преимущества на архитектуре Blackwell.
Цель данного документа — предоставить инженерной команде MVP все необходимые технические данные для немедленной реализации "Sleep Mode", а также стратегическую дорожную карту для интеграции с nvCOMP с целью достижения SOTA-производительности при переключении моделей.


2. Категория 1: vLLM "Sleep Mode" - API и Механика


Функция "Sleep Mode" предназначена для освобождения ресурсов GPU (в первую очередь VRAM) путем выгрузки или отбрасывания весов модели и KV-кэша, сохраняя при этом критическую инфраструктуру инференс-движка для быстрого возобновления работы.2


2.1. Активация и API Эндпоинты


Активация "Sleep Mode" требует явного включения как на уровне CLI, так и на уровне переменных окружения.
Флаги запуска:
Для включения функции "Sleep Mode" сервер vLLM (или vllm serve) должен быть запущен с двумя ключевыми параметрами:
1. --enable-sleep-mode: Этот флаг CLI активирует внутреннюю логику "Sleep Mode" в движке vLLM.1
2. export VLLM_SERVER_DEV_MODE=1: Эта переменная окружения обязательна для предоставления доступа к эндпоинтам POST /sleep и POST /wake_up.1
Причина необходимости VLLM_SERVER_DEV_MODE связана с безопасностью. Эти эндпоинты предоставляют прямой, низкоуровневый контроль над состоянием движка и не являются частью стандартного, обращенного к клиенту API (например, OpenAI-совместимого). Они предназначены для внутреннего использования в доверенной среде.3
Пример запуска сервера (Терминал):


Bash




# Установка dev-режима для предоставления API /sleep и /wake_up
export VLLM_SERVER_DEV_MODE=1

# Запуск vLLM-сервера с включенной функцией 'sleep mode'
vllm serve Qwen/Qwen3-0.6B \
   --enable-sleep-mode \
   --port 8000

1
HTTP API Эндпоинты (Online Serving):
После запуска сервера управление состоянием сна осуществляется через следующие HTTP-эндпоинты 5:
* Перевод в сон (Уровень 1 или 2):
Bash
curl -X POST [URL_REMOVED] \
-H "Content-Type: application/json" \
-d '{"level": "1"}' # или '{"level": "2"}'

* Пробуждение (Загрузка весов):
Bash
curl -X POST [URL_REMOVED]

* Пробуждение с тегами (для RLHF):
Тег tags позволяет выборочно восстанавливать компоненты, например, только веса, что полезно для избежания OOM при обновлении весов.7
Bash
# Допустимые теги: "weights", "kv_cache"
curl -X POST "[URL_REMOVED]"

* Проверка статуса:
Bash
curl -X GET [URL_REMOVED]

Python API (Offline Inference):
Для использования в скриптах (например, в циклах RLHF) доступен прямой Python API 2:


Python




from vllm import LLM, SamplingParams

# Инициализация LLM с поддержкой sleep mode
llm = LLM("Qwen/Qwen2.5-0.5B-Instruct", enable_sleep_mode=True)
sampling_params = SamplingParams(temperature=0, max_tokens=10)
prompt = "Hello, my name is"

# 1. Генерация
output1 = llm.generate(prompt, sampling_params)

# 2. Перевод в сон (Уровень 1: веса на CPU, KV-кэш отброшен)
llm.sleep(level=1)

#... (VRAM свободна, можно выполнять другие GPU-задачи)...

# 3. Пробуждение (веса восстанавливаются из CPU)
llm.wake_up()

# 4. Повторная генерация для верификации
output2 = llm.generate(prompt, sampling_params)

assert output1.outputs.text == output2.outputs.text

2


2.2. Механика: Уровень 1 (L1) vs. Уровень 2 (L2)


vLLM предлагает два различных уровня гибернации, предназначенных для разных сценариев использования. Понимание их различий имеет решающее значение для проектирования системы.5
   * Level 1 (L1) Sleep:
   * Действие: Выгружает веса модели из VRAM в CPU RAM и отбрасывает (discards) KV-кэш.5
   * Пробуждение: Восстанавливает веса из CPU RAM обратно в VRAM.2
   * Сценарий: Идеально подходит для временного освобождения GPU для выполнения других задач (например, пакетной аналитики) с намерением позже возобновить работу с той же моделью.2
   * Требование: Требует наличия достаточного объема CPU RAM для хранения полной копии весов модели.2
   * Level 2 (L2) Sleep:
   * Действие: Полностью отбрасывает (discards) как веса модели, так и KV-кэш из VRAM.2 Буферы модели (например, тензоры масштабирования RoPE) могут оставаться в CPU.2
   * Пробуждение: Движок готов к загрузке новых весов (либо тех же самых с диска, либо обновленных весов в сценарии RLHF).2
   * Сценарий: Оптимален для сценариев, требующих смены модели (загрузка другой модели в тот же "теплый" движок) или для циклов обучения (RLHF, DPO), где старые веса больше не нужны и будут заменены обновленными.2
   * Требование: Не требует значительного CPU RAM для хранения весов, что делает его идеальным для сред с ограниченным объемом системной памяти.1
Сравнительная таблица L1 vs L2 vs "No Sleep" (полная перезагрузка):
Метрика
	No Sleep (Холодный старт)
	Level 1 Sleep
	Level 2 Sleep
	Действие
	vllm serve...
	llm.sleep(level=1)
	llm.sleep(level=2)
	Веса Модели (в VRAM)
	N/A (Загружаются)
	Выгружены на CPU
	Отброшены (Discarded)
	KV-кэш (в VRAM)
	N/A (Создается)
	Отброшен (Discarded)
	Отброшен (Discarded)
	Потребление CPU RAM
	Минимальное
	Высокое (~GB на модель)
	Минимальное (~MB на модель)
	Время пробуждения
	N/A (Полная перезагрузка)
	Очень быстрое (0.26s / 0.82s)
	Быстрое (0.85s / 2.58s)
	Общее время (Пример)
	357.1 с
	112.6 с
	124.6 с
	Лучший Сценарий
	Одиночная модель, без смены
	Частая смена (при наличии RAM)
	Ограниченная RAM, смена моделей
	Данные о времени и потреблении основаны на эталонных тестах vLLM.1


2.3. Секрет Производительности: Что Сохраняется


Ключевое преимущество "Sleep Mode" (даже L2) перед полным "холодным" перезапуском заключается не только в управлении весами. "Холодный" старт несет в себе огромные неявные издержки, которых "Sleep Mode" позволяет избежать.1
При вызове llm.sleep() (L1 или L2), процесс vLLM и его базовый движок остаются в "теплом" состоянии. В частности, vLLM сохраняет 1:
   1. Состояние Процесса (Process State): Сам Python-процесс и его CUDA-контекст не уничтожаются.
   2. Аллокаторы Памяти (Allocator): Инициализированные пулы памяти vLLM (включая PagedAttention) остаются активными.
   3. Скомпилированные JIT-ядра (JIT Kernels): Оптимизированные CUDA-ядра, скомпилированные "на лету" (например, FlashInfer 9), сохраняются.
   4. Захваченные CUDA-графы (CUDA Graphs): Это наиболее важный компонент. vLLM выполняет "dummy run" при запуске для захвата всей последовательности вызовов GPU-ядер в виде CUDA-графа.10 Этот граф затем "переигрывается" при каждом шаге инференса, что радикально снижает задержки на запуск ядер.10 "Sleep Mode" сохраняет эти захваченные графы в VRAM.11
При "холодном" старте (убийство и перезапуск сервера) все эти четыре компонента должны быть созданы с нуля, что занимает десятки секунд. "Sleep Mode" L2, хотя и требует загрузки весов с диска (как и "холодный" старт), пропускает все эти шаги ре-инициализации. Именно поэтому тесты показывают, что L2-пробуждение обеспечивает инференс на 61-88% быстрее, чем "холодный" старт, и в целом на 2.9x быстрее для всей рабочей нагрузки.1


2.4. Ограничения и Риски


Внедрение "Sleep Mode" требует понимания его ограничений и потенциальных рисков.
   * Критический Риск Безопасности (VLLM_SERVER_DEV_MODE):
Как упоминалось, флаг VLLM_SERVER_DEV_MODE необходим для активации API.5 Этот флаг также активирует другие эндпоинты для отладки, например /reset_prefix_cache.3 В документации прямо указано: "Это риск безопасности. Должно быть включено только в доверенных средах".3 Это связано с тем, что vLLM и PyTorch Distributed по умолчанию не имеют протоколов авторизации или шифрования для меж-узлового взаимодействия.13
Рекомендация: Серверы vLLM, использующие VLLM_SERVER_DEV_MODE, должны быть развернуты в полностью изолированной сети (например, приватная VPC или подсеть), доступ к которой имеет только внутренний оркестратор MVP.
   * Требования к Платформе (CUDA):
Официальная документация vLLM (например, в дистрибутивах Red Hat AI) указывает, что --enable-sleep-mode "Поддерживается только для платформы CUDA".3 Хотя существуют форки vLLM с поддержкой "Sleep Mode" для других аппаратных средств, таких как Huawei Ascend NPU 5, основная реализация ориентирована на CUDA.
   * Требования к Ресурсам (L1):
Уровень L1 не является "бесплатным". Он переносит нагрузку с VRAM на CPU RAM. Инженеры должны убедиться, что хост-машина имеет достаточно системной памяти для хранения полных весов модели.2
   * Риски Стабильности и Баги:
"Sleep Mode" — это сложная функция, затрагивающая низкоуровневое управление памятью. Исторически наблюдались проблемы:
      * Несовместимость с AsyncLLM: Сообщалось о проблемах при использовании AsyncLLM (асинхронного движка) с режимами sleep/wake_up.15
      * Баг буферов L2: В прошлом существовал баг, при котором L2-сон некорректно восстанавливал буферы модели (например, running mean/var в BatchNorm), что приводило к ошибкам при перезагрузке весов. Это было исправлено (PR #16889), но демонстрирует сложность функции.8
      * Ошибки управления памятью: На некоторых аппаратных платформах (особенно не-NVIDIA) наблюдалась ошибка "Memory usage increased after sleeping", когда внутренние проверки vLLM (asserts) не срабатывали из-за особенностей управления памятью драйвером.16


3. Категория 2: Интеграция NVIDIA nvCOMP для Сжатия Чекпоинтов


Для минимизации времени простоя при использовании "Sleep Mode" L2 (смена модели) критически важно максимально ускорить загрузку новых весов. Вместо загрузки терабайтов несжатых тензоров (чекпоинтов) с диска, SOTA-подход заключается в использовании GPU-ускоренного сжатия.


3.1. Обзор SOTA: Библиотека NVIDIA nvCOMP


NVIDIA nvCOMP — это высокопроизводительная библиотека для беспотерьного сжатия и декомпрессии данных, оптимизированная для выполнения непосредственно на GPU.17
      * Ключевые Особенности:
      * Поддержка форматов: Включает стандартные форматы (Snappy, ZSTD, Deflate, LZ4) и высокооптимизированные для GPU форматы (Bitcomp, GDeflate, gANS, Cascaded).17
      * Python API: Предоставляет Python API для "упрощенной интеграции и взаимодействия" с фреймворками, такими как PyTorch.17
      * Готовность к Blackwell: Оптимизирован для использования нового аппаратного движка декомпрессии (DE) в архитектуре NVIDIA Blackwell (см. Раздел 3.3).17


3.2. Практическая Реализация: Сжатие PyTorch тензоров (Zero-Copy)


Ключевым механизмом для интеграции nvCOMP с PyTorch является протокол __cuda_array_interface__. Этот стандартный интерфейс 18 позволяет PyTorch GPU-тензору предоставлять свой указатель на VRAM, форму и тип данных другим библиотекам, таким как nvCOMP, без какого-либо копирования данных.
API nvcomp.as_array() разработан для "обертывания" внешних буферов, которые предоставляют этот интерфейс.20 Это позволяет выполнять сжатие "на месте" (in-place) или, точнее, VRAM-to-VRAM (GPU-тензор -> сжатый GPU-буфер).
Пример кода: GPU-сжатие PyTorch state_dict (тензор за тензором):
Следующий код демонстрирует полный цикл сжатия и декомпрессии тензора float16 (типичный для весов LLM) на GPU.


Python




import torch
import numpy as np
from nvidia import nvcomp

# 1. Инициализация тензора PyTorch (веса модели) на GPU
# Используем float16, как в большинстве современных LLM
original_data_gpu = torch.randn(
   4096 * 4096, 
   dtype=torch.float16
).cuda()

# 2. Инициализация кодека nvCOMP
# ВАЖНО: Использовать array-protocol type strings [20]
# '<f2' = 2-байтовый (float16) little-endian float.
# '<f4' = 4-байтовый (float32) little-endian float.
# Ошибка: Использование 'f16' вызовет ошибку или приведение к uint8 [22, 31]
codec = nvcomp.Codec(algorithm="LZ4", data_type="<f2") 

print(f"Original tensor size: {original_data_gpu.nbytes / 1e6:.2f} MB")

# 3. Создание Zero-Copy обертки nvCOMP
# nvarr_d указывает на ту же VRAM, что и original_data_gpu [20]
nvarr_d = nvcomp.as_array(original_data_gpu)

# 4. Асинхронное сжатие на GPU (VRAM -> VRAM)
# com_arr - это nvcomp.Array, живущий в VRAM
com_arr = codec.encode(nvarr_d)

print(f"Compressed array size (in VRAM): {com_arr.buffer_size / 1e6:.2f} MB")

# 5. Декомпрессия на GPU (для верификации)
# decom_arr - это nvcomp.Array, живущий в VRAM
decom_arr = codec.decode(com_arr)

# 6. Верификация (обратное преобразование в PyTorch тензор)
# Этот шаг также использует __cuda_array_interface__ для zero-copy
decompressed_tensor_gpu = torch.as_tensor(
   decom_arr, 
   device=original_data_gpu.device
)

# 7. Проверка
assert torch.equal(original_data_gpu, decompressed_tensor_gpu)
print("Verification SUCCESS: Original and decompressed tensors match.")


20
Этот воркфлоу позволяет асинхронно сжимать state_dict модели на GPU и сохранять сжатые байты на диск (NVMe/S3), не затрагивая CPU и не насыщая шину PCIe несжатыми данными.


3.3. Преимущество Blackwell: Аппаратный движок декомпрессии (DE)


Архитектура NVIDIA Blackwell (B100/B200), анонсированная на GTC 2024/2025 23, представляет фундаментальное изменение для этого воркфлоу. Blackwell включает выделенный аппаратный движок декомпрессии (Decompression Engine, DE).17
      * Как это работает: DE — это аппаратный блок с фиксированной функцией, интегрированный в "copy engine" (движок копирования) GPU.25 Это позволяет выполнять "fused copy-decompress operations" (слияные операции копирования-декомпрессии).17
      * Сценарий (H100/Hopper): (1) Скопировать сжатые данные (с NVMe/S3) в VRAM. (2) Запустить CUDA-ядра (на SM) для декомпрессии (VRAM -> VRAM).
      * Сценарий (B100/Blackwell): (1) Выполнить одну операцию "копирования-и-декомпрессии", которая читает сжатые данные и записывает уже декомпрессированные данные напрямую в VRAM, обходя SM.25
Асимметричное Преимущество: Важно отметить, что это Decompression Engine. Он аппаратно ускоряет codec.decode(), но не codec.encode(). Эта асимметрия идеально соответствует сценарию инференса: "сжать один раз" (офлайн, используя SM) и "распаковывать много раз" (при загрузке модели, используя аппаратный DE).
Производительность и Прозрачность API:
Аппаратный DE обеспечивает пропускную способность декомпрессии до 600 GB/s.17 В реальных тестах (Polars PDS benchmark на B100) использование аппаратного DE показало "на 35% более быстрое время выполнения end-to-end" по сравнению с программной декомпрессией на CUDA-ядрах.27
Самое важное для MVP-команды: библиотека nvCOMP обеспечивает бесшовную интеграцию с DE.26 Код, написанный для Hopper (Раздел 3.2), автоматически и без изменений будет использовать аппаратный DE при запуске на Blackwell. Это обеспечивает будущую готовность (future-proof) решения по управлению чекпоинтами.


4. Стратегический план интеграции: Соединение vLLM "Sleep Mode" и nvCOMP


Текущие SOTA-инструменты для сжатия моделей, такие как vllm-project/llm-compressor, в основном фокусируются на квантизации (например, FP4, INT8, GPTQ).28 Интеграция беспотерьного сжатия (lossless compression) чекпоинтов с помощью nvCOMP для быстрой смены L2-спящих моделей представляет собой новую и высокоэффективную стратегию.


4.1. Проектирование воркфлоу: Высокопроизводительная смена моделей (L2 + nvCOMP)


Этот воркфлоу описывает замену Модели А (активна) на Модель Б (хранится в сжатом виде) с использованием L2 Sleep и nvCOMP.
      1. Шаг 1 (Сон): Внешний оркестратор MVP (или Kubernetes-контроллер) определяет необходимость смены модели и отправляет HTTP-запрос POST /sleep -d '{"level": 2"}' на эндпоинт vLLM-сервера Модели А.5
      2. Шаг 2 (vLLM Освобождение VRAM): Движок vLLM Модели А немедленно отбрасывает свои веса и KV-кэш из VRAM.2 Более 90% VRAM освобождается.2 Критически важно, что CUDA-графы, JIT-ядра и аллокаторы Модели А сохраняются.1
      3. Шаг 3 (Загрузка): Оркестратор MVP инициирует чтение сжатого чекпоинта Модели Б (например, model_B.safetensors.lz4) с быстрого NVMe-хранилища или S3 в pinned (закрепленную) CPU память.
      4. Шаг 4 (Декомпрессия): Оркестратор вызывает nvcomp.decode_async(), используя GPU (на H100) или аппаратный DE (на B100).25 Происходит fused copy-decompress: сжатые данные передаются по шине PCIe и записываются в VRAM уже в декомпрессированном виде.
      5. Шаг 5 (Пробуждение): Оркестратор вызывает POST /wake_up. Движок vLLM реинициализирует свои внутренние указатели, но не CUDA-контекст или CUDA-графы.
      6. Шаг 6 (Загрузка весов): Оркестратор (через внутренний API vLLM, аналогичный llm.load_weights, упомянутому в контексте RLHF 7) указывает движку vLLM на уже находящийся в VRAM тензор с весами Модели Б.
      7. Результат: Модель Б активна. Весь процесс "холодного" старта (30-100+ секунд 1) был заменен на <1-2 секунды I/O и декомпрессии (в зависимости от размера модели и пропускной способности I/O).


4.2. Синергия L2 Sleep и nvCOMP для динамических рабочих нагрузок (RLHF/DPO)


В сценариях обучения с подкреплением (PPO, DPO) 5 vLLM и PyTorch (FSDP) должны быстро обмениваться контролем над VRAM.
      1. Шаг 1 (Инференс): outputs = llm.generate(...). vLLM генерирует ответы, используя текущие веса (политику).
      2. Шаг 2 (Сон): llm.sleep(level=2). vLLM мгновенно освобождает VRAM, отбрасывая веса и KV-кэш.8
      3. Шаг 3 (Тренировка): optimizer.step(). PyTorch/FSDP выполняет шаг оптимизации, используя всю доступную VRAM для вычисления градиентов и обновления весов.
      4. Шаг 4 (Сжатие и Чекпоинт - Параллельно): Новые, обновленные веса (model.state_dict()) обертываются с помощью nvcomp.as_array() и асинхронно сжимаются на GPU через codec.encode(). Сжатый буфер асинхронно записывается на диск.
      5. Шаг 5 (Пробуждение): llm.wake_up(). vLLM "просыпается", готовый принять новые веса.8
      6. Шаг 6 (Обновление весов): llm_engine.model_executor...load_weights(...). vLLM загружает новые веса (из Шага 3) из тензора PyTorch, который уже находится в VRAM.7 Цикл возвращается к Шагу 1.
В этом воркфлоу "Sleep Mode" L2 обеспечивает критически важное "переключение контекста" VRAM между инференсом и тренировкой. nvCOMP (Шаг 4) играет вспомогательную, но ключевую роль: он позволяет создавать чекпоинты (дампы) модели после каждого шага тренировки с использованием GPU-сжатия, не блокируя CPU и не насыщая шину PCIe, что было бы невозможно при сохранении несжатых весов.


5. Итоговые рекомендации и дорожная карта внедрения


      * Фаза 1: Немедленное внедрение MVP (Безопасность и L1)
      1. Активировать --enable-sleep-mode 3 и VLLM_SERVER_DEV_MODE=1 5 на всех серверах vLLM, предназначенных для динамической нагрузки.
      2. Немедленно: Изолировать все vLLM-серверы с VLLM_SERVER_DEV_MODE=1 в приватной VPC/подсети. Создать внутренний API-шлюз/оркестратор, который является единственным сервисом, имеющим право вызывать эндпоинты .../sleep, /wake_up, /is_sleeping.13
      3. Внедрить L1 Sleep (level=1) 5 для базовых сценариев "освобождения VRAM" (например, для запуска пакетных задач аналитики или CUDA-тестов на том же GPU), обеспечив наличие достаточного CPU RAM.
      * Фаза 2: Продвинутая интеграция (L2 + nvCOMP)
      1. Выделить инженерные ресурсы (1-2 инженера, 2-3 спринта) для создания "Менеджера Чекпоинтов".
      2. Этот сервис должен:
      * Читать state_dict PyTorch или safetensors.
      * Сжимать тензоры на GPU с использованием Python API nvcomp (используя код из Раздела 3.2).22
      * Управлять сохранением и извлечением сжатых чекпоинтов из бэкенда хранения (NVMe/S3).
      3. Реализовать полный воркфлоу, описанный в Разделе 4.1, для быстрой смены моделей.
      4. Провести бенчмаркинг времени "L2-wake-and-load" в зависимости от размера модели и формата сжатия (LZ4 для скорости vs ZSTD для степени сжатия).
      * Фаза 3: Оптимизация под Blackwell
      1. При развертывании MVP на оборудовании NVIDIA Blackwell, никаких изменений кода nvcomp не потребуется.26
      2. Провести повторный бенчмаркинг Фазы 2. Ожидается 30-35% ускорение (или более) 27 времени "L2-wake-and-load" (Шаг 4 из Раздела 4.1) за счет автоматического использования аппаратного движка декомпрессии (DE).17
      * Тестирование и Риски: Выделить специальное время для тестирования стабильности AsyncLLM в связке с sleep() 15 и проверки отсутствия регрессий, связанных с восстановлением буферов модели.8


6. Приложение: Справочник по коду




6.1. Примеры конфигурации и вызовов API vLLM "Sleep Mode"


Запуск сервера vLLM (Docker/CLI):


Bash




# Устанавливаем ENV-переменную для включения эндпоинтов /sleep, /wake_up
export VLLM_SERVER_DEV_MODE=1

# Запускаем сервер vLLM (пример для Phi-3-vision)
vllm serve microsoft/Phi-3-vision-128k-instruct \
 --enable-sleep-mode \
 --port 8001

1
HTTP API curl (Управление сервером):


Bash




# Перевести в сон L1 (Веса на CPU)
curl -X POST [URL_REMOVED] -H "Content-Type: application/json" -d '{"level": "1"}'

# Проверить статус
curl -X GET [URL_REMOVED]
# Ожидаемый ответ: {"is_sleeping": true, "sleep_level": 1}

# Пробудить
curl -X POST [URL_REMOVED]

# Проверить статус
curl -X GET [URL_REMOVED]
# Ожидаемый ответ: {"is_sleeping": false, "sleep_level": 0}

# Перевести в сон L2 (Отбросить веса)
curl -X POST [URL_REMOVED] -H "Content-Type: application/json" -d '{"level": "2"}'

5
Python API (Offline/RLHF):


Python




from vllm import LLM

# Инициализация с поддержкой сна
llm = LLM("model_name", enable_sleep_mode=True)

#... работа...

# Перевод в сон L1
llm.sleep(level=1)

#... VRAM свободна...

# Пробуждение
llm.wake_up()

#... работа...

2


6.2. Пример кода: Сжатие/Декомпрессия PyTorch-тензоров с nvCOMP (GPU-to-GPU)


Этот скрипт демонстрирует end-to-end процесс GPU-ускоренного сжатия и декомпрессии тензора torch.float16 с использованием zero-copy интерфейсов.


Python




import torch
import numpy as np
from nvidia import nvcomp

# --- Конфигурация ---
# Алгоритм сжатия (LZ4 быстрый, ZSTD/gANS сжимают лучше)
COMPRESSION_ALGORITHM = "LZ4"
# Тип данных тензора PyTorch
TORCH_DTYPE = torch.float16
# Соответствующий array-protocol string 
# <f2 = float16, <f4 = float32, <i8 = int64
NVCOMP_DATA_TYPE = "<f2" 

# Проверка доступности CUDA
if not torch.cuda.is_available():
   print("CUDA недоступна. nvCOMP требует наличия GPU NVIDIA.")
   exit()

print(f"PyTorch CUDA device: {torch.cuda.get_device_name(0)}")
print(f"nvCOMP version: {nvcomp.__version__}")

# 1. Создание исходного тензора (например, один слой из state_dict)
original_data_gpu = torch.rand(
   (16384, 16384), # 256M элементов
   dtype=TORCH_DTYPE, 
   device="cuda"
)
original_size_bytes = original_data_gpu.nbytes
print(f"\n[1. Исходные данные] \n  Тип: {original_data_gpu.dtype} \n  Размер: {original_size_bytes / 1e6:.2f} MB")

# 2. Инициализация кодека nvCOMP
# Используем корректный data_type для float16
try:
   codec = nvcomp.Codec(
       algorithm=COMPRESSION_ALGORITHM, 
       data_type=NVCOMP_DATA_TYPE
   )
   print(f"[2. Инициализация кодека] \n  Алгоритм: {COMPRESSION_ALGORITHM} \n  Тип данных: {NVCOMP_DATA_TYPE}")
except ValueError as e:
   print(f"Ошибка инициализации кодека: {e}")
   print("Убедитесь, что используете правильный data_type, например '<f2>' для float16.")
   exit()

# 3. Zero-Copy обертка (PyTorch Tensor -> nvcomp.Array)
# Это не копирует данные, nvarr_d просто указывает на VRAM тензора
nvarr_d = nvcomp.as_array(original_data_gpu)
print("[3. Zero-Copy] \n  Создана обертка nvcomp.as_array()
")

# 4. Сжатие на GPU (VRAM -> VRAM)
# com_arr - это новый nvcomp.Array в VRAM, содержащий сжатые данные
com_arr = codec.encode(nvarr_d)
compressed_size_bytes = com_arr.buffer_size
compression_ratio = original_size_bytes / compressed_size_bytes
print(f"[4. Сжатие GPU] \n  Сжатый размер (VRAM): {compressed_size_bytes / 1e6:.2f} MB \n  Коэффициент сжатия: {compression_ratio:.2f}x")

# --- В этот момент com_arr.cpu() можно сохранить на диск ---
# compressed_bytes_host = com_arr.cpu() 
# with open("model_layer.safetensors.lz4", "wb") as f:
#     f.write(compressed_bytes_host)
# ---

# 5. Декомпрессия на GPU (VRAM -> VRAM)
# На Blackwell этот шаг будет использовать аппаратный DE 
decom_arr = codec.decode(com_arr)
print(f"[5. Декомпрессия GPU] \n  Размер в VRAM: {decom_arr.buffer_size / 1e6:.2f} MB")

# 6. Верификация (nvcomp.Array -> PyTorch Tensor)
# Этот шаг также zero-copy, используя __cuda_array_interface__
decompressed_tensor_gpu = torch.as_tensor(
   decom_arr, 
   device=original_data_gpu.device
)

# 7. Проверка
print("[6. Верификация]")
if torch.equal(original_data_gpu, decompressed_tensor_gpu):
   print("  SUCCESS: Исходный и декомпрессированный тензоры полностью совпадают.")
else:
   print("  FAILURE: Тензоры не совпадают.")

# Сравнение форм
assert original_data_gpu.shape == decompressed_tensor_gpu.shape
assert original_data_gpu.dtype == decompressed_tensor_gpu.dtype


20
Источники
      1. Zero-Reload Model Switching with vLLM Sleep Mode, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      2. Sleep Mode - vLLM, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      3. vLLM server arguments | Red Hat AI Inference Server | 3.0 - Red Hat Documentation, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      4. Chapter 2. Complete list of vLLM server arguments - Red Hat Documentation, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      5. Sleep Mode Guide - vllm-ascend - Read the Docs, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      6. vLLM server arguments | Red Hat AI Inference Server | 3.1, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      7. [RFC]: Better support for weight updating while waking up from sleep mode for RLHF #15254 - GitHub, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      8. No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL - Hugging Face, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      9. Run High-Performance LLM Inference Kernels from NVIDIA Using FlashInfer, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      10. Inside vLLM: Anatomy of a High-Throughput LLM Inference System, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      11. [Doc]: GPU memory conflict between sleeping and woken vLLM instances #14644 - GitHub, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      12. Fix #15483 : Add error handling for model-dependent endpoints during sleep mode #16536, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      13. Security - vLLM, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      14. Security Guide - vLLM, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      15. [Bug]: AsyncLLM sleep then wake_up produces meaningless outputs · Issue #17103 · vllm-project/vllm - GitHub, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      16. What situations could cause the 'Memory usage increased after sleeping' problem?, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      17. NVIDIA nvCOMP - NVIDIA Developer, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      18. CUDA Array Interface (Version 3) — Numba CUDA - GitHub Pages, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      19. CUDA Array Interface (Version 2) - Numba, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      20. Python API — nvCOMP - NVIDIA Docs, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      21. nvCOMP Python API Basics — nvCOMP - NVIDIA Docs Hub, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      22. nvComp: Unknown output data type · Issue #235 · NVIDIA ... - GitHub, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      23. GTC 2025 - NVIDIA Investor Relations, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      24. GTC March 2025 Keynote with NVIDIA CEO Jensen Huang - YouTube, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      25. Speeding Up Data Decompression with nvCOMP and the NVIDIA Blackwell Decompression Engine | NVIDIA Technical Blog, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      26. Speeding Up Data Decompression with nvCOMP and the NVIDIA Blackwell Decompression Engine | daily.dev, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      27. RAPIDS Brings Zero-Code-Change Acceleration, IO Performance Gains, and Out-of-Core XGBoost | NVIDIA Technical Blog, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      28. Issues · vllm-project/llm-compressor - GitHub, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      29. vllm-project/llm-compressor: Transformers-compatible library for applying various compression algorithms to LLMs for optimized deployment with vLLM - GitHub, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]
      30. NVFP4 Quant exception · Issue #1590 · vllm-project/llm-compressor - GitHub, дата последнего обращения: ноября 8, 2025, [URL_REMOVED]