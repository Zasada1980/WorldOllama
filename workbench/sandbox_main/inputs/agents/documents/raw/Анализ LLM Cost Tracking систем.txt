Анализ полноты систем отслеживания затрат LLM для мультиагентных приложений: LangSmith, Helicone и OpenAI API




РАЗДЕЛ I. ВВЕДЕНИЕ: НОВАЯ ЭРА СЛОЖНОСТИ В УПРАВЛЕНИИ ЗАТРАТАМИ НА LLM




A. Определение проблемы: От Запроса-Ответа к Мультиагентным "Черным Ящикам"


Ранние приложения на базе больших языковых моделей (LLM) характеризовались простой и предсказуемой юнит-экономикой: один запрос пользователя приводил к одному вызову API и, следовательно, к одной прогнозируемой стоимости. Однако современная экосистема быстро сместилась в сторону мультиагентных систем. В этих сложных архитектурах один запрос конечного пользователя может инициировать непредсказуемую цепную реакцию внутренних вызовов, в которой несколько программных агентов (планировщики, исполнители, валидаторы) взаимодействуют, вызывают инструменты и рекурсивно обращаются к LLM для выполнения одной задачи.
Это превратило приложения в "черные ящики" 1, сделав отслеживание затрат фундаментальной проблемой и "препятствием номер один для надежного развертывания".1 Вопрос "сколько стоит один ответ?" 2 стал нетривиальным и критически важным для бизнеса. Как следствие, на рынке наблюдается явный сдвиг: фокус разработчиков сместился от простого "создания агентов" к острой необходимости в "управлении агентами", что подтверждается многочисленными запросами сообщества на инструменты для гранулярного отслеживания затрат "на пользователя", "на чат" или "на выполнение рабочего процесса агента".3


B. Методология: Три уровня иерархии контроля затрат


Запрошенный анализ охватывает три инструмента, которые, на первый взгляд, являются конкурентами, но фундаментально представляют три различных архитектурных уровня контроля затрат. Понимание этого различия имеет решающее значение для оценки их "полноты":
1. Уровень API (Источник): OpenAI usage API. Это необработанный "источник истины" для затрат, генерируемых моделями OpenAI. Он предоставляет данные о биллинге постфактум, но требует значительных усилий по самостоятельной разработке (DIY) для агрегации и анализа.4
2. Уровень Наблюдаемости (Observability): LangSmith. Это комплексная платформа для мониторинга производительности приложений (APM), аналогичная Datadog или New Relic, но для LLM. Она интегрируется с приложением (часто через LangChain/LangGraph) для наблюдения и трассировки выполнения, добавляя метаданные о затратах к этим трассам.5
3. Уровень Шлюза (Gateway): Helicone. Это прокси-сервер (API Gateway), который находится между приложением и провайдерами LLM. Он перехватывает запросы, что позволяет ему не только пассивно наблюдать, но и активно контролировать, кэшировать, маршрутизировать и модифицировать их.6
Этот отчет будет оценивать каждый из этих архитектурных подходов на соответствие четырем ключевым критериям, определенным в запросе. Анализ выявит, что выбор инструмента — это, по сути, выбор архитектурной точки контроля (биллинг, приложение или сеть), и у каждого выбора есть глубокие компромиссы с точки зрения точности, гранулярности и контроля.


C. Структура отчета


Отчет последовательно анализирует каждый из трех продуктов по четырем критериям:
* Критерий 1: Калькулятор затрат и точность учета токенов.
* Критерий 2: Агрегация затрат (на пользователя, задачу, агента).
* Критерий 3: Управление бюджетом и системы оповещений.
* Критерий 4: Стратегии и инструменты оптимизации затрат.
Завершающий раздел синтезирует эти выводы, предлагая сводную оценку и стратегические рекомендации по выбору архитектуры для управления затратами в производственных мультиагентных системах.


РАЗДЕЛ II. КРИТЕРИЙ 1: КАЛЬКУЛЯТОР ЗАТРАТ И ТОЧНОСТЬ УЧЕТА ТОКЕНОВ


Основой любой системы отслеживания затрат является ее способность точно подсчитывать стоимость. Этот критерий оценивает, как каждая система справляется с расчетом стоимости на основе токенов и насколько эти расчеты точны.


A. Основа (Ground Truth): OpenAI API и Прямой Расчет


Провайдеры LLM, такие как OpenAI, предоставляют подробные прайс-листы, которые служат "основной истиной" для всех расчетов.8 Однако эта "истина" сложна. Ключевым нововведением является дифференцированное ценообразование на "Cached input" (кэшированные входные токены). Например, OpenAI может взимать $1.250 / 1M$ токенов за обычный ввод и всего $0.125 / 1M$ токенов (т.е. со скидкой 90%) за кэшированный ввод.8 Это имеет огромное значение для RAG-приложений (Retrieval-Augmented Generation) с длинными, повторяющимися контекстами. Разработчики могут программно получать данные о затратах через Usage API 4 или просматривать их на панели управления 4, но ответственность за точный расчет с учетом кэширования ложится на них.


B. LangSmith: Расчет на основе наблюдаемости


LangSmith отслеживает затраты на основе токенов, которые либо "получены из подсчета токенов и цен на модели", либо "напрямую указаны" в данных запуска.11 Эта система хорошо работает для простых сценариев и предполагает линейное ценообразование. Для моделей с нелинейным ценообразованием LangSmith рекомендует вычислять затраты на стороне клиента.11
Однако здесь выявляется критический недостаток в полноте данных LangSmith. Платформа, работая на уровне приложения, не имеет возможности узнать, был ли конкретный запрос обслужен из кэша провайдера (например, кэша OpenAI). В результате она по умолчанию предполагает полную стоимость. Эта проблема была прямо отмечена в открытом запросе на GitHub 12, где разработчик заявил: "в настоящее время... любой, кто использует калькулятор цен Langsmith... получает совершенно неверные затраты".12
Это означает, что LangSmith по своей конструкции является инструментом оценки затрат (cost estimation) для отладки (debugging), а не инструментом финансового учета (financial accounting). Его данные о затратах нельзя использовать для сверки счетов OpenAI, если используется кэширование на стороне провайдера.


C. Helicone: Расчет на основе шлюза


Helicone, будучи прокси-шлюзом, занимает принципиально иную архитектурную позицию. Он не просто наблюдает за вызовами, он их контролирует. Во-первых, Helicone поддерживает "Реестр моделей" (Model Registry) с ценами в реальном времени для более чем 300 моделей от различных провайдеров.13 Во-вторых, и это самое важное, Helicone сам реализует собственный слой кэширования.7
Поскольку Helicone активно управляет кэшем, он абсолютно точно знает, был ли запрос обслужен из кэша (и, следовательно, не был отправлен провайдеру) или нет. Это позволяет ему рассчитывать реальную экономию и предоставлять финансово точные данные.7 Таким образом, архитектура шлюза по своей природе более точна для расчета затрат, чем архитектура наблюдаемости.


Таблица 1. Сравнительный анализ точности калькуляции затрат




Параметр
	OpenAI API
	LangSmith (Observability)
	Helicone (Gateway)
	Метод расчета
	Данные биллинга API
	Вывод (Токены * Цена)
	Перехват (Gateway)
	Поддержка >1 провайдера
	Нет (Только OpenAI)
	Да (Требует настройки)
	Да (Встроено, 300+ моделей)
	Учет кэширования (Провайдер)
	Только вручную
	Нет (Критический пробел) 12
	Да (Если управляется Helicone)
	Учет кэширования (Собственное)
	Н/П
	Н/П
	Да (Ключевая функция) 7
	Оценка точности (FinOps)
	Высокая (Источник)
	Низкая (Только оценка)
	Высокая (Для FinOps)
	

РАЗДЕЛ III. КРИТЕРИЙ 2: АГРЕГАЦИЯ ЗАТРАТ В МУЛЬТИАГЕНТНЫХ СИСТЕМАХ


Это наиболее важный критерий для мультиагентных приложений. Он требует возможности детализировать совокупные затраты по трем измерениям: "на пользователя", "на задачу" и "на агента".


A. Основы: Агрегация "на пользователя" (Per-User)


Все три системы предоставляют базовые механизмы для этого:
* OpenAI API: Поддерживает параметр user в вызовах API.15 Новое Usage API 17 и панель управления 18 позволяют фильтровать и группировать затраты по этому идентификатору.
* LangSmith: Позволяет разработчикам передавать user_id в качестве метаданных.19 Панели мониторинга поддерживают "Group by" по этому полю.20
* Helicone: Использует "Custom Properties" (Кастомные свойства).21 Передавая собственный заголовок, такой как Helicone-Property-UserID или Helicone-Property-UserTier 13, можно фильтровать, группировать и анализировать затраты на пользователя.22


B. Продвинутая агрегация: "на задачу" и "на агента" (Per-Task / Per-Agent)


Здесь проявляются фундаментальные архитектурные различия и решается ядро мультиагентной проблемы.
* OpenAI API: Полностью отсутствует. API не имеет концепции "задачи" или "агента". Единственный обходной путь, предложенный в сообществе, — это создание отдельных API-ключей для каждого агента, проекта или пользователя 18, что является громоздким и не масштабируемым решением.
* LangSmith (Подход "Трассировка"): Это главная сила LangSmith. Благодаря тесной интеграции с LangChain и LangGraph, он автоматически создает иерархические трассировки (traces) для каждого вызова, показывая каждый шаг, который предпринял агент.23 Для LangGraph он может отслеживать стоимость каждого узла (node execution) в графе.24 Это обеспечивает чрезвычайно гранулярную атрибуцию затрат "на агента" (стоимость узла) и "на задачу" (общая стоимость трассировки).11
* Helicone (Подход "Шлюз/Сессия"): Helicone, будучи фреймворк-агностиком, использует два механизма:
   1. Custom Properties (для "агента"): Разработчик должен вручную добавить заголовок к каждому вызову API, идентифицирующий агента, например Helicone-Property-AgentName: "PlannerAgent".21 Это требует дополнительной работы по инструментированию кода.
   2. Sessions (для "задачи"): Helicone позволяет группировать несколько вызовов API (возможно, от разных агентов) в одну логическую "Сессию" с помощью заголовка Helicone-Session-Id.13 Это идеально подходит для ответа на вопрос "сколько стоило полное выполнение этой задачи?".1


C. Анализ полноты и конкурирующие философии


LangSmith и Helicone представляют две конкурирующие философии агрегации:
1. LangSmith ("White-Box"): Благодаря тесной интеграции с фреймворком LangChain, LangSmith автоматически понимает внутреннюю структуру графа агентов и предоставляет трассировку "из коробки".23 Это подход "белого ящика".
2. Helicone ("Black-Box"): Helicone не знает и не заботится о том, как устроен ваш агент (будь то LangChain, AutoGen или собственный код).1 Он видит только входящие и исходящие вызовы API. Разработчик должен вручную пометить эти вызовы с помощью заголовков, чтобы Helicone мог сгруппировать их.13 Это подход "черного ящика".
LangSmith проще в настройке для пользователей LangChain, но создает сильную привязку к поставщику (vendor lock-in). Helicone требует больше ручной работы по маркировке, но обеспечивает гибкость и работает с любым фреймворком.
Однако здесь кроется фундаментальное противоречие в ценообразовании LangSmith. Сила LangSmith — это гранулярная трассировка "на узел".25 Но его ценовая модель для развертываний (Deployment) взимает плату в размере $0.001 за выполнение узла ($0.001 per node executed).24 Мультиагентная задача может легко включать 10-20 узлов. Как цитируется в источнике 28, пользователь Reddit жаловался, что эта плата "эффективно удваивает мои COGS" (стоимость проданных товаров) и является "примерно в 10 раз выше, чем они ожидали".
Возникает парадокс: функция, которая делает LangSmith наиболее привлекательным для агрегации (трассировка узлов), одновременно делает его непомерно дорогим для использования в больших масштабах. Вы платите премию за наблюдение за вашими затратами, которая может быть сопоставима или даже превышать сами затраты на LLM.


Таблица 2. Матрица возможностей агрегации затрат




Измерение агрегации
	OpenAI API
	LangSmith (White-Box)
	Helicone (Black-Box)
	Агрегация "на пользователя"
	Да (параметр user) 15
	Да (метаданные user_id) 19
	Да (свойство Helicone-Property-UserID) 13
	Агрегация "на задачу"
	Нет
	Да (Автоматически) (Стоимость трассировки) 11
	Да (Вручную) (Заголовок Helicone-Session-Id) 13
	Агрегация "на агента"
	Нет
	Да (Автоматически) (Стоимость узла LangGraph) [24]
	Да (Вручную) (Свойство Helicone-Property-AgentName) 21
	Цена гранулярности
	Н/П
	Высокая ($0.001 / узел) 28
	Низкая (включено в стоимость)
	

РАЗДЕЛ IV. КРИТЕРИЙ 3: УПРАВЛЕНИЕ БЮДЖЕТОМ И СИСТЕМЫ ОПОВЕЩЕНИЙ


Этот критерий оценивает способность систем не только отслеживать расходы, но и контролировать их с помощью бюджетов и оповещений.


A. OpenAI: "Мягкие" лимиты и отсутствие контроля


OpenAI позволяет пользователям устанавливать мягкий месячный бюджет в своих настройках биллинга.8 Вы можете настроить оповещение по электронной почте при достижении определенного порога этого бюджета.8
Однако критический недостаток заключается в том, что это не "жесткий" лимит. Документация OpenAI 29 и пользовательские отчеты 8 подтверждают, что API не прекратит обслуживать запросы после достижения лимита. Пользователь несет полную ответственность за любой перерасход.8 Более того, эти лимиты нельзя установить программно через API.29


B. LangSmith: Оповещения о производительности, а не о затратах


LangSmith имеет мощную систему оповещений 5, интегрированную с веб-хуками 30 и другими сервисами. Однако, при ближайшем рассмотрении, эти оповещения предназначены для APM (Application Performance Monitoring), а не для FinOps (Financial Operations).
Документация и анонсы 31 четко указывают, что оповещения срабатывают на "уровень ошибок, задержку запуска и оценки обратной связи". Стоимость заметно отсутствует в этом списке.
LangSmith позволяет устанавливать "лимиты использования" (Usage Limits).34 Но документация 36 явно разъясняет: "Это лимиты на использование (количество трассировок), а не на расходы... В настоящее время вы не можете установить лимит расходов в продукте". Это фундаментальный пробел в полноте по данному критерию.


C. Helicone: Фокус на FinOps и контроле


В отличие от LangSmith, Helicone явно ориентирован на FinOps. Он предлагает:
1. Оповещения о расходах: Возможность настройки пороговых значений расходов для получения уведомлений.13
2. Регулярные отчеты: Ежедневные, еженедельные или ежемесячные отчеты о расходах, доставляемые по Email или в Slack.37
3. Контроль (Rate Limiting): Helicone, как шлюз, предлагает "Rate Limiting" (ограничение скорости запросов).14 Хотя это и не прямой бюджетный лимит, он может быть использован для реализации жесткого ограничения на количество вызовов, тем самым эффективно ограничивая максимальные затраты.


D. Сравнительный анализ с Open-Source (LiteLLM)


Чтобы подчеркнуть пробелы в коммерческих инструментах, полезно взглянуть на open-source решения. LiteLLM — это популярный прокси-сервер с открытым исходным кодом 39, который также предоставляет функции шлюза.
LiteLLM включает в себя BudgetManager 41 для установки и отслеживания бюджетов на пользователя, ключ API или команду.42 Самое главное, когда бюджет превышен, LiteLLM вызывает ошибку BudgetExceededError.41 Это и есть настоящий, программный "жесткий" лимит, которого не хватает и OpenAI, и LangSmith. Он также имеет комплексные оповещения о бюджете.44
Этот "рыночный провал" в коммерческих платформах, вероятно, не случаен. Модели ценообразования OpenAI и LangSmith (оплата за использование) напрямую конфликтуют с предоставлением клиентам инструментов для прекращения использования. Архитектура шлюза (Helicone, LiteLLM) по своей природе позволяет реализовать настоящие "жесткие" лимиты, которые необходимы для производственного контроля затрат.


РАЗДЕЛ V. КРИТЕРИЙ 4: СТРАТЕГИИ И ИНСТРУМЕНТЫ ОПТИМИЗАЦИИ ЗАТРАТ


Последний критерий оценивает, как платформы помогают активно снижать затраты, а не просто подсчитывать их. Существует два типа оптимизации: реактивная (помощь человеку в поиске проблем) и проактивная (автоматическое снижение затрат системой).


A. Реактивная оптимизация (Поиск проблем)


Этот подход заключается в предоставлении разработчикам данных для ручной оптимизации.
* LangSmith: Это основная функция LangSmith. Как инструмент наблюдаемости 5, он превосходно помогает "идентифицировать горячие точки затрат".45 Разработчики могут просматривать трассировки, видеть, какие шаги агента 23 или промты являются самыми дорогими, и вручную их оптимизировать.
* OpenAI: Предоставляет общие рекомендации, такие как минификация JSON (поскольку пробелы токенизируются и оплачиваются) 47, использование fine-tuning 49 или Batch API.50
* Helicone: Также предоставляет панели мониторинга для выявления неэффективности.7


B. Проактивная оптимизация (Автоматическое снижение затрат)


Этот подход включает автоматические системные функции, которые снижают затраты без ручного вмешательства. Здесь архитектурные различия становятся наиболее очевидными.
* Helicone: Это основное ценностное предложение Helicone.13 Будучи шлюзом, он может перехватывать запросы и выполнять две мощные проактивные оптимизации:
   1. Кэширование (Caching): Это самая мощная функция. Она включается одной строкой в заголовке (Helicone-Cache-Enabled: "true") 7 и может мгновенно сократить расходы на 15-30% 7 или даже 90% 51 для приложений с повторяющимися запросами. Helicone предоставляет сложные элементы управления, такие как Cache-Bucket-Max-Size (для недетерминированных промтов) и Cache-Seed (для кэшей на пользователя).52
   2. Маршрутизация (AI Gateway): Helicone может "интеллектуально направлять ваши запросы к оптимальной модели".6 Например, он может автоматически выбирать самую дешевую модель из своего реестра 13, которая соответствует заданным критериям производительности.
* LangSmith: Полностью отсутствует. LangSmith — это платформа наблюдаемости, а не шлюз. Он пассивен. Он не может перехватить, кэшировать или перенаправить запрос. Он записывает то, что произошло, но не вмешивается в это. Его поддержка кэширования 53 — это просто чтение данных о кэшировании, предоставляемых провайдером, что, как мы установили в Критерии 1, он делает неточно.12
* OpenAI: Предлагает кэширование на уровне провайдера.8 Однако это ручная оптимизация, требующая от разработчика управления ключами кэша, в отличие от автоматического кэширования на уровне прокси в Helicone.


C. Синтез: Наблюдаемость (APM) против Оптимизации (Gateway)


Архитектурная модель продукта (Observability vs. Gateway) напрямую предопределяет его способность к оптимизации. LangSmith, как платформа наблюдаемости, по своей конструкции пассивна и может поддерживать только реактивную оптимизацию (помогать человеку). Helicone, как шлюз, по своей конструкции активен и может выполнять проактивную оптимизацию (кэширование, маршрутизация), которая архитектурно невозможна для LangSmith.


Таблица 3. Сравнительный анализ функций оптимизации затрат




Тип оптимизации
	OpenAI API
	LangSmith (Observability)
	Helicone (Gateway)
	Реактивная (Дашборды)
	Базовая
	Высокая (APM) [45]
	Высокая (FinOps) 7
	Проактивная (Авто-кэширование)
	Нет (только ручное) 53
	Отсутствует
	Высокая (Встроено) 7
	Проактивная (Маршрутизация)
	Нет
	Отсутствует
	Высокая (Встроено) 6
	

РАЗДЕЛ VI. СИНТЕЗ И СТРАТЕГИЧЕСКИЕ РЕКОМЕНДАЦИИ




A. Сводная оценка полноты информации


Анализ выявляет значительные пробелы в "полноте" у всех рассматриваемых решений, но по разным причинам.
* OpenAI API: Это источник данных, а не решение. Он предоставляет необработанные данные о биллинге 4 и базовую агрегацию по пользователям.15 Ему не хватает всех продвинутых функций: агрегации агентов, "жестких" бюджетов и проактивной оптимизации.
* LangSmith: Это лучший в своем классе инструмент APM для отладки агентов. Он превосходен в трассировке и агрегации "белого ящика".23 Однако его неполнота с точки зрения FinOps очевидна и фундаментальна:
   1. Критерий 1 (Калькулятор): Неточный из-за неспособности отслеживать кэш провайдера.12
   2. Критерий 2 (Агрегация): Карательная ценовая модель ($0.001 / узел) 28, которая делает его гранулярность непомерно дорогой.
   3. Критерий 3 (Бюджет): Явное отсутствие "жестких" лимитов расходов.36
   4. Критерий 4 (Оптимизация): Архитектурная неспособность выполнять проактивную оптимизацию (кэширование/маршрутизация).
* Helicone: Это настоящее FinOps-решение (шлюз). Он напрямую решает три из четырех критериев:
   1. Критерий 1 (Калькулятор): Точный, поскольку он контролирует прокси и кэш.
   2. Критерий 2 (Агрегация): Гибкий, но ручной (требует маркировки кода).13
   3. Критерий 3 (Бюджет): Предоставляет оповещения о расходах 13 и контроль через Rate Limiting.14
   4. Критерий 4 (Оптимизация): Лучший в своем классе благодаря проактивному кэшированию и маршрутизации.6


B. Таблица 4. Сводная матрица: Полнота по критериям пользователя




Критерий
	LangSmith
	Helicone
	OpenAI API
	1. Калькулятор затрат
	Низкая (Неточный) 12
	Высокая
	Низкая (Только данные)
	2. Агрегация (Агент/Задача)
	Высокая (APM) / Низкая (Цена) 28
	Высокая (FinOps, Ручная) 21
	Отсутствует
	3. Бюджет и оповещения
	Низкая (Нет лимитов) 36
	Средняя (Есть оповещения/контроль) 13
	Низкая (Нет лимитов) 29
	4. Оптимизация затрат
	Низкая (Только реактивная) [45]
	Высокая (Проактивная) 7
	Отсутствует
	

C. Рекомендуемые архитектурные паттерны


Не существует одного "полного" инструмента. Выбор зависит от приоритетов:
1. Паттерн 1: "LangSmith-Native" (Приоритет: Качество/Отладка).
   * Описание: Использование LangSmith для всего: трассировки, мониторинга и оценки затрат.
   * Плюсы: Глубокая интеграция с LangChain, лучшая в своем классе автоматическая трассировка.23
   * Минусы: Дорого в масштабе 28, неточный финансовый учет 12, отсутствие контроля затрат.36
   * Рекомендация: Подходит для R&D, прототипирования и отладки качества агентов, но не для производственного контроля затрат.
2. Паттерн 2: "Шлюз в первую очередь" (Приоритет: Затраты/FinOps).
   * Описание: Использование Helicone (или open-source LiteLLM) в качестве основного шлюза для всех запросов.
   * Плюсы: Полный контроль FinOps, проактивная оптимизация 7, фреймворк-агностик.1
   * Минусы: Требует ручной маркировки кода для агрегации агентов.21
   * Рекомендация: Подходит для производственных, высоконагруженных, чувствительных к затратам приложений.
3. Паттерн 3: "DIY / Open-Source" (Приоритет: Контроль/Отсутствие Lock-in).
   * Описание: Использование OpenAI API напрямую, прокси-сервера LiteLLM 39 и дашбордов Grafana.54
   * Плюсы: Полный контроль, отсутствие vendor lock-in, "жесткие" бюджеты 41, нулевая стоимость ПО.
   * Минусы: Высокие затраты на разработку, настройку и поддержку.
   * Рекомендация: Подходит для крупных предприятий с выделенными платформами командами.


D. Заключительный вывод: Оптимальная гибридная архитектура


Анализ показывает, что LangSmith и Helicone/LiteLLM не являются взаимоисключающими; они решают две разные, но одинаково важные проблемы:
* LangSmith решает проблему APM / Качества: "Почему мой агент выбрал этот инструмент и дал плохой ответ?"
* Helicone решает проблему FinOps / Надежности: "Как мне автоматически снизить затраты и предотвратить перерасход в $50,000?"
Пытаться использовать LangSmith для FinOps — значит использовать неточный инструмент, который к тому же дорого обходится. Пытаться использовать Helicone для гранулярной отладки логики агента — значит использовать инструмент "черного ящика", который не видит внутренней работы.
Таким образом, наиболее "полное" и зрелое архитектурное решение — это гибридный стек с четким разделением ответственности:
1. Уровень APM (Качество): Приложение использует LangSmith SDK (или OTel 5) для отправки подробных данных трассировки в LangSmith. Эта система используется инженерами исключительно для отладки качества и логики агентов.
2. Уровень FinOps (Затраты): Все исходящие вызовы LLM из приложения принудительно направляются через прокси-шлюз (Helicone или LiteLLM). Эта система используется для точного учета, кэширования, маршрутизации и принудительного применения "жестких" бюджетных лимитов.
Эта архитектура позволяет получить лучшее из двух миров: гранулярную отладку от LangSmith, не страдая от его неточного учета затрат, и жесткий финансовый контроль и оптимизацию от шлюза.
