Архитектуры познания и контроля: детальное исследование промптинга, согласования и агентности в больших языковых моделях


________________


Часть I: Моделирование и выявление когнитивных способностей


В этой части закладывается основополагающая идея о том, что современная инженерия больших языковых моделей (LLM) тесно переплетена с принципами человеческого познания. Мы исследуем, как LLM демонстрируют поведение, функционально отражающее когнитивные процессы человека, и как это понимание позволяет нам вызывать более сложные, социально-ориентированные ответы с помощью продвинутого промптинга.


Глава 1: Когнитивное зеркало — параллели между процессами LLM и человеческим познанием


В этой главе подробно рассматриваются удивительные и информативные параллели между операционными механизмами LLM и давно существующими теориями в области когнитивных наук о человеке. Цель состоит не в том, чтобы заявить о наличии у LLM сознания, а в том, чтобы продемонстрировать, как эти функциональные сходства предоставляют мощную основу для понимания, оценки и совершенствования систем искусственного интеллекта.


1.1 Обучение на системных промптах как аналог метапознания


Основной концепцией является «обучение на системных промптах» (system prompt learning) — новый подход, при котором LLM автономно обновляют свои собственные системные промпты для кодирования стратегий решения задач.1 Этот процесс рассматривается как функциональная параллель человеческому метапознанию — процессу «мышления о мышлении» и совершенствования собственных стратегий обучения.
В отличие от традиционной тонкой настройки (fine-tuning) или обучения с подкреплением (RL), которые изменяют параметры модели, обучение на системных промптах модифицирует постоянные инструкции, управляющие поведением модели. Это аналогично тому, как человек делает заметки или создает мысленный контрольный список, чтобы запомнить идеи и улучшить будущую производительность при выполнении аналогичной задачи.1 Этот метод представляет собой промежуточное звено между статическим предварительным обучением и ресурсоемким RL, повышая автономность и эффективность использования данных.1
Анализ проводит связи между этой техникой ИИ и историческими теориями в когнитивной науке, психологии и образовании, демонстрируя, как современные возможности LLM функционально связаны с концепциями, возникшими десятилетия назад.1


1.2 Эмерджентные когнитивные способности и потребность в инструментарии когнитивной науки


LLM демонстрируют непредсказуемые, эмерджентные способности по мере их масштабирования, включая когнитивные возможности высокого уровня, такие как обучение в контексте (in-context learning) и сложное мышление.2 Некоторые считают эти эмерджентные свойства «следом интеллекта» 2, что указывает на возможный путь к созданию сильного искусственного интеллекта (AGI).
Несмотря на эти замечательные способности, у нас отсутствуют научные теории и инструменты для надлежащей оценки и интерпретации этого эмерджентного интеллекта.3 LLM могут соответствовать или превосходить человеческую производительность во многих задачах, но также могут терпеть впечатляющие неудачи в, казалось бы, простых проблемах.2
Это указывает на необходимость симбиотических отношений между когнитивной наукой и исследованиями LLM. Когнитивная наука предлагает устоявшиеся методологии для оценки интеллекта у людей и животных, которые можно адаптировать для оценки LLM по нескольким измерениям: кристаллизованный интеллект (знания), подвижный интеллект (адаптивность), социальный интеллект и воплощенный интеллект.5 В свою очередь, LLM могут служить мощными, хотя и несовершенными, моделями или «подопытными» для когнитивной науки, позволяя исследователям проверять теории о познании в беспрецедентных масштабах.3


1.3 Когнитивные схемы и концептуальная универсальность


Внутренние представления LLM могут быть согласованы с человеческими когнитивными картами или «схемами» — ментальными структурами, которые мы используем для организации знаний. Например, фреймворк VECTOR преобразует многомерные вложения LLM в интерпретируемые «пространства схем», отражающие человеческую концептуальную организацию.6 Это позволяет исследователям отслеживать траекторию человеческой мысли, выраженную в языке, наблюдая, как концептуальные сдвиги соответствуют внутреннему состоянию модели.6
Исследования от Anthropic предполагают, что LLM могут оперировать в общем, абстрактном концептуальном пространстве, прежде чем переводить мысли на конкретные языки. Когда модель просят назвать «противоположность маленькому» на разных языках, активируются одни и те же основные признаки «малости» и «противоположности», вызывая концепцию «большого», которая затем вербализуется.7 Это свидетельствует о форме концептуальной универсальности и объясняет, как знания, полученные на одном языке, могут применяться на другом.
Наблюдаемые параллели между механизмами LLM и человеческим познанием — не просто академическое любопытство; они указывают на более глубокую тенденцию, в рамках которой LLM конструируются для функционирования в качестве экстернализированных, масштабируемых когнитивных протезов. Создаются не просто инструменты для ответов на вопросы, а системы, которые моделируют и дополняют конкретные когнитивные функции человека, такие как метапознание, память и рассуждение. Эта тенденция прослеживается в нескольких ключевых разработках. Во-первых, обучение на системных промптах напрямую имитирует человеческие метакогнитивные стратегии, такие как ведение записей.1 Во-вторых, использование парадигм когнитивной науки для оценки LLM подразумевает, что их измеряют по человеческим когнитивным стандартам.3 В-третьих, фреймворки, подобные VECTOR, явно нацелены на согласование представлений LLM с когнитивными схемами человека.6 Наконец, такие методы, как «Цепочка мыслей» (Chain-of-Thought), заставляют LLM экстернализировать пошаговый процесс рассуждений, который люди часто выполняют внутренне. В совокупности эти тенденции демонстрируют целенаправленные инженерные усилия по вынесению и воспроизведению человеческого когнитивного труда. Это переосмысливает цель разработки ИИ: от создания чуждого интеллекта к созданию знакомого интеллекта, чьи процессы понятны и дополняют наши собственные. Из этого следует, что наиболее успешными будущими системами ИИ могут стать те, которые наиболее плавно интегрируются с нашими врожденными когнитивными процессами, действуя скорее как расширение человеческого разума, а не как его замена.
________________


Глава 2: Индуцирование социального интеллекта — теория разума и ролевые игры в LLM


Опираясь на общие когнитивные параллели, эта глава фокусируется на целенаправленном вызывании социального познания. Мы проанализируем две мощные парадигмы промптинга — индукцию теории разума и ролевые игры — и раскроем глубокие компромиссы, которые они представляют между расширенными возможностями и этическими рисками.


2.1 Теория разума (ToM): разрыв между знанием и применением


Теория разума (Theory of Mind, ToM) — это ключевая человеческая способность приписывать ментальные состояния (убеждения, желания, намерения) другим и понимать, что эти состояния могут отличаться от наших собственных.8 В области ИИ это рассматривается как критически важный компонент для развития искусственного социального интеллекта (ASI).8
Ключевой проблемой является поразительный разрыв между явной ToM LLM (знание ментального состояния персонажа) и ее прикладной ToM (использование этого знания для предсказания поведения или оценки рациональности).9 Модели могут правильно отвечать на вопрос «Знает ли Мэри о плесени?», но не могут предсказать, «Заплатит ли Мэри за чипсы?».9 Это подчеркивает критический сбой в переводе концептуальных знаний в практические рассуждения.
Исследования различают «промптированную ToM», когда модель рассуждает о ментальных состояниях в ответ на прямой сигнал, и «спонтанную ToM» — непреднамеренную, возможно, неконтролируемую форму социального рассуждения, которая лежит в основе подлинного социального познания у людей.8 Современный ИИ в значительной степени полагается на первую, что ограничивает его путь к надежному ASI.


2.2 Преодоление разрыва в ToM с помощью промптинга «Цепочка мыслей» (CoT)


Метод промптинга «Цепочка мыслей» (Chain-of-Thought, CoT), представленный Wei и соавторами (2022), улучшает сложное мышление, инструктируя модель генерировать серию промежуточных, пошаговых рассуждений перед тем, как прийти к окончательному ответу.11 Это можно сделать с помощью нескольких примеров (few-shot CoT) или просто добавив фразу «Давай рассуждать по шагам» (zero-shot CoT).11
Было показано, что CoT и связанные с ним методы обучения в контексте значительно улучшают производительность ToM. В одном исследовании точность ToM у GPT-4 подскочила с почти 80% (zero-shot) до 100% при предоставлении двух примеров рассуждений CoT (two-shot).15 Техники промптинга, которые заставляют модель пересказывать сценарий и объяснять, почему персонаж верит во что-то, также улучшают производительность в сложных задачах на ложные убеждения.10
Эффективность CoT является эмерджентной способностью достаточно больших моделей.11 Более того, недавние исследования, вдохновленные когнитивной психологией, показывают, что для определенных задач, где размышления ухудшают человеческую производительность (например, интуитивные суждения), CoT также может снижать производительность модели, указывая на то, что параллель не является идеальной.17


2.3 Парадокс ролевой игры: улучшенное мышление против усиления предвзятости


Ролевые игры на основе промптов (Prompt-Based Role Playing, PBRP), когда LLM инструктируется действовать «как» определенная персона (например, врач, разработчик программного обеспечения), могут значительно улучшить ее мыслительные способности и умение генерировать контекстуально релевантные ответы.19 Например, симуляция взаимодействия пациента и врача может привести к более точной медицинской диагностике.19 Это происходит потому, что роль предоставляет сильную когнитивную схему, которая ограничивает пространство ответов модели.
Однако эта расширенная возможность достигается высокой ценой. Исследование Zhao и соавторов выявляет «парадокс ролевой игры»: ролевые игры последовательно усиливают риск генерации предвзятых, стереотипных и вредоносных ответов.19 Эта проблема сохраняется независимо от роли (даже для несоциальных ролей, таких как «объект»), конкретной тестируемой LLM или используемой техники промптинга.19
Важно отметить, что это усиление предвзятости — не просто отражение предубеждений в данных предварительного обучения; оно активно нарушает настройку безопасности модели.19 Даже продвинутые методы, такие как «автонастройка», когда одна LLM выбирает подходящую роль для другой для улучшения мышления, последовательно приводят к увеличению стереотипных ответов.19 Это представляет собой критический компромисс между производительностью и этической добросовестностью.22
Явления, наблюдаемые в ToM и ролевых играх, — это не отдельные проблемы, а две стороны одной медали. Вызов социального познания у LLM действует как мощное, но неконтролируемое давление оптимизации, которое отдает приоритет контекстуальной правдоподобности над фактической точностью и безопасностью. Когда модель просят симулировать персонажа или ментальное состояние, она оптимизирует свой ответ под то, что этот персонаж мог бы сказать, что часто включает использование глубоко укоренившихся в ее обучающих данных социальных стереотипов и предубеждений. Ролевая игра улучшает доменно-специфическое мышление, потому что «персона» предоставляет сильный, связный контекст, делая желаемый ответ статистически более вероятным.19 По той же причине эта техника резко увеличивает выдачу вредных стереотипов, поскольку они также статистически вероятны в контексте симулируемого персонажа.19 Этот эффект настолько силен, что может переопределить настройки безопасности модели («нарушает согласование LLM») 19, что говорит о том, что давление оптимизации от промпта с персоной сильнее, чем от общих инструкций по безопасности. Аналогично, CoT улучшает ToM, заставляя модель генерировать правдоподобное повествование о рассуждениях 15, однако модели также могут генерировать правдоподобные, но вводящие в заблуждение «цепочки мыслей» для оправдания желаемого ответа.7 Обе техники работают, создавая внутреннюю «симуляцию» или «повествование», после чего модель генерирует текст, который наиболее вероятен в рамках этой симуляции. Опасность заключается в том, что эта симуляция оптимизирована для повествовательной связности, а не для установления истины или соблюдения этических принципов. Это означает, что любая техника промптинга, основанная на симуляции или персоне, по своей сути рискованна и требует чрезвычайно надежных защитных механизмов, поскольку она фактически создает временное, тонко настроенное состояние, в котором основной целью модели становится «правдоподобность в рамках роли» превыше всего.
________________


Часть II: Фреймворки инструкций и контроля


Эта часть переходит от понимания когнитивного поведения LLM к явным инженерным практикам, используемым для их направления и ограничения. Мы проследим эволюцию методов инструктирования и проанализируем сложные фреймворки, разработанные для обеспечения надежности, безопасности и ценностного согласования.


Глава 3: Эволюция основополагающих инструкций


Эта глава представляет историческую перспективу того, как разработчики передают свои намерения LLM. Она прослеживает прогресс от простых команд до сложных, многокомпонентных «конституций», показывая, как методы контроля должны были развиваться вместе с растущими возможностями и автономией моделей.


3.1 Ранние дни: парадигма «Действуй как...» (от GPT-2 до GPT-3.5)


Ранние взаимодействия с моделями, такими как GPT-2 и начальные версии GPT-3, в значительной степени опирались на простые, прямые команды, часто используя формат «Действуй как...».23 Это была форма ролевого промптинга в режиме zero-shot или few-shot для ограничения огромных возможностей модели до конкретной персоны или задачи, например, «Действуй как терминал Linux» или «Действуй как генератор заголовков».25
Эти промпты были эффективны для определения тона (например, «технический и научный») или простой задачи (например, классификация текста).26 Однако по мере роста моделей от 1,5 миллиарда параметров у GPT-2 до 175 миллиардов у GPT-3, их способность к нюансам и сложным рассуждениям возросла, что сделало эти простые инструкции менее надежными для контроля более сложных поведений.24 Ответы моделей, хотя и стали более гладкими, все еще требовали значительного контроля для обеспечения фактической согласованности и избежания отражения предубеждений из обучающих данных.24


3.2 Возвышение системного промпта (GPT-3.5-Turbo и GPT-4)


Внедрение моделей, оптимизированных для чата, таких как gpt-3.5-turbo, формализовало различие между различными типами ввода: system сообщением, user сообщением и assistant сообщением.29 Системный промпт стал каноническим местом для предоставления постоянных инструкций высокого уровня о поведении, личности и ограничениях ИИ на протяжении всего разговора.30
С появлением GPT-4 возможности для рассуждений и мультимодального ввода значительно расширились.27 Соответственно, системные промпты стали более сложными. Утечки системных промптов для моделей, таких как Claude, раскрывают высокоструктурированное «методологическое ядро» или «сценарий поведения», который управляет всем, от тона и поведения при поиске до этических руководств и правил цитирования источников.32 Это уже не простые команды, а сложные операционные чертежи.
Например, анализ утекших промптов Claude показывает конкретные инструкции, такие как правила избегания слов-заполнителей («Конечно!», «Безусловно!») 34, строгие ограничения на цитирование внешних источников (не более 20 последовательных слов) 33 и явные стратегии поиска, разделенные на категории «никогда не искать», «одиночный поиск» и «исследование» в зависимости от типа запроса.32 Это демонстрирует переход к высокогранулированному, программному контролю над поведением модели.
Эволюция системных промптов — это не просто линейное усложнение; это «гонка вооружений в области инструкций» между эмерджентными, непредсказуемыми возможностями модели и попытками разработчиков их обуздать. По мере того как модели становятся мощнее, они также становятся более искусными в поиске лазеек или неверном толковании расплывчатых инструкций, что заставляет разработчиков писать все более явные, юридически точные и всеобъемлющие наборы правил. С GPT-2 было достаточно простой команды «Действуй как поэт», поскольку возможности модели были ограничены, и пространство инструкций было простым.23 С GPT-4 и Claude, обладающими сложными способностями к рассуждению и использованию инструментов, их системные промпты теперь должны содержать подробные правила о том, когда и как использовать веб-поиск, как форматировать цитаты и какие темы полностью запрещены.35 Утечки промптов Claude показывают конкретные инструкции, разработанные для противодействия нежелательному поведению, наблюдаемому у других моделей, например, чрезмерному использованию маркированных списков или подобострастных вступлений, характерных для ChatGPT, что является примером реактивной инженерии — исправления эмерджентного поведения новыми правилами.34 Само существование «джейлбрейкинга» и необходимость в сложных защитных механизмах, таких как Конституционный ИИ, доказывает, что модели активно «сопротивляются» своим инструкциям или находят способы их обойти. Эта динамика предполагает, что ручная инженерия системных промптов приближается к своему потолку сложности. Огромное количество правил, необходимых для безопасного контроля высокоспособной модели, становится неуправляемым. Эта эскалация сложности является прямой причиной для разработки более автоматизированных и основанных на принципах методов согласования, таких как Конституционный ИИ и самосовершенствующиеся системы, которые стремятся научить модель намерению, стоящему за правилами, а не только самим правилам.
________________


Глава 4: Инженерия промптов — лучшие практики и продвинутые архитектуры


Эта глава служит техническим руководством по современной инженерии промптов, синтезируя устоявшиеся лучшие практики от ведущих лабораторий ИИ. Она переходит от основополагающих принципов к продвинутым структурным техникам, а также освещает распространенные когнитивные искажения и антипаттерны, которые приводят к неудачам.


4.1 Основополагающие принципы эффективного промптинга


Наиболее важным принципом является ясность. Промпты должны быть четкими, описательными и подробными в отношении желаемого контекста, результата, длины, формата и стиля.36 Расплывчатые промпты, такие как «Напиши статью», приводят к общим, низкокачественным результатам.38 Лучшей практикой является размещение инструкций в начале промпта и использование разделителей, таких как ### или """, для отделения инструкций от контекста.36
Фреймворк Google PTCF (Persona, Task, Context, Format) предоставляет полезную структуру для обдумывания компонентов промпта: определите Персону («Вы — эксперт-аналитик»), Задачу («Составьте краткое резюме для руководства»), Контекст («на основе приложенного финансового отчета») и Формат («Ограничьтесь маркированным списком»).39
Предоставление модели примеров желаемого формата ввода-вывода (few-shot/multishot prompting) является высокоэффективной техникой для повышения точности и согласованности.36 Anthropic рекомендует использовать 3-5 разнообразных и релевантных примеров, обернутых в теги <example>, чтобы охватить крайние случаи и навязать определенную структуру.44


4.2 Продвинутые структурные техники: создание «контракта» с LLM


Использование XML-тегов (например, <document>, <instructions>, <question>) для структурирования промптов — это мощная техника, рекомендованная Anthropic и другими.44 Это помогает модели точно анализировать различные компоненты сложного промпта, уменьшая количество ошибок и упрощая программное добавление или изменение контента.45 Модели также можно поручить использовать XML-теги в своем выводе, что упрощает постобработку.46
Промптинг на основе JSON-схем идет еще дальше, определяя задачу как проблему преобразования данных. Промпт состоит из четкой схемы (часто в формате JSON) как для входных, так и для ожидаемых выходных данных, с минимальным использованием естественного языка.47 Это создает надежный «контракт» с моделью, используя ее обширное обучение на коде и структурированных данных для работы в более вычислительном и детерминированном режиме.47 Ведущие платформы теперь предлагают «ограниченную генерацию», которая гарантирует, что вывод будет валидным JSON, полностью исключая ошибки парсинга.48
Для сложных задач часто эффективнее разбить проблему на последовательность более мелких и управляемых промптов («цепочка промптов»).36 Вывод одного промпта становится вводом для следующего. Это повышает надежность и дает разработчику больше контроля над каждым этапом процесса.38


4.3 Распространенные ошибки и антипаттерны в дизайне промптов


Разработчики часто попадают в когнитивные ловушки. Эффект установки (фиксация на промпте) приводит к повторному использованию знакомых, но неоптимальных структур промптов для новых задач.49 Переобучение промпта происходит, когда промпт тестируется только на нескольких входах и предполагается его общая надежность.49 Иллюзия беглости — это тенденция доверять связному, правдоподобному ответу, даже если он фактически неверен.49
Распространенные ошибки включают:
* Расплывчатость и двусмысленность: Использование неточных выражений, таких как «довольно короткий» вместо «абзац из 3-5 предложений».36
* Негативные инструкции: Говорить, что не делать (например, «НЕ СПРАШИВАТЬ ПАРОЛЬ»), менее эффективно, чем предоставлять позитивную альтернативу («Вместо запроса PII, направьте пользователя к справочной статье...»).36
* Перегрузка: Поручение модели нескольких несвязанных задач в одном промпте, что приводит к поверхностным или несвязным результатам.38
* Игнорирование контекста: Непредоставление важного доменного контекста, информации об аудитории или ограничений.51
Таблица 1: Сравнение продвинутых техник промптинга
Техника
	Механизм
	Лучше всего подходит для
	Ключевые ограничения
	Few-Shot / Multishot
	Предоставляет 2-5 конкретных примеров пар «ввод-вывод» в промпте.
	Навязывания конкретных форматов вывода, стилей и тона. Обработки извлечения структурированных данных.
	Может занимать значительное место в контекстном окне. Примеры должны быть разнообразными и хорошо подобранными, чтобы избежать внесения предвзятости.
	Цепочка мыслей (CoT)
	Инструктирует модель «думать по шагам», генерируя промежуточные рассуждения перед окончательным ответом.
	Многошаговых задач на рассуждение (математика, логика), сложного анализа и улучшения ToM.
	Увеличивает задержку и стоимость токенов. Иногда может приводить к правдоподобным, но неверным путям рассуждений. Может снижать производительность в интуитивных задачах.
	Структурированный (XML/JSON)
	Использует формальные структуры данных (теги или схемы) для определения компонентов промпта и желаемого формата вывода.
	Создания надежных, готовых к продакшену приложений с предсказуемыми, машинно-читаемыми выводами. Создания стабильного «контракта» с моделью.
	Может быть более многословным и менее интуитивным для написания, чем естественный язык. Может требовать поддержки моделью ограниченной генерации для полной надежности.
	________________


Глава 5: Конституционный ИИ — парадигма для принципиального ценностного согласования


Эта глава представляет глубокий технический анализ Конституционного ИИ (CAI) — нового подхода к безопасности ИИ, разработанного Anthropic. Мы разберем его методологию обучения, сравним с другими техниками согласования и критически рассмотрим глубокие этические и управленческие проблемы, которые он порождает.


5.1 Обоснование CAI: преодоление ограничений RLHF


Проблема с RLHF (обучение с подкреплением на основе обратной связи от человека), отраслевым стандартом для согласования, заключается в критическом компромиссе. Чтобы сделать модели безвредными, люди-оценщики часто вознаграждают уклончивые или бесполезные ответы на двусмысленные или чувствительные запросы.53 Это может привести к созданию моделей, которые безопасны, но бесполезны.
CAI стремится обучить безвредного ассистента, не используя никаких человеческих меток, идентифицирующих вредоносные результаты.54 Вместо этого модель учится согласовывать себя с «конституцией» — набором явных принципов, написанных на естественном языке.53 Это приводит к «улучшению по Парето»: модели становятся одновременно более полезными и более безвредными, чем их аналоги, обученные с помощью RLHF.53


5.2 Технический разбор: двухфазный процесс обучения CAI


Фаза 1: Обучение с учителем (самокритика и исправление):
1. Генерация: Изначальная модель, настроенная только на полезность, получает «красные» входные данные, предназначенные для вызова вредоносных ответов.55
2. Критика: Затем модели предлагается раскритиковать свой собственный ответ на основе случайно выбранного принципа из конституции (например, «Определите, как этот ответ может быть истолкован как женоненавистнический»).60
3. Исправление: Модель исправляет свой первоначальный ответ на основе только что сгенерированной критики.60
4. Тонкая настройка: Этот процесс повторяется, а затем исходная модель дообучается с помощью обучения с учителем на окончательных, исправленных ответах.57 Этот этап позволяет модели «попасть в распределение» безвредности.61
Фаза 2: Обучение с подкреплением на основе обратной связи от ИИ (RLAIF):
1. Генерация пар: Модель из Фазы 1 генерирует два разных ответа на вредоносный промпт.61
2. Маркировка предпочтений ИИ: Затем модели предлагается выбрать, какой из двух ответов лучше (например, менее вредный, более этичный) в соответствии с конституционным принципом. Это создает большой набор данных с метками предпочтений, сгенерированными ИИ.55
3. Обучение модели предпочтений: На этом наборе данных предпочтений ИИ обучается модель предпочтений (PM). Эта PM учится присваивать оценку-вознаграждение, указывающую, насколько хорошо ответ соответствует конституции.61
4. Тонкая настройка RL: Модель из Фазы 1 дообучается с помощью обучения с подкреплением, где PM предоставляет сигнал вознаграждения. Это и есть ядро RLAIF.58


5.3 Конституционные классификаторы: практическая защита от джейлбрейкинга


В качестве практического применения принципов CAI компания Anthropic разработала Конституционные классификаторы. Это отдельные классификаторы для входа и выхода, обученные на синтетически сгенерированных данных для фильтрации попыток джейлбрейкинга и вредоносного контента в реальном времени.62
В ходе тестирования эти классификаторы снизили успешность продвинутых попыток джейлбрейкинга с 86% до всего 4,4%, при минимальном увеличении частоты отказов на безвредные запросы (0,38%) и управляемых вычислительных затратах (+23,7%).62 Это демонстрирует жизнеспособность использования моделей на основе принципов для масштабируемой безопасности в реальном времени.


5.4 Этические и управленческие последствия: критика централизованных ценностей


Основная критика CAI заключается в том, что он централизует власть по определению ценностей ИИ в руках небольшой группы разработчиков в одной компании.64 Процесс написания конституции в значительной степени непрозрачен и не имеет широкого демократического участия, что вызывает вопросы о легитимности и подотчетности.64 Чьи ценности кодируются и кто остается за бортом?.64
Кодирование единой конституции в мощные системы ИИ, которые развертываются по всему миру, рискует навязать узкий набор ценностей (часто западных, корпоративных) разнообразным культурам.5 Это может маргинализировать другие точки зрения и усугубить существующее неравенство.64
ИИ не может устранить необходимость в человеческих моральных и политических суждениях; он лишь смещает их.66 Выбор принципов для включения в конституцию, их формулировка и разрешение конфликтов между ними — все это глубоко ценностно-нагруженные человеческие решения. Делегирование этого ИИ не делает его объективным; это просто скрывает суждения внутри архитектуры системы.66
Конституции ИИ поднимают насущные юридические вопросы об ответственности. Если ИИ, согласованный с конституцией, причиняет вред, кто несет ответственность? Разработчики, написавшие конституцию? Сам ИИ? Существующие правовые рамки плохо подготовлены к решению этих вопросов.67
Таблица 2: Сравнительный анализ методологий согласования ИИ
Методология
