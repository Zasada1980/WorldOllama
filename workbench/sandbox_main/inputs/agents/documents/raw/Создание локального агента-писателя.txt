Архитектура локального ИИ-писца: комплексное руководство по созданию и развертыванию мультиагентной системы для написания текстов




Раздел 1: Основы агентных систем для написания текстов


Этот раздел закладывает концептуальную основу, утверждая, что высококачественное автоматизированное написание текстов — это не задача для одного ИИ, а совместный процесс, выполняемый командой специализированных агентов. В нем будут определены ключевые компоненты этой новой парадигмы.


1.1 За пределами одиночных агентов: аргументы в пользу коллективного интеллекта


Традиционный подход к использованию больших языковых моделей (LLM) для сложных задач часто сводится к созданию одного всеобъемлющего промпта. Однако для задач, требующих многоэтапного рассуждения, исследования и творчества, таких как написание качественных текстов, этот метод имеет свои ограничения. Более совершенная парадигма заключается в использовании мультиагентных систем (MAS), которые распределяют работу между несколькими специализированными агентами.1
В отличие от одноагентных систем, где одна модель отвечает за весь спектр задач, MAS организует совместную работу автономных агентов, каждый из которых играет определенную роль. Исследования показывают, что такой подход обеспечивает значительные преимущества, включая повышенную скорость, надежность и устойчивость к неопределенным данным.1 Эта архитектура выходит за рамки простого последовательного мышления (Chain-of-Thought, CoT), создавая динамическую среду, в которой агенты могут делегировать задачи и взаимодействовать друг с другом для достижения общей цели.1 Цель состоит в том, чтобы оркестровать сложные рабочие процессы, которые выходят за рамки возможностей одного агента, каким бы мощным он ни был.2
Эффективность этой модели заключается в специализации. Вместо того чтобы пытаться создать одного универсального «писателя», мы проектируем систему, состоящую из «планировщика», «исследователя», «автора» и «редактора». Такой подход позволяет решать проблемы, которые слишком велики для одноагентных систем, и обеспечивает более надежный и качественный результат.1


1.2 Анатомия ИИ-агента: строительные блоки команды


В основе мультиагентных фреймворков, таких как CrewAI, лежит концепция «агента» — автономной единицы, способной выполнять задачи, принимать решения, использовать инструменты и общаться с другими агентами.3 Понимание его структуры имеет решающее значение для создания эффективной системы. Ключевыми атрибутами агента являются:
* Роль (role): Определяет функцию и область экспертизы агента в команде. Это не просто метка, а фундаментальная часть его системного промпта. Например, вместо общей роли «Писатель» следует использовать более конкретные, такие как «Специалист по технической документации» или «Креативный рассказчик».5
* Цель (goal): Индивидуальная задача, которая направляет процесс принятия решений агента. Цель должна быть четкой, ориентированной на результат и включать критерии качества.5
* Предыстория (backstory): Обеспечивает контекст и личность агента, обогащая взаимодействие. Хорошая предыстория описывает опыт агента, его стиль работы и ценности, создавая целостный образ.4
Эти три атрибута формируют «персону» агента, которая напрямую влияет на стиль и качество генерируемого им текста. Помимо них, существуют и другие важные параметры:
* Инструменты (tools): Возможности или функции, доступные агенту, такие как поиск в интернете, чтение файлов или использование API.4
* LLM (llm): Языковая модель, которая служит «мозгом» агента. Разным агентам в одной команде можно назначать разные модели для оптимизации производительности и затрат.3
* Разрешение делегирования (allow_delegation): Позволяет агенту передавать задачи другим агентам, что является ключевым элементом для совместной работы.4
Проектирование эффективных ИИ-агентов, по сути, является упражнением в создании персон. Качество role, goal и backstory напрямую и причинно влияет на качество конечного результата, поскольку эти элементы программно объединяются в системный промпт, который направляет LLM. Расплывчатые определения приводят к созданию шаблонного, «звучащего как ИИ» текста. Детальная, ориентированная на эксперта персона, напротив, заставляет модель генерировать текст в определенном стиле и предметной области. Таким образом, «мягкий» навык написания убедительной предыстории агента оказывает «жесткое» техническое влияние на конечный продукт.


1.3 Архетипичная команда для написания текстов: определение ключевых ролей


На основе анализа успешных реализаций можно выделить стандартную и эффективную архитектуру команды для написания текстов. Эта структура имитирует реальный редакционный процесс и включает в себя следующие роли:
* Планировщик (Planner): Этот агент берет высокоуровневый запрос пользователя и разбивает его на серию четких, выполнимых подзадач или ключевых вопросов. Его задача — создать структуру будущего текста.6 Например, на запрос «написать статью о квантовых вычислениях» планировщик может составить план: 1. Что такое квантовые вычисления? 2. Ключевые принципы (суперпозиция, запутанность). 3. Потенциальные применения. 4. Текущие проблемы и вызовы.
* Исследователь (Researcher): Выполняет план, созданный планировщиком. Используя инструменты (например, веб-поиск, поиск по локальным документам), он собирает информацию по каждому из подвопросов.6 Результатом его работы является набор релевантных данных, готовых для синтеза.
* Автор (Author/Writer): Синтезирует собранные исследователем данные в связный черновик, следуя первоначальному плану. Этот агент фокусируется на структуре, повествовании и ясности изложения.8
* Редактор/Критик (Editor/Critic): Проверяет черновик на точность, стиль, согласованность и грамматику. Что особенно важно, этот агент может запрашивать доработку у автора, создавая итеративный цикл обратной связи, который значительно повышает качество конечного продукта.2
Эта структура обеспечивает разделение труда, где каждый агент выполняет задачу, в которой он наиболее силен, что приводит к созданию более качественного и проработанного контента, чем мог бы создать один агент-универсал.


1.4 Оркестрация и процессный поток: управление рабочим процессом


Эффективность команды агентов зависит не только от их индивидуальных качеств, но и от того, как организовано их взаимодействие. Фреймворки, такие как CrewAI, предлагают две основные модели управления процессом 11:
* Последовательный процесс (Process.sequential): Задачи выполняются в строгом, заранее определенном порядке. Этот подход похож на конвейер: результат работы одного агента становится входными данными для следующего. Это идеальный вариант для линейных и предсказуемых рабочих процессов, таких как предложенная выше схема «Планировщик → Исследователь → Автор → Редактор».13 Контекст передается от задачи к задаче неявно, обеспечивая плавный поток информации.13
* Иерархический процесс (Process.hierarchical): В этой модели появляется «менеджер» (manager_llm или manager_agent), который контролирует всю команду. Менеджер не выполняет задачи сам, а динамически делегирует их подчиненным агентам в зависимости от текущей ситуации, а затем проверяет результаты. Этот подход более гибкий и надежный для сложных или непредсказуемых сценариев, где может потребоваться изменение плана в ходе выполнения.11
Выбор процесса является ключевым архитектурным решением. Для основной цели создания агента-«писателя» рекомендуется начинать с последовательного процесса из-за его простоты и предсказуемости. Переход к иерархической модели оправдан, когда задачи становятся более сложными и требуют адаптивного управления. Этот переход от программирования к управлению знаменует собой сдвиг парадигмы: разработчик превращается из программиста в архитектора и менеджера, который фокусируется на составе команды, дизайне рабочего процесса и протоколах коммуникации, а не только на алгоритмической логике.


Раздел 2: Локальная ИИ-экосистема: оборудование и модели


Этот раздел закладывает основу для создания локальной среды. В нем рассматриваются критически важные вопросы о том, какое оборудование необходимо и какие модели ИИ лучше всего подходят для задачи, что позволяет пользователю принимать обоснованные решения еще до написания первой строки кода.


2.1 Локальный движок для инференса: установка и настройка Ollama


Ollama является краеугольным камнем локальной ИИ-инфраструктуры. Это инструмент, который упрощает загрузку, управление и запуск больших языковых моделей на вашем собственном компьютере, абстрагируясь от сложностей настройки и предоставляя стандартизированный API.15
Установка Ollama проста и зависит от операционной системы 16:
* macOS и Windows: Загрузите установщик с официального сайта ollama.com.
* Linux: Выполните в терминале команду: curl -fsSL https://ollama.com/install.sh | sh.
После установки доступны следующие основные команды CLI:
* ollama pull <model_name>: Загружает модель из библиотеки (например, ollama pull llama3.1).16
* ollama run <model_name>: Запускает модель в интерактивном чате.16
* ollama list: Показывает список всех загруженных локально моделей.18
* ollama rm <model_name>: Удаляет модель с вашего компьютера.16
Ollama запускает локальный сервер, доступный по адресу http://localhost:11434, который служит API-точкой для интеграции с фреймворками, такими как CrewAI и LangChain.19 Для более тонкой настройки моделей можно использовать Modelfile. Этот файл позволяет задавать системные сообщения, изменять параметры, такие как «температура» (креативность), и импортировать модели в формате GGUF.16


2.2 Глубокое погружение в оборудование: практическое руководство по VRAM и системной RAM


Наиболее важным аппаратным компонентом для локального запуска LLM является видеопамять (VRAM) графического процессора (GPU).21 Объем VRAM напрямую определяет, модели какого размера вы сможете запускать эффективно. Существует эмпирическое правило для расчета требуемой VRAM: (Размер модели в миллиардах параметров) × (Байт на параметр после квантования) + (Накладные расходы).22
Требования к оборудованию можно разделить на несколько уровней:
* Начальный уровень (8–12 ГБ VRAM): Подходит для моделей размером 7B–13B с 4-битным квантованием. Примеры GPU: NVIDIA RTX 3060 (12 ГБ), RTX 4060 (8 ГБ).21
* Высокий уровень (16–24+ ГБ VRAM): Позволяет комфортно работать с моделями до 30B или даже 70B с агрессивным квантованием. Примеры GPU: NVIDIA RTX 3090 (24 ГБ), RTX 4090 (24 ГБ).22
* Рабочие станции / Унифицированная память: Для очень больших моделей (70B+) требуется 48 ГБ VRAM и более. Альтернативой являются компьютеры Apple Silicon (M1/M2/M3/M4) с архитектурой унифицированной памяти, где системная RAM используется как VRAM, что позволяет запускать огромные модели.21
Системная оперативная память (RAM) также играет важную роль, особенно при загрузке моделей. Рекомендуется иметь минимум 16 ГБ RAM, а для комфортной работы — 32 ГБ и более.17
Размер модели (параметры)
	VRAM для FP16 (полная точность)
	VRAM для INT8 (8-битное квантование)
	VRAM для INT4 (4-битное квантование)
	Рекомендуемый минимум GPU
	7B
	~14 ГБ
	~7 ГБ
	~3.5 ГБ
	RTX 3060 12GB
	13B
	~26 ГБ
	~13 ГБ
	~6.5 ГБ
	RTX 3060 12GB / RTX 4060 8GB
	30B
	~60 ГБ
	~30 ГБ
	~15 ГБ
	RTX 3090 / 4090 24GB
	70B
	~140 ГБ
	~70 ГБ
	~35 ГБ
	2× RTX 3090 / RTX 6000 Ada 48GB
	Таблица 1: Требования к VRAM для локальных LLM. Данные синтезированы из.21
Эта таблица наглядно демонстрирует, как квантование делает большие модели доступными для потребительского оборудования.


2.3 Искусство квантования: баланс между производительностью и ресурсами


Квантование — это процесс снижения точности весов модели (например, с 16-битных чисел с плавающей запятой до 4-битных целых чисел). Этот процесс значительно уменьшает размер модели и, как следствие, требования к VRAM, часто с минимальной или незаметной потерей качества для большинства задач.22
Например, модель размером 13B, которой в полной точности (FP16) требуется около 26 ГБ VRAM, после 4-битного квантования (INT4) может работать на GPU с 8–10 ГБ VRAM.22 Это не просто небольшая оптимизация, а ключевая технология, которая делает запуск мощных LLM на локальном оборудовании реальностью. Для большинства локальных сценариев использования рекомендуется начинать с 4-битных (например, Q4_K_M) или 5-битных квантованных моделей, так как они предлагают наилучший баланс между производительностью и потреблением ресурсов.24


2.4 Выбор «мозга»: оценка LLM с открытым исходным кодом для написания текстов


Выбор правильной LLM является решающим фактором для успеха агентной системы. Не существует одной «лучшей» модели; выбор зависит от конкретной задачи (например, креативное письмо против технической документации) и аппаратных ограничений.26 Для оценки моделей можно использовать открытые рейтинги, такие как Open LLM Leaderboard от Hugging Face.28
Ниже представлен анализ ведущих моделей с открытым исходным кодом, подходящих для задач написания текстов:
* Семейство Llama 3 (8B, 70B): Модели от Meta, известные своей способностью следовать инструкциям и вести диалог. Llama 3.1 8B Instruct — отличная отправная точка для систем с 8–16 ГБ VRAM.16
* Семейство Mistral/Mixtral: Mistral 7B считается одной из лучших малых моделей. Mixtral 8x7B (смесь экспертов) по производительности на некоторых задачах сопоставима с моделями класса GPT-4. Mistral Large отмечается как сильный локальный копирайтер.22
* Семейство DeepSeek: Эти модели показывают высокие результаты в задачах, связанных с кодированием и написанием технической документации.22
* Phi-4: Компактная, но мощная модель от Microsoft, отлично подходящая для задач программирования.22
Модель
	Лучше всего подходит для
	Сильные стороны
	Слабые стороны
	Рекомендуемая VRAM (INT4)
	Llama 3.1 8B
	Диалоги, общее следование инструкциям
	Сбалансированная производительность, сильное сообщество
	Может быть менее креативной, чем специализированные модели
	6-8 ГБ
	Mistral 7B
	Копирайтинг, быстрое прототипирование
	Высокая производительность для своего размера, хорошо следует промптам
	Меньший контекст по сравнению с новыми моделями
	6-8 ГБ
	Mixtral 8x7B
	Сложные рассуждения, многозадачность
	Производительность класса GPT-4 на многих задачах
	Более высокие требования к VRAM
	20-24 ГБ
	DeepSeek Coder 33B
	Разработка ПО, техническая документация
	Превосходная генерация и анализ кода
	Менее универсальна для креативных задач
	20-24 ГБ
	Phi-4 Mini
	Кодирование, логические задачи на компактном оборудовании
	Высокая производительность для малого размера
	Ограниченный объем знаний по сравнению с большими моделями
	6-8 ГБ
	Таблица 2: Сравнительный анализ ведущих LLM с открытым исходным кодом для задач написания текстов. Данные синтезированы из.22


Раздел 3: Основная реализация: команда для написания текстов с CrewAI и Ollama


Этот раздел является практическим ядром отчета. Он представляет собой полное, пошаговое руководство по созданию и запуску команды «писателей» с использованием рекомендованного стека технологий.


3.1 Настройка среды разработки


Правильная настройка окружения является необходимым условием для создания воспроизводимого и стабильного приложения. Первым шагом является создание виртуального окружения Python для изоляции зависимостей проекта.


Bash




# Создание виртуального окружения
python -m venv.venv

# Активация окружения (macOS/Linux)
source.venv/bin/activate

# Активация окружения (Windows)
.venv\Scripts\activate

После активации окружения необходимо установить требуемые пакеты. Основными компонентами являются crewai для создания агентов и langchain-community (или langchain-ollama), который обеспечивает интеграцию с локальным сервером Ollama.31


Bash




pip install crewai 'crewai[tools]' langchain-community langchain-openai

Пакет langchain-openai используется, поскольку многие интеграции с локальными моделями используют API, совместимый с OpenAI, что будет рассмотрено далее.


3.2 Создание эффективных агентов в коде


На этом этапе теоретические архетипы агентов из Раздела 1.3 преобразуются в программный код с использованием класса Agent из библиотеки CrewAI.4 Важно уделить особое внимание детальному описанию role, goal и backstory, так как это напрямую влияет на поведение агента.5
Ниже приведен пример кода для создания агентов «исследователя» и «писателя», которые будут работать с локальной моделью 10:


Python




from crewai import Agent

# (Инициализация LLM будет показана в п. 3.4)
# ollama_llm =...

# Определение агента-исследователя
researcher = Agent(
 role='Старший научный сотрудник',
 goal='Находить и анализировать передовую информацию по заданной теме',
 backstory="""Вы — опытный исследователь с многолетним стажем работы в ведущем технологическом издании. 
 Вы мастерски находите малоизвестные факты и выявляете глубинные тренды, 
 превращая разрозненные данные в структурированные и понятные выводы.""",
 verbose=True,
 allow_delegation=False,
 llm=ollama_llm # Передача локальной LLM
)

# Определение агента-писателя
writer = Agent(
 role='Технический писатель и блогер',
 goal='Написать увлекательную и информативную статью на основе предоставленных исследований',
 backstory="""Вы — известный технический писатель, способный объяснять сложные темы простым и ясным языком. 
 Ваши статьи отличаются логичной структурой, убедительной аргументацией и живым стилем изложения. 
 Вы превращаете сухие факты в захватывающее повествование.""",
 verbose=True,
 allow_delegation=False,
 llm=ollama_llm # Передача той же локальной LLM
)



3.3 Проектирование рабочего процесса: структурирование задач


Задачи для агентов определяются с помощью класса Task. Каждая задача должна иметь четкое description (описание), expected_output (ожидаемый результат) и agent, ответственного за ее выполнение.14 Описание задачи должно быть максимально подробным, включая информацию о входных данных, формате вывода и, по возможности, примеры.5
Для нашего сценария мы определим две последовательные задачи:


Python




from crewai import Task

# Задача для исследователя
research_task = Task(
 description="Провести всестороннее исследование темы '{topic}'. Собрать ключевые факты, статистику и последние тенденции.",
 expected_output='Подробный отчет в формате bullet-point, содержащий все найденные факты и анализ тенденций по теме.',
 agent=researcher
)

# Задача для писателя
# Эта задача неявно использует результат research_task
write_task = Task(
 description="На основе предоставленного исследовательского отчета написать содержательную статью на тему '{topic}'. Статья должна быть хорошо структурирована, иметь введение, основную часть и заключение.",
 expected_output='Готовая статья объемом не менее 500 слов в формате Markdown.',
 agent=writer
)

В последовательном процессе (Process.sequential) результат выполнения research_task автоматически передается в контекст для write_task.13


3.4 Подключение к локальному «мозгу»: интеграция с Ollama


Это ключевой шаг, который связывает фреймворк CrewAI с локально запущенной языковой моделью. Интеграция часто осуществляется через слой совместимости, который имитирует API OpenAI. Это стало стандартом де-факто в индустрии, поскольку позволяет с минимальными изменениями в коде переключаться между локальными и облачными моделями.33
Для подключения к Ollama используется класс ChatOpenAI из langchain_openai, но с указанием локального base_url.10


Python




from langchain_openai import ChatOpenAI

# Инициализация LLM-клиента для подключения к Ollama
ollama_llm = ChatOpenAI(
   model="llama3.1", # Укажите имя модели, загруженной в Ollama
   base_url="http://localhost:11434/v1",
   api_key="NA" # API-ключ не требуется для локального запуска
)

Хотя использование класса с названием ChatOpenAI для подключения к локальной модели Llama может показаться нелогичным, это распространенная и рабочая практика. Однако стоит отметить, что более новые версии фреймворков, таких как CrewAI и AutoGen, предлагают нативные, более интуитивные способы интеграции, например, LLM(model="ollama/llama3.1") в CrewAI или api_type: "ollama" в AutoGen, что является более надежным подходом.19


3.5 Запуск команды: полный, работающий пример


Теперь соберем все компоненты в единый, готовый к запуску скрипт. Он будет определять LLM, агентов, задачи, собирать их в Crew с последовательным процессом и запускать рабочий процесс с помощью метода kickoff().8


Python




# --- Полный скрипт ---
from crewai import Agent, Task, Crew, Process
from langchain_openai import ChatOpenAI

# 1. Инициализация LLM
ollama_llm = ChatOpenAI(
   model="llama3.1", 
   base_url="http://localhost:11434/v1",
   api_key="NA"
)

# 2. Определение агентов
researcher = Agent(
 role='Старший научный сотрудник',
 goal='Находить и анализировать передовую информацию по заданной теме',
 backstory="""Вы — опытный исследователь...""",
 verbose=True,
 allow_delegation=False,
 llm=ollama_llm
)

writer = Agent(
 role='Технический писатель и блогер',
 goal='Написать увлекательную и информативную статью на основе предоставленных исследований',
 backstory="""Вы — известный технический писатель...""",
 verbose=True,
 allow_delegation=False,
 llm=ollama_llm
)

# 3. Определение задач
research_task = Task(
 description="Провести всестороннее исследование темы '{topic}'.",
 expected_output='Подробный отчет в формате bullet-point...',
 agent=researcher
)

write_task = Task(
 description="На основе предоставленного исследовательского отчета написать статью на тему '{topic}'.",
 expected_output='Готовая статья объемом не менее 500 слов...',
 agent=writer
)

# 4. Сборка и запуск команды
writing_crew = Crew(
 agents=[researcher, writer],
 tasks=[research_task, write_task],
 process=Process.sequential,
 verbose=2
)

# Запуск рабочего процесса
topic_to_write = "Будущее искусственного интеллекта в разработке программного обеспечения"
result = writing_crew.kickoff(inputs={'topic': topic_to_write})

print("######################")
print("Результат работы команды:")
print(result)

Этот скрипт предоставляет пользователю работающий прототип мультиагентной системы для написания текстов, который подтверждает правильность всей предыдущей настройки.


Раздел 4: Расширенная функциональность: агенты с локальными знаниями


Этот раздел поднимает агента-«писателя» с уровня простого генератора текста до уровня осведомленного помощника, способного анализировать частные, локальные документы. Это ключ к созданию по-настоящему кастомизированных и мощных инструментов для написания текстов.


4.1 Введение в генерацию с дополненной выборкой (RAG)


Генерация с дополненной выборкой (Retrieval-Augmented Generation, RAG) — это техника, которая расширяет возможности LLM, позволяя им получать доступ к внешней информации перед генерацией ответа.36 Процесс работает следующим образом:
1. Поиск (Retrieval): Когда поступает запрос, система сначала ищет релевантную информацию во внешней базе знаний (например, в коллекции локальных документов, базе данных или на веб-сайте).
2. Дополнение (Augmentation): Найденная информация добавляется в качестве контекста к исходному запросу.
3. Генерация (Generation): LLM получает расширенный промпт (запрос + контекст) и генерирует ответ, основываясь как на своих внутренних знаниях, так и на предоставленных актуальных данных.
Этот подход «заземляет» модель на фактической, актуальной или проприетарной информации, что значительно снижает вероятность «галлюцинаций» и позволяет агентам работать с данными, на которых они не обучались.36


4.2 Использование встроенных RAG-инструментов CrewAI


CrewAI предлагает набор готовых инструментов для работы с локальными файлами, что является самым простым способом реализовать RAG. К ним относятся PDFSearchTool, DOCXSearchTool, TXTSearchTool и DirectorySearchTool.38
Чтобы использовать такой инструмент, его необходимо инициализировать и добавить в список tools соответствующего агента (обычно исследователя).


Python




from crewai import Agent
from crewai_tools import PDFSearchTool

# Инициализация инструмента для поиска по конкретному PDF-файлу
pdf_search_tool = PDFSearchTool(pdf='path/to/my_document.pdf')

# Добавление инструмента агенту-исследователю
researcher_with_rag = Agent(
 role='Исследователь документов',
 goal='Извлекать информацию из предоставленных PDF-файлов',
 backstory='Вы эксперт по семантическому поиску в документах.',
 tools=[pdf_search_tool], # Добавление инструмента
 llm=ollama_llm,
 verbose=True
)

Теперь агент researcher_with_rag может выполнять семантический поиск внутри указанного PDF-документа для выполнения своих задач.


4.3 Настройка встроенных инструментов для локальных моделей


Критически важная деталь, которую часто упускают из виду, заключается в том, что RAG-инструменты CrewAI по умолчанию используют API OpenAI для создания эмбеддингов (векторных представлений текста) и суммаризации найденного контента.40 Для создания полностью локальной системы необходимо переопределить это поведение.
Это делается с помощью специального словаря config, передаваемого при инициализации инструмента. В этом словаре нужно указать локальную модель для эмбеддингов и, при необходимости, для LLM, используемой в RAG-процессе.


Python




from crewai_tools import PDFSearchTool

# Конфигурация для использования локальных моделей через Ollama
# (Требуется установка дополнительных зависимостей, например, langchain-ollama)
local_rag_config = {
 "llm": {
   "provider": "ollama",
   "config": {
     "model": "llama3.1",
     "base_url": "http://localhost:11434"
   },
 },
 "embedder": {
   "provider": "ollama",
   "config": {
     "model": "nomic-embed-text", # Популярная модель для эмбеддингов
     "base_url": "http://localhost:11434"
   },
 },
}

# Инициализация инструмента с локальной конфигурацией
local_pdf_tool = PDFSearchTool(
   pdf='path/to/my_document.pdf',
   config=local_rag_config
)

Без этой настройки «локальный» RAG-инструмент будет совершать внешние вызовы к API OpenAI, нарушая основное требование пользователя о работе в локальной среде.


4.4 Создание кастомного RAG-инструмента с нуля


Для более сложных сценариев можно создать собственный инструмент. CrewAI предоставляет два способа: наследование от класса BaseTool или использование декоратора @tool.41
Создадим простой инструмент LocalDirectorySearchTool, который ищет информацию в текстовых файлах в указанной директории.


Python




import os
from crewai.tools import tool

@tool("Local Directory Search Tool")
def local_directory_search(directory: str, query: str) -> str:
   """
   Ищет релевантную информацию в текстовых файлах (.txt) в указанной директории.
   Возвращает фрагменты текста, содержащие ключевые слова из запроса.
   """
   relevant_snippets =
   for filename in os.listdir(directory):
       if filename.endswith(".txt"):
           filepath = os.path.join(directory, filename)
           try:
               with open(filepath, 'r', encoding='utf-8') as f:
                   content = f.read()
                   if query.lower() in content.lower():
                       # Для простоты вернем первые 500 символов файла
                       snippets = f"Найден релевантный контент в файле '{filename}':\n{content[:500]}...\n\n"
                       relevant_snippets.append(snippets)
           except Exception as e:
               relevant_snippets.append(f"Не удалось прочитать файл {filename}: {e}\n")
   
   if not relevant_snippets:
       return "Релевантная информация не найдена."
       
   return "".join(relevant_snippets)

Этот самодельный инструмент дает больше гибкости и позволяет реализовать любую логику поиска, необходимую для конкретного проекта.42


4.5 Интеграция векторной базы данных: ChromaDB для постоянных знаний


Для работы с большими базами знаний простое сканирование файлов становится неэффективным. Решением являются векторные базы данных, такие как ChromaDB, которые оптимизированы для быстрого поиска семантически близких фрагментов текста.36
Процесс создания и использования RAG-пайплайна с ChromaDB выглядит следующим образом 43:
1. Загрузка документов: Используются загрузчики из LangChain для чтения данных из различных
