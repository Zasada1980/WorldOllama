---
applyTo: '**'
---
# Инструкция использования шпаргалки локального ИИ

Перед выполнением любой задачи, связанной с локальными LLM, генерацией текста, настройкой инференса или интеграцией моделей:
1. Перед стартом задачи открой разделы шпаргалки:
   - 1–5 (выбор инструмента) и 16 (доступ Open WebUI на `http://localhost:3000`).
   - 8 (chat template) для корректного форматирования сообщений.
   - 17 (переменные окружения) и 20 (перезапуск) если работаешь с сервером.
   - 18 (контекст) при длинной истории диалога.
   - 22 (предупреждения/ошибки) при сбоях.
2. Для API вызовов — используй текущий кастомный эндпоинт Ollama `http://127.0.0.1:11435` (если не изменён; иначе обнови переменную `OLLAMA_HOST`).
3. UI взаимодействие — Open WebUI `http://localhost:3000` (чат, плагины, RAG) или text-generation-webui если нужны расширенные плагины.
4. Минимальная интеграция в код — прямой HTTP Ollama или llama.cpp bindings.
5. Дообучение: сначала поиск готовых LoRA/QLoRA адаптаций (HF), затем оценка необходимости собственного fine‑tune.
6. Производительность: разделы 12 (оптимизация), 19 (offload), 23 (латентность).
7. Увеличение контекста: см. 24 и оцени свободную VRAM.
8. Чеклист перед задачей: раздел 25.

Всегда синхронизируй изменения: при смене порта, контекста или появлении новых LoRA обнови дату и соответствующие разделы.
