# AI Agent Codebase Guide

## ‚ö†Ô∏è CRITICAL: Core Development Protocols

This codebase has specific verification requirements due to past issues with simulated outputs and documentation-instead-of-code changes.

## ‚ö†Ô∏è CRITICAL: Core Development Protocols

This codebase has specific verification requirements due to past issues with simulated outputs and documentation-instead-of-code changes.

## üõë Non-Negotiable Rules

### 1. NO SIMULATION (Reality Check)
**PROHIBITED:** Writing fake terminal output, invented logs, or claiming status ("Server running", "Files cleaned") without executing actual commands.

**REQUIRED:** Every system state claim must be proven with real command execution:
- Want to say "File created"? ‚Üí Execute `Test-Path` or `ls` and show output
- Want to say "Server working"? ‚Üí Execute `curl http://localhost:8003/health` and show JSON
- Want to say "Model loaded"? ‚Üí Execute `nvidia-smi` and show VRAM usage

**RED FLAG:** If your response lacks terminal command block but claims success ‚Üí hallucination.

### 2. CODE OVER DOCS (Direct Action)
**PROHIBITED:** When asked to change settings (port, model, paths), writing "plans" in Markdown files (`instructions.md`, `notes.txt`).

**REQUIRED:** Change configuration ONLY in SOURCE CODE (`.py`, `.json`, `.yaml`, `.ps1`):
1. Find file (grep/search)
2. Read context
3. Replace code string
4. Show `Get-Content` of changed lines for verification

### 3. ENGINEERING APPROACH (Batch Operations)
**PROHIBITED:** Trying to edit many files (>3) "manually" through chat. You will get confused and hallucinate.

**REQUIRED:** For mass operations (e.g., remove links from 100 files):
1. Write Python script (tool) that does this (e.g., `tools/clean_library.py`)
2. Execute script via terminal
3. Show real script output (counters, logs)

### 4. TERMINAL VISIBILITY (Show Your Work)
**PROHIBITED:** Hiding command output with phrases "I ran this behind the scenes".

**REQUIRED:** Full cycle must be visible: Command ‚Üí Console Output ‚Üí Your Analysis
- If command returns error ‚Äî DON'T HIDE IT. Error is information. Show it, we'll fix together.









**–ü–ï–†–ï–î –Ω–∞—á–∞–ª–æ–º —Ä–∞–±–æ—Ç—ã —Å LightRAG –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π:**
–ü—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª `E:\AGENTS\docs\lightrag-troubleshooting.instructions.md`

–≠—Ç–æ—Ç —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏ —Ä–µ—à–µ–Ω–∏—è —Ç–∏–ø–æ–≤—ã—Ö –ø—Ä–æ–±–ª–µ–º:
- –°–æ—Å—Ç–æ—è–Ω–∏–µ —Ö—Ä–∞–Ω–∏—Ç—Å—è –Ω–∞ –¥–∏—Å–∫–µ (kv_store_doc_status.json —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ)
- –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫ 500 (–ø–µ—Ä–µ–≥—Ä—É–∑–∫–∞ –æ—á–µ—Ä–µ–¥–∏, –ù–ï –ø–æ—Ç–µ—Ä—è –¥–∞–Ω–Ω—ã—Ö)
- –ö–æ–Ω—Ñ–ª–∏–∫—Ç Docker Ollama vs –õ–æ–∫–∞–ª—å–Ω—ã–π Ollama
- –ß–µ–∫–ª–∏—Å—Ç –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–º —Å–µ—Ä–≤–µ—Ä–∞
- –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö/–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π

**–ü–†–ê–í–ò–õ–û:** –ü—Ä–∏ –ª—é–±—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å `kv_store_doc_status.json`, –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ troubleshooting —Ñ–∞–π–ª–∞.

## AI_Librarian_Core: –ü–æ–ª–Ω–∞—è –ò—Å—Ç–æ—Ä–∏—è –ü—Ä–æ–µ–∫—Ç–∞

### üéØ –≠–≤–æ–ª—é—Ü–∏—è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

**–§–∞–∑–∞ 1: Docker + Open WebUI (–£–°–¢–ê–†–ï–õ–û)**
- –ò–∑–Ω–∞—á–∞–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞: Qwen 2.5 32B (q4_k_m) ‚Äî **FAILED** (OOM –Ω–∞ 16GB VRAM)
- –û—Ç–∫–∞—Ç –Ω–∞ Qwen 2.5 14B (q4_k_m) ‚Äî **SUCCESS** (~9-10GB VRAM)
- Docker Compose —Å Ollama + Open WebUI –Ω–∞ –ø–æ—Ä—Ç—É 3001
- –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: 9 Floor_*.md ‚Üí 3 —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–∞
- **–ü—Ä–æ–±–ª–µ–º–∞:** –î—É–±–ª–∏–∫–∞—Ç Open WebUI (–æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞ 3000, docker –Ω–∞ 3001)

**–§–∞–∑–∞ 2: –ú–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ LightRAG (–¢–ï–ö–£–©–ï–ï)**
- –û—Ç–∫–∞–∑ –æ—Ç Docker Compose –≤ –ø–æ–ª—å–∑—É –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ Windows Ollama (11434)
- –í–Ω–µ–¥—Ä–µ–Ω–∏–µ LightRAG GraphRAG –¥–ª—è –≥—Ä–∞—Ñ-–ø–æ–∏—Å–∫–∞ –∑–Ω–∞–Ω–∏–π
- FastAPI —Å–µ—Ä–≤–µ—Ä `lightrag_server.py` –Ω–∞ –ø–æ—Ä—Ç—É 8003
- Micro-chunking: 10KB —á–∞–Ω–∫–∏ –≤–º–µ—Å—Ç–æ —Ü–µ–ª—ã—Ö —Ñ–∞–π–ª–æ–≤ (—Ä–µ—à–µ–Ω–∏–µ Ollama 4096 —Ç–æ–∫–µ–Ω-–ª–∏–º–∏—Ç–∞)
- **–ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï:** `nest_asyncio.apply()` –≤ `startup_event()` –¥–ª—è Event Loop

**–§–∞–∑–∞ 3: GPU Optimization (–í –ü–†–û–¶–ï–°–°–ï)**
- –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞: GPU —Ä–∞–±–æ—Ç–∞–ª–∞ –Ω–∞ 13GB –≤–º–µ—Å—Ç–æ 16GB VRAM
- –†–µ—à–µ–Ω–∏–µ: MSI Afterburner +2000 –ú–ì—Ü Memory Clock
- –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª–µ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ GPU —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–∫—Ä–∏–ø—Ç—ã –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

---

## Architecture Overview

**Dual-system architecture:** Two independent AI ecosystems running on the same machine:

### System 1: AGENTS (Multi-Agent Platform)
**Location:** `E:\AGENTS\`  
**Ollama Port:** `11435` (custom)  
**LLM:** `llama3:8b-instruct-q4_K_M`

**Core Agents:**
- **librarian-agent** ‚Äî Document indexing/search with 9-floor library structure (FastAPI port 8003)
- **market-analyzer** ‚Äî Israeli PC parts market scraping/analysis (FastAPI port 8002)
- **open-webui-bridge** ‚Äî Direct filesystem access to Open WebUI container

**Integration:** `User ‚Üí Open WebUI (port 3000) ‚Üí Agent APIs (8002-8003) ‚Üí Ollama (11435) ‚Üí llama3`

### System 2: AI_Librarian_Core (LightRAG GraphRAG)
**Location:** `E:\AI_Librarian_Core\`  
**Ollama Port:** `11434` (default)  
**LLM:** `qwen2.5:14b-instruct-q4_k_m`  
**Architecture:** LightRAG graph-based knowledge system with persistent state

**Integration:** `lightrag_server.py (FastAPI) ‚Üí Ollama (11434) ‚Üí qwen2.5 + nomic-embed-text`

**CRITICAL:** These systems use **different Ollama ports** and **different models**. Never mix configurations.

## Critical Developer Workflows

### Environment Setup (All Agents)
```powershell
# Each agent has isolated venv
cd E:\AGENTS\<agent-name>
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

### Starting Ollama Services

**For AGENTS system (port 11435):**
```powershell
$env:OLLAMA_HOST = "http://127.0.0.1:11435"
curl http://localhost:11435/api/tags
ollama list | Select-String "llama3"
```

**For AI_Librarian_Core (port 11434 - default):**
```powershell
curl http://localhost:11434/api/tags
ollama list | Select-String "qwen2.5"
```

**CRITICAL:** If seeing Docker Ollama conflicts in logs (`/usr/bin/ollama runner`), stop Docker Ollama:
```powershell
docker stop ollama; docker rm ollama
```
LightRAG requires local Windows Ollama for proper GPU discovery.

### Librarian Agent Workflows
```powershell
# ONE-COMMAND START (recommended)
cd E:\AGENTS\librarian-agent
worlds\librarian\scripts\start_librarian_world.ps1

# Manual reindex (E:\AGENTS\Documents ‚Üí library/)
$env:PYTHONPATH = "E:\AGENTS\librarian-agent"
python reindex.py

# Manual API start
.\start_api_server.ps1  # Or: uvicorn openwebui.api_server:app --port 8003

# Integration: Import openwebui/librarian_tool.py into Open WebUI ‚Üí Settings ‚Üí Tools
```

**Key Pattern**: Librarian uses **9-floor structure** (`Floor_01_*.md`, `Floor_02_*.md`...) with JSON indexes. Agent MUST cite sources as `–≠—Ç–∞–∂ X, —Ñ–∞–π–ª Floor_0X_*.md, —Ä–∞–∑–¥–µ–ª F{X}-S{Y}`.

### LightRAG (AI_Librarian_Core) Workflows
```powershell
# Server must run in separate terminal (see Pitfall #2)
cd E:\AI_Librarian_Core
Start-Process powershell -ArgumentList "-NoExit", "-Command", "python lightrag_server.py"

# Wait for server startup, then ingest documents
Start-Sleep 10
python ingest_library.py  # Reads E:\AGENTS\Documents_cleaned

# Check indexing progress (persistent on disk!)
Get-Content lightrag_cache\kv_store_doc_status.json | ConvertFrom-Json

# Monitor VRAM (models loaded = >6GB VRAM)
nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits

# AUTO-COMPLETION: Run indexing to 100%
.\run_index_to_completion.ps1  # Handles restarts, requeues stuck chunks

# AUTO-RESTART: Cycle indexing every 10 min (handles event loop crashes)
.\auto_restart_indexing.ps1

# Check graph files were created
Get-ChildItem lightrag_cache\*.graphml, lightrag_cache\vdb_*.json
```

**State Management:** LightRAG uses **persistent disk state** (`kv_store_doc_status.json`). Never assume restart = data loss. Always check disk state first.

**Chunk Strategy:** Files auto-split into **10KB micro-chunks** (~3-4K tokens) to avoid Ollama 4096 token limit. File 3.3MB = ~330 chunks, takes 15-20 min.

**Critical Files:**
- `lightrag_cache/graph_chunk_entity_relation.graphml` ‚Äî Knowledge graph (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å >100KB –ø–æ—Å–ª–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)
- `lightrag_cache/vdb_entities.json` ‚Äî Entity vectors (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å >500KB)
- `lightrag_cache/kv_store_doc_status.json` ‚Äî Progress state (survives restarts)
- `lightrag_cache/kv_store_full_docs.json` ‚Äî Full document texts (~7MB –¥–ª—è –ø–æ–ª–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏)

**Query Modes:**
```powershell
# Test query via API
$body = @{query="–ß—Ç–æ —Ç–∞–∫–æ–µ –¢–†–ò–ó?"; mode="hybrid"} | ConvertTo-Json
Invoke-RestMethod http://localhost:8003/query -Method Post -Body $body -ContentType "application/json"
```
Modes: `naive` (vector only), `local` (local graph), `global` (global graph), `hybrid` (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

### Market Analyzer Workflows
```powershell
# Activate environment
cd E:\AGENTS\market-analyzer
.\activate.ps1

# Start REST API (port 8002)
uvicorn openwebui.api_server:app --port 8002 --reload

# CLI agent for interactive queries
python scripts\interactive_agent.py

# Data collection from Yad2/Facebook
python scripts\run_collection.py --categories cpu gpu --max-pages 5

# Query database
python -c "from src.storage.database import DatabaseManager; db=DatabaseManager('data/market.db'); print(db.get_recent_listings(10))"
```

**Data Flow:** `collectors/*.py ‚Üí parsers/*.py ‚Üí database.py (SQLite) ‚Üí scoring/*.py ‚Üí llm_agent/agent.py`

### Open WebUI Bridge Workflows
```powershell
# Recreate container with volume mounts to E:\AGENTS\open-webui-bridge\
E:\AGENTS\agents_tools\recreate_openwebui_with_bridge.ps1

# Direct file access (NO docker cp needed)
explorer E:\AGENTS\open-webui-bridge\data\  # Access webui.db directly
Copy-Item "file.pdf" "E:\AGENTS\open-webui-bridge\static\"

# Status check
python E:\AGENTS\agents_tools\bridge_sync.py status

# Backup (before experiments)
python E:\AGENTS\agents_tools\backup_openwebui.py
```

## Project-Specific Conventions

### Configuration Files
- All agents use `config.yaml` (not `.env`) with **Windows absolute paths**: `E:\AGENTS\...`
- Ollama URL: `http://localhost:11435` (NOT default 11434)
- Open WebUI container: `docker://open-webui` with `OLLAMA_BASE_URL=http://host.docker.internal:11435`

### Path Patterns
```python
# ALWAYS use Windows-style absolute paths
documents_path: "E:\\AGENTS\\Documents"
library_path: "E:\\AGENTS\\librarian-agent\\library"

# In Python code
from pathlib import Path
config_path = Path(r"E:\AGENTS\<agent>\config.yaml")
```

### Agent Response Format (Librarian Example)
```
üìç **–ò—Å—Ç–æ—á–Ω–∏–∫:** –≠—Ç–∞–∂ {N}: {–ù–∞–∑–≤–∞–Ω–∏–µ}
üìÑ **–§–∞–π–ª:** Floor_0{N}_{–ù–∞–∑–≤–∞–Ω–∏–µ}.md
üìå **–†–∞–∑–¥–µ–ª:** F{N}-S{Y} - {–ó–∞–≥–æ–ª–æ–≤–æ–∫}
```

**Anti-Pattern**: Never invent filenames (`architecture.md`) ‚Äî use ACTUAL floor files from JSON indexes.

### Testing Integration
```powershell
# Test if Ollama model responds
curl -X POST http://localhost:11435/api/generate -d '{"model":"llama3:8b-instruct-q4_K_M","prompt":"Test"}'

# Check Open WebUI container health
docker exec open-webui curl http://localhost:8080/health

# Verify API server
curl http://localhost:8003/health
```

## Data Flow Patterns

### Librarian Agent Pipeline
```
Documents/ (raw) ‚Üí scanner.py ‚Üí preprocess.py ‚Üí clustering.py (9 floors max)
                                                      ‚Üì
                                            floors_writer.py (Markdown)
                                                      ‚Üì
                                            index_writer.py (JSON)
                                                      ‚Üì
                                    librarian_agent.py (llama3 queries)
```

### Market Analyzer Pipeline
```
Yad2/Facebook ‚Üí collectors/*.py ‚Üí parsers/*.py ‚Üí storage/database.py (SQLite)
                                                          ‚Üì
                                                  scoring/*.py
                                                          ‚Üì
                                            llm_agent/agent.py (llama3)
```

### LightRAG Indexing Pipeline
```
Documents_cleaned/ ‚Üí ingest_library.py (chunking) ‚Üí HTTP POST /insert
                                                          ‚Üì
                                                  lightrag_server.py
                                                          ‚Üì
                                    LightRAG (graph + vectors) ‚Üí lightrag_cache/
                                                          ‚Üì
                                    kv_store_doc_status.json (persistent state)
```

**CRITICAL:** LightRAG server checks `kv_store_doc_status.json` before processing. Chunks marked "processed" are skipped automatically.

## Common Pitfalls

### 1. VRAM Check Before Reporting (CRITICAL)
**–ü–ï–†–ï–î –ª—é–±—ã–º –æ—Ç—á—ë—Ç–æ–º –æ —Å—Ç–∞—Ç—É—Å–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏:**
1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å VRAM —á–µ—Ä–µ–∑ `nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits`
2. –ï—Å–ª–∏ VRAM < 6000 MB (6 GB) ‚Üí **–ø—Ä–æ—Ü–µ—Å—Å –ù–ï —Ä–∞–±–æ—Ç–∞–µ—Ç**, –º–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
3. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ 30 —Å–µ–∫—É–Ω–¥, –µ—Å–ª–∏ –ø–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∏–∑–∫—É—é VRAM
4. –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ VRAM > 6 GB ‚Üí –æ—Ç—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –æ–± —É—Å–ø–µ—à–Ω–æ–π —Ä–∞–±–æ—Ç–µ

```powershell
# CORRECT: Check VRAM before reporting
$vram = nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits
if ($vram -lt 6000) {
    Write-Host "‚ö†Ô∏è VRAM –Ω–∏–∑–∫–∞—è ($vram MB) - –ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –∑–∞–ø—É—â–µ–Ω!" -ForegroundColor Red
    # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –∏–ª–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
    Start-Sleep 30
    $vram2 = nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits
    if ($vram2 -lt 6000) {
        Write-Host "‚ùå –ü—Ä–æ—Ü–µ—Å—Å –ù–ï —Ä–∞–±–æ—Ç–∞–µ—Ç (VRAM: $vram2 MB)" -ForegroundColor Red
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫
    }
}
```

**–ü–†–ê–í–ò–õ–û**: VRAM < 6 GB = –º–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã = –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ù–ï —Ä–∞–±–æ—Ç–∞–µ—Ç. –ù–ï –æ—Ç—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –æ–± —É—Å–ø–µ—Ö–µ –ø–æ–∫–∞ VRAM –Ω–µ > 6 GB.

### 2. Server Startup in Terminals (CRITICAL)
```powershell
# WRONG: Background server in same terminal, then commands kill it
run_in_terminal("python server.py", isBackground=true)
run_in_terminal("curl http://localhost:8003/health")  # KILLS SERVER!

# CORRECT: Start server in SEPARATE PowerShell window
Start-Process powershell -ArgumentList "-NoExit", "-Command", "cd E:\AI_Librarian_Core; python lightrag_server.py"
# Then use current terminal for checks
Start-Sleep 5; curl http://localhost:8003/health
```

**ROOT CAUSE**: Tool simplifies commands to same terminal ‚Üí subsequent command interrupts background process

**APPLIES TO**:
- FastAPI/Uvicorn servers (`python lightrag_server.py`)
- Flask servers
- Ollama serve
- Any long-running background process

**RULE**: ALWAYS use `Start-Process powershell` for servers, NEVER `run_in_terminal(..., isBackground=true)` if planning to run more commands.

### 3. Ollama Port Confusion
```powershell
# WRONG: Default Ollama port for AGENTS system
curl http://localhost:11434/api/tags  # This is AI_Librarian_Core port!

# CORRECT: Custom port for AGENTS system
$env:OLLAMA_HOST = "http://127.0.0.1:11435"
curl http://localhost:11435/api/tags

# Remember:
# - E:\AGENTS\* ‚Üí port 11435 ‚Üí llama3:8b
# - E:\AI_Librarian_Core ‚Üí port 11434 ‚Üí qwen2.5:14b
```

### 2. PYTHONPATH for Librarian
```powershell
# WRONG: Run without PYTHONPATH
python openwebui\api_server.py  # ImportError

# CORRECT:
$env:PYTHONPATH = "E:\AGENTS\librarian-agent"
python openwebui\api_server.py
```

### 3. Docker Ollama Conflicts (LightRAG)
**Symptoms:** Logs show `/usr/bin/ollama runner`, GPU discovery failures, low GPU utilization (<10%)

**Cause:** Docker Ollama (Linux) conflicts with Windows Ollama, can't access GPU properly

**Solution:**
```powershell
# Stop Docker Ollama completely
docker stop ollama; docker rm ollama

# Verify local Ollama works
curl http://localhost:11434/api/tags

# Restart LightRAG server
Get-Process python | Stop-Process -Force
Start-Process powershell -ArgumentList "-NoExit", "-Command", "cd E:\AI_Librarian_Core; python lightrag_server.py"
```

**RULE:** LightRAG requires **local Windows Ollama only** (port 11434). Docker Ollama breaks GPU discovery.

### 3. Open WebUI Bridge Editing
```powershell
# WRONG: Edit webui.db while container running
docker exec open-webui sqlite3 /app/backend/data/webui.db

# CORRECT: Stop container first
docker stop open-webui
start E:\AGENTS\open-webui-bridge\data\webui.db  # DB Browser for SQLite
docker start open-webui
```

### 4. LightRAG State Persistence
**WRONG:** Assuming restart = data loss, restarting indexing from scratch

**CORRECT:** Check persistent state first:
```powershell
# Read current indexing progress
Get-Content E:\AI_Librarian_Core\lightrag_cache\kv_store_doc_status.json | ConvertFrom-Json

# If seeing 500 errors, DON'T restart - just slow down requests
# Add delays in ingest_library.py: time.sleep(1) between chunks
```

**RULE:** `kv_store_doc_status.json` survives restarts. 410 processed chunks stay processed. Always check state before restarting.

### 5. Librarian Index Inconsistency
Check `library/library_index.json` matches actual `Floor_*.md` files. If mismatch, re-run `reindex.py`.

## Key Files Reference

| Pattern | Purpose | Example |
|---------|---------|---------|
| `**/config.yaml` | Agent configuration | Ollama URL, model name, paths |
| `**/requirements.txt` | Python dependencies | `sentence-transformers`, `fastapi`, `requests` |
| `**/*.ps1` | PowerShell scripts | `start_api_server.ps1`, `auto_restart_indexing.ps1` |
| `**/openwebui/api_server.py` | FastAPI backend | Port 8003 (librarian), 8002 (market-analyzer) |
| `**/openwebui/*_tool.py` | Open WebUI Tool | Import into Settings ‚Üí Tools |
| `docs/*.instructions.md` | Agent guidelines | Cosmos DB patterns, lightrag-troubleshooting |
| `**/CHEAT_SHEET.md` | Quick reference docs | AI_Librarian_Core, librarian-agent |
| `**/README.md` | Project documentation | Setup guides, architecture overview |
| `**/*_REPORT.md` | Milestone documentation | DEPLOYMENT_REPORT, PHASE_4_COMPLETE |
| `lightrag_cache/*.json` | LightRAG persistent state | kv_store_doc_status, vdb_entities |
| `lightrag_cache/*.graphml` | Knowledge graph file | graph_chunk_entity_relation |

## Documentation Structure

**AGENTS system:** `E:\AGENTS\docs/`
- `architecture/` ‚Äî System diagrams
- `guides/` ‚Äî Step-by-step procedures
- `reference/` ‚Äî API specs, data formats
- `*.instructions.md` ‚Äî Agent-specific rules (applied via frontmatter `applyTo: '**'`)

**AI_Librarian_Core:** `E:\AI_Librarian_Core/`
- `README.md` ‚Äî Main setup guide (Docker approach, –£–°–¢–ê–†–ï–õ–û)
- `CHEAT_SHEET.md` ‚Äî Quick commands for Docker version
- `QUICKSTART.md` / `QUICKSTART_WEBUI.md` ‚Äî Fast setup guides
- `PHASE_4_COMPLETE.md` ‚Äî LightRAG migration success report
- `CHUNKING_SOLUTION.md` ‚Äî Micro-chunking strategy (10KB chunks)
- `DEPLOYMENT_SUCCESS_nest_asyncio.md` ‚Äî Event loop fix verification
- `ANALYSIS_16GB_VRAM.md` ‚Äî VRAM optimization analysis
- `CONSOLIDATION_REPORT.md` ‚Äî 9 Floor files ‚Üí 3 consolidated files
- `docs/gpu-optimization-todo.md` ‚Äî MSI Afterburner tuning checklist
- `docs/rtx-5060ti-16gb-safe-tuning-roadmap.md` ‚Äî GPU overclocking guide

## Quick Reference

```powershell
# Check what's running
docker ps  # Open WebUI container
netstat -ano | Select-String ":8003"  # Librarian API / LightRAG Server
netstat -ano | Select-String ":11435"  # Ollama (AGENTS)
netstat -ano | Select-String ":11434"  # Ollama (AI_Librarian_Core)

# Logs
docker logs open-webui
Get-Content E:\AGENTS\librarian-agent\library\reindex.log
Get-Content E:\AI_Librarian_Core\lightrag_server.log

# Kill stuck API server
.\stop_api_server.ps1  # Or find PID: Get-Process -Id <PID> | Stop-Process

# LightRAG status check
Get-Content E:\AI_Librarian_Core\lightrag_cache\kv_store_doc_status.json | ConvertFrom-Json
Get-ChildItem E:\AI_Librarian_Core\lightrag_cache\*.graphml
```

## Critical Learnings from AI_Librarian_Core

### 1. nest_asyncio Event Loop Fix
**Problem:** LightRAG's internal `asyncio.run()` conflicts with FastAPI's event loop
**Solution:** Apply `nest_asyncio.apply()` **INSIDE** `@app.on_event("startup")`, NOT at module level
**Why:** Uvicorn creates loop with `loop_factory` parameter that globally-patched asyncio.run() can't handle

```python
# ‚ùå WRONG
import nest_asyncio
nest_asyncio.apply()  # Too early!

app = FastAPI()

# ‚úÖ CORRECT
app = FastAPI()

@app.on_event("startup")
async def startup_event():
    nest_asyncio.apply()  # After loop creation
    # ... initialize LightRAG
```

### 2. Micro-Chunking Strategy
**Problem:** Ollama limit **4096 tokens** (~15K chars), –Ω–æ files –±—ã–ª–∏ 3.3MB
**Evolution:**
- –ü–æ–ø—ã—Ç–∫–∞ 1: 500KB chunks ‚Üí Deadlock (13x overflow)
- –ü–æ–ø—ã—Ç–∫–∞ 2: 200KB chunks ‚Üí Deadlock (13x overflow)
- **–†–µ—à–µ–Ω–∏–µ:** 10KB micro-chunks (~3-4K tokens, 25% safety margin)

**Result:** File 3.3MB = 330 chunks, ~15 min indexing, 100% reliable

### 3. GPU Memory Discovery (MSI Afterburner)
**Problem:** RTX 5060 Ti –ø–æ–∫–∞–∑—ã–≤–∞–ª–∞ 13GB VRAM –≤–º–µ—Å—Ç–æ 16GB
**Root Cause:** Memory Clock –Ω–µ –±—ã–ª overclocked –≤ MSI Afterburner
**Solution:** +2000 MHz Memory Clock ‚Üí –ø–æ–ª–Ω—ã–µ 16GB –¥–æ—Å—Ç—É–ø–Ω—ã
**Files:** `docs/gpu-optimization-todo.md`, `docs/rtx-5060ti-16gb-safe-tuning-roadmap.md`

### 4. Library Consolidation
**Original:** 9 Floor_*.md files (—Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
**Consolidated:** 3 thematic files:
- `01_Architecture_Methodology.md` (3.3 MB) ‚Äî 4 floor files
- `05_Prompts_Instructions.md` (523 KB) ‚Äî 1 floor file
- `Uncategorized.md` (3.1 MB) ‚Äî 4 floor files (Code + Misc)

**Benefit:** Easier RAG indexing, thematic grouping
**Script:** `consolidate_library.py`

### 5. Persistent State Architecture
**Critical Understanding:** LightRAG **NEVER** loses progress on restart
- `kv_store_doc_status.json` survives server crashes
- 410 processed chunks stay processed after reboot
- Error 500 = queue overload, NOT data loss
- **Solution:** Add delays in `ingest_library.py`, requeue stuck "processing" chunks

**Auto-Recovery Scripts:**
- `run_index_to_completion.ps1` ‚Äî Monitors pending count, auto-restarts on stall
- `auto_restart_indexing.ps1` ‚Äî Cycles every 10 min (prevents event loop crashes)
- `requeue_processing.py` ‚Äî Resets chunks stuck in "processing" >10 min

### 6. Docker Ollama Conflicts
**Symptoms:** Logs show `/usr/bin/ollama runner`, GPU discovery failures
**Cause:** Docker Ollama (Linux) can't access Windows GPU properly
**Solution:** Use **local Windows Ollama ONLY** for LightRAG (port 11434)
```powershell
docker stop ollama; docker rm ollama  # Remove Docker Ollama
curl http://localhost:11434/api/tags  # Verify local Ollama
```

### 7. Query Reranking
**Enhancement:** LightRAG results post-processed through qwen2.5:14b
**Purpose:** Improve answer quality for Russian queries
**Implementation:** `rerank_func()` in `lightrag_server.py`
**Modes:** naive, local, global, **hybrid** (recommended)
