# AI Agent Codebase Guide ‚Äî WORLD_OLLAMA

**Purpose:** Guide AI coding agents to be immediately productive in this multi-service AI knowledge system.  
**Updated:** 2025-12-02  
**Version:** 2.1  
**Last Verified:** ORDER 42 complete, v0.3.0-alpha released

---

## üéØ Project At A Glance

**WORLD_OLLAMA** is a local-first AI knowledge system combining:
- **Desktop Client** (Tauri/Svelte) - User interface with 6 panels + Flows automation
- **CORTEX** (LightRAG) - GraphRAG knowledge server (port 8004)
- **Ollama** (qwen2.5:14b) - LLM inference (port 11434)
- **LLaMA Factory** - Model fine-tuning platform
- **Knowledge Base** - 486+ TRIZ documents (7.7 MB)

**Root:** `E:\WORLD_OLLAMA\` (NEVER hardcode - use `WORLD_OLLAMA_ROOT` env var or `get_project_root()`)  
**GPU:** RTX 5060 Ti 16GB VRAM  
**OS:** Windows 11 (PowerShell primary shell)

---

## ‚ö†Ô∏è CRITICAL: Development Protocols

### 1. NO SIMULATION ‚Äî Verify Everything
**PROHIBITED:** Fake terminal output, invented logs, claiming status without proof.

**REQUIRED:** Execute commands and show real output:
```powershell
# Want to say "File exists"? ‚Üí Show proof
Test-Path E:\WORLD_OLLAMA\services\lightrag\data\*.json

# Want to say "Server running"? ‚Üí Verify health
Invoke-RestMethod http://localhost:8004/health

# Want to say "Models loaded"? ‚Üí Check VRAM
nvidia-smi --query-gpu=memory.used --format=csv,noheader
```

### 1.a Agent Interaction Directive ‚Äî No Manual Prompts

–ê–≥–µ–Ω—Ç –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–æ—Å–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤—ã–ø–æ–ª–Ω—è—Ç—å –∫–æ–º–∞–Ω–¥—ã –≤—Ä—É—á–Ω—É—é, –∫—Ä–æ–º–µ —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ –¥–µ–π—Å—Ç–≤–∏–µ –ø–æ —Å–≤–æ–µ–π –ø—Ä–∏—Ä–æ–¥–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∏–ª–∏ —Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è.

- –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: MCP (`myshell/execute_command`) –∏ VS Code Terminal (`run_in_terminal`) –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥.
- –ò—Å–∫–ª—é—á–µ–Ω–∏—è: –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ (–ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã, –∑–∞–ø—É—Å–∫ –æ–∫–æ–Ω Desktop Client), —Ä—É—á–Ω—ã–µ –∫–ª–∏–∫–∏ –≤ UI.
- –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∂–µ –≤—ã–ø–æ–ª–Ω–∏–ª —Ä—É—á–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø—É—Å—Ç–∏–ª —Å–µ—Ä–≤–∏—Å—ã), –∞–≥–µ–Ω—Ç –æ–±—è–∑–∞–Ω –ø–µ—Ä–µ–π—Ç–∏ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å —Ä—É—á–Ω—ã–µ —É–∫–∞–∑–∞–Ω–∏—è.

### 2. CODE OVER DOCS ‚Äî Direct Action
When asked to change settings (ports, models, paths):
- ‚ùå DON'T write plans in markdown
- ‚úÖ DO modify source code (`.py`, `.rs`, `.yaml`, `.ps1`)
- ‚úÖ DO show `Get-Content` verification after changes

### 3. Path Agnosticism ‚Äî Dynamic Root Resolution
**NEVER hardcode** `E:\WORLD_OLLAMA\` in code:

```rust
// ‚úÖ CORRECT - Dynamic resolution
let root = std::env::var("WORLD_OLLAMA_ROOT")
    .or_else(|_| std::env::current_exe()
        .map(|p| p.parent().unwrap().parent().unwrap().to_string_lossy().to_string()))
    .unwrap();

// ‚ùå WRONG - Hardcoded path
let root = "E:\\WORLD_OLLAMA\\";
```

See `TASK 16.1` in consolidated reports for context.

---

### üìã –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –ù–ê–í–ò–ì–ê–¶–ò–Ø –ü–û –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–ò

**–ü–†–ê–í–ò–õ–û:** –ü–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∞–≥–µ–Ω—Ç **–û–ë–Ø–ó–ê–ù**:

1. **–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞:**
   - üîµ Desktop Client / TASK ‚Üí `docs/tasks/TASKS_CONSOLIDATED_REPORT.md`
   - ü§ñ –ú–æ–¥–µ–ª–∏ / –û–±—É—á–µ–Ω–∏–µ ‚Üí `docs/models/MODELS_CONSOLIDATED_REPORT.md`
   - üèóÔ∏è –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ / CORTEX ‚Üí `docs/infrastructure/INFRASTRUCTURE_CONSOLIDATED_REPORT.md`
   - üìä –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ / –ü—Ä–æ–µ–∫—Ç ‚Üí `PROJECT_MAP.md`, `DOCUMENTATION_STRUCTURE_ANALYSIS.md`
   - üì¶ –†–µ–ª–∏–∑ / –°—Ç–∞—Ç—É—Å ‚Üí `docs/release/v0.1.0/`, `PROJECT_STATUS.md`

2. **–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç:**
   ```python
   # –ü—Å–µ–≤–¥–æ–∫–æ–¥
   if query.contains("TASK", "Desktop Client", "UI"):
       read_file("docs/tasks/TASKS_CONSOLIDATED_REPORT.md")
   elif query.contains("model", "training", "TD-010"):
       read_file("docs/models/MODELS_CONSOLIDATED_REPORT.md")
   elif query.contains("CORTEX", "RAG", "Security", "Ollama"):
       read_file("docs/infrastructure/INFRASTRUCTURE_CONSOLIDATED_REPORT.md")
   ```

3. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:**
   - –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —É–ø–æ–º–∏–Ω–∞–µ—Ç –¥—Ä—É–≥–æ–π ‚Üí –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–π
   - –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Üí –Ω–∞–π—Ç–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π TASK –æ—Ç—á—ë—Ç –≤ –∞—Ä—Ö–∏–≤–µ

**–ü–†–ò–ú–ï–† WORKFLOW:**

**–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:** "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Command DSL?"

**–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞:**
1. ‚úÖ –ß–∏—Ç–∞—é `DOCUMENTATION_STRUCTURE_ANALYSIS.md` ‚Üí –æ–ø—Ä–µ–¥–µ–ª—è—é –∫–ª–∞—Å—Ç–µ—Ä: **Desktop Client Tasks**
2. ‚úÖ –í–∏–∂—É —á—Ç–æ TASK 8 (Commands Panel) –≤ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –æ—Ç—á—ë—Ç–µ
3. ‚úÖ –ß–∏—Ç–∞—é `docs/tasks/TASKS_CONSOLIDATED_REPORT.md` ‚Üí –Ω–∞—Ö–æ–∂—É —Ä–∞–∑–¥–µ–ª TASK 8
4. ‚úÖ –ò–∑–≤–ª–µ–∫–∞—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Command DSL
5. ‚úÖ –û—Ç–≤–µ—á–∞—é —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞: `docs/tasks/TASKS_CONSOLIDATED_REPORT.md`, —Ä–∞–∑–¥–µ–ª TASK 8

**–ë–ï–ó —ç—Ç–æ–π –ø—Ä–æ—Ü–µ–¥—É—Ä—ã** ‚Üí –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –Ω–µ –Ω–∞–π—Ç–∏ –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–∞–Ω–Ω—ã–µ

---

### üó∫Ô∏è –ö–ê–†–¢–ê –ù–ê–í–ò–ì–ê–¶–ò–ò –ü–û –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–ò (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê–Ø –°–ü–†–ê–í–ö–ê)

**–î–ª—è –∞–≥–µ–Ω—Ç–∞:** –≠—Ç–æ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ **–í–°–ï–ì–î–ê** –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –ø–∞–º—è—Ç–∏

| –¢–µ–º–∞ –∑–∞–ø—Ä–æ—Å–∞ | –ü–µ—Ä–≤–∏—á–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ | –î–µ—Ç–∞–ª—å–Ω—ã–π –∞—Ä—Ö–∏–≤ |
|--------------|-------------------|-----------------|
| **TASK 4-15 (Desktop Client)** | `docs/tasks/TASKS_CONSOLIDATED_REPORT.md` | `client/TASK_*_REPORT.md` |
| **System Status Panel** | `docs/tasks/TASKS_CONSOLIDATED_REPORT.md` (TASK 4) | `client/TASK4_REPORT.md` |
| **Settings + Profiles** | `docs/tasks/TASKS_CONSOLIDATED_REPORT.md` (TASK 5) | `client/TASK5_REPORT.md` |
| **Library Panel** | `docs/tasks/TASKS_CONSOLIDATED_REPORT.md` (TASK 6-7) | `client/TASK_6_COMPLETION_REPORT.md` |
| **Commands Panel (DSL)** | `docs/tasks/TASKS_CONSOLIDATED_REPORT.md` (TASK 8) | `client/TASK_8_COMPLETION_REPORT.md` |
| **Training UI** | `docs/tasks/TASKS_CONSOLIDATED_REPORT.md` (TASK 12.2) | `client/TASK_12_2_COMPLETION_REPORT.md` |
| **–ú–æ–¥–µ–ª–∏ (TD-010v2/v3)** | `docs/models/MODELS_CONSOLIDATED_REPORT.md` | `docs/TD010v2_DEPLOYMENT_COMPLETE.md` |
| **VRAM Calculator** | `docs/models/MODELS_CONSOLIDATED_REPORT.md` | `docs/qwen3b_training_requirements.md` |
| **CORTEX Configuration** | `docs/infrastructure/INFRASTRUCTURE_CONSOLIDATED_REPORT.md` | `docs/CORTEX_CONFIGURATION_REFERENCE.md` |
| **Security (API Keys)** | `docs/infrastructure/INFRASTRUCTURE_CONSOLIDATED_REPORT.md` | `docs/SECURE_ENCLAVE_REPORT.md` |
| **RAG Quality** | `docs/infrastructure/INFRASTRUCTURE_CONSOLIDATED_REPORT.md` | `docs/reports/RAG_QUALITY_REPORT.md` |
| **Orchestration Scripts** | `docs/infrastructure/INFRASTRUCTURE_CONSOLIDATED_REPORT.md` | `scripts/START_ALL.ps1`, `STOP_ALL.ps1` |
| **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞** | `PROJECT_MAP.md` | `DOCUMENTATION_STRUCTURE_ANALYSIS.md` |
| **–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏** | `docs/project/DOCUMENTATION_STRUCTURE_ANALYSIS.md` | ‚Äî |
| **–ò–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏** | `docs/project/INDEX_NEW.md` | ‚Äî |

---

### üéØ –ö–û–ù–¢–ï–ö–°–¢–ù–´–ï –ü–†–ê–í–ò–õ–ê

**1. –ü—Ä–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–∏ TASK –Ω–æ–º–µ—Ä–∞:**
```python
# –í–°–ï–ì–î–ê –∑–∞–≥—Ä—É–∂–∞—Ç—å –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç –ü–ï–†–í–´–ú
read_file("docs/tasks/TASKS_CONSOLIDATED_REPORT.md")

# –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è ‚Üí –Ω–∞–π—Ç–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
if need_details:
    read_file(f"client/TASK_{task_number}_COMPLETION_REPORT.md")
```

**2. –ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –º–æ–¥–µ–ª—è–º–∏:**
```python
# –í–°–ï–ì–î–ê –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å –≤ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –æ—Ç—á—ë—Ç–µ
read_file("docs/models/MODELS_CONSOLIDATED_REPORT.md")

# –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å ‚Üí –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–µ—Ç–∞–ª–∏
if model_name == "TD-010v2":
    read_file("docs/TD010v2_DEPLOYMENT_COMPLETE.md")
```

**3. –ü—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ–± –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ:**
```python
# –í–°–ï–ì–î–ê –Ω–∞—á–∏–Ω–∞—Ç—å —Å –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞
read_file("docs/infrastructure/INFRASTRUCTURE_CONSOLIDATED_REPORT.md")

# –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –∫–æ–¥ ‚Üí –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–¥ –∏–∑ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞
# –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –≥–æ—Ç–æ–≤—ã–µ code snippets
```

---

### üìä –í–ï–ö–¢–û–†–ù–´–ô –ì–†–ê–§ (ALWAYS IN MEMORY)

**6 —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:**

1. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ (4 —Ñ–∞–π–ª–∞):**
   - `PROJECT_MAP.md` ‚Üí `README.md` ‚Üí `MANUAL.md` ‚Üí `CHANGELOG.md`

2. **–°—Ç–∞—Ç—É—Å –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å (4 —Ñ–∞–π–ª–∞):**
   - `PROJECT_STATUS.md` ‚Üí `docs/project/DOCUMENTATION_STRUCTURE_ANALYSIS.md` ‚Üí `docs/project/INDEX_NEW.md` ‚Üí `docs/project/DOCUMENTATION_REORGANIZATION_COMPLETE.md`

3. **Desktop Client Tasks (11 TASK):**
   - `docs/tasks/TASKS_CONSOLIDATED_REPORT.md` ‚Üí –≤—Å–µ TASK 4-15

4. **–ú–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ (7 –æ—Ç—á—ë—Ç–æ–≤):**
   - `docs/models/MODELS_CONSOLIDATED_REPORT.md` ‚Üí TD-010v2/v3 –æ—Ç—á—ë—Ç—ã

5. **–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (4 –æ—Ç—á—ë—Ç–∞ + 2 –ª–æ–≥–∞):**
   - `docs/infrastructure/INFRASTRUCTURE_CONSOLIDATED_REPORT.md` ‚Üí CORTEX/Security/RAG/Orchestration

6. **UX —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ (8 —Ñ–∞–π–ª–æ–≤):**
   - `UX_SPEC/*.md` ‚Üí UI/UX –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

**–ü–†–ê–í–ò–õ–û:** –ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º ‚Üí –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä ‚Üí –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

---

## üèóÔ∏è Architecture ‚Äî Data Flow Patterns

### Service Communication (Critical Integration Points)

```
Desktop Client (Tauri)
  ‚Üì invoke("start_training_job")
Rust Backend (commands.rs)
  ‚Üì Command::new("pwsh")
PowerShell Script (start_agent_training.ps1)
  ‚Üì llamafactory-cli train
LLaMA Factory Process
  ‚Üì writes training_status.json (PULSE v1)
Rust Singleton Poller (training_manager.rs)
  ‚Üì app.emit("training_status_update")
Svelte UI (TrainingPanel.svelte)
```

**Key Insight:** UI doesn't call Python directly. All training goes through Rust ‚Üí PowerShell ‚Üí llamafactory-cli.

### CORTEX (LightRAG) Request Flow

```
UI API call (client.ts)
  ‚Üì fetch("http://localhost:8004/query")
FastAPI Server (lightrag_server.py)
  ‚Üì rag.query(mode="hybrid")
LightRAG Library
  ‚Üì ollama_model_complete(qwen2.5:14b)
Ollama (port 11434)
  ‚Üì returns context + entities
FastAPI Response
  ‚Üì JSON with answer + sources
UI renders with chain-of-thought
```

**Key Insight:** `mode="hybrid"` is recommended (adaptive local/global search). `enable_rerank=False` due to LightRAG bug.

---

## üöÄ Critical Developer Workflows

### Starting Services (REQUIRED for development)

```powershell
# ONE-COMMAND START (recommended)
pwsh E:\WORLD_OLLAMA\scripts\START_ALL.ps1

# Verify all services running
pwsh E:\WORLD_OLLAMA\scripts\CHECK_STATUS.ps1 -Detailed

# Expected output:
# ‚úì Ollama (11434): Running
# ‚úì CORTEX (8004): Running  
# ‚óã Neuro-Terminal (8501): Down (optional)
```

**CRITICAL:** Desktop Client expects CORTEX running. If `CHECK_STATUS` shows CORTEX down, troubleshoot before running client.

### Desktop Client Development

```powershell
# Terminal 1: Keep services running (see above)

# Terminal 2: Run Tauri dev mode
cd E:\WORLD_OLLAMA\client
npm run tauri dev  # Opens window automatically

# Hot reload: Edit .svelte files ‚Üí auto-refresh
# Rust changes: Requires manual restart (Ctrl+C ‚Üí re-run)
```

### Training a Model (E2E Workflow)

```powershell
# 1. Verify services
pwsh scripts\CHECK_STATUS.ps1

# 2. Option A: Via UI
#    - Open Desktop Client ‚Üí Training Panel
#    - Select profile (e.g., "triz_engineer")
#    - Set epochs (1-5)
#    - Click "Start Training"

# 3. Option B: Via Script (for testing)
pwsh scripts\start_agent_training.ps1 `
  -ProfileName "triz_engineer" `
  -DataPath "E:\WORLD_OLLAMA\services\llama_factory\data\triz_synthesis_v1.jsonl" `
  -OutputDir "E:\WORLD_OLLAMA\models\test_output" `
  -Epochs 1

# 4. Monitor PULSE status
Get-Content E:\WORLD_OLLAMA\services\llama_factory\training_status.json

# 5. Check logs
Get-Content E:\WORLD_OLLAMA\logs\training\train-*.log -Tail 20
```

**Known Issue (ORDER 43):** Training fails if HuggingFace model is gated. Either:
- Login: `huggingface-cli login` (requires HF token)
- OR switch to open model in `llama3_lora_sft.yaml`

### VRAM Monitoring (GPU Health Check)

```powershell
# Check VRAM usage (models loaded when >6GB)
nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader

# Expected when CORTEX indexing:
# 8500, 16384  (8.5GB used, healthy)

# If <6000 MB ‚Üí Models NOT loaded, indexing broken
```

**RULE:** VRAM <6GB = indexing not working. Don't report success until VRAM >6GB.

---

## üìö Documentation Navigation Protocol

### Quick Reference Map

| Topic | Primary Source | Detailed Archive |
|-------|---------------|------------------|
| **Desktop Client (TASK 4-16)** | `docs/tasks/TASKS_CONSOLIDATED_REPORT.md` | `client/docs/TASK_*_REPORT.md` |
| **Models (TD-010v2/v3)** | `docs/models/MODELS_CONSOLIDATED_REPORT.md` | `docs/TD010v2_DEPLOYMENT_COMPLETE.md` |
| **Infrastructure (CORTEX)** | `docs/infrastructure/INFRASTRUCTURE_CONSOLIDATED_REPORT.md` | `docs/CORTEX_CONFIGURATION_REFERENCE.md` |
| **Architecture** | `PROJECT_MAP.md`, `README.md` | `DOCUMENTATION_INDEX.md` |
| **Current Status** | `PROJECT_STATUS_SNAPSHOT_v4.0.md` | `CHANGELOG_v0.3.0.md` |

### Context Gathering Strategy

When working on a task:

1. **Identify cluster** (Desktop/Models/Infrastructure/Architecture)
2. **Read consolidated report** for overview
3. **Check detailed reports** only if needed
4. **Verify with code** (don't trust docs alone)

**Example:** "How does Command DSL work?"
```
1. Category: Desktop Client
2. Read: docs/tasks/TASKS_CONSOLIDATED_REPORT.md ‚Üí TASK 8
3. If need code: grep "parseCommand" client/src/lib/
4. Verify: Check client/src/lib/components/CommandsPanel.svelte
```

---

## üîç Key Files Reference

| File Pattern | Purpose | Example |
|--------------|---------|---------|
| `scripts/*.ps1` | Service orchestration | `START_ALL.ps1`, `CHECK_STATUS.ps1` |
| `client/src-tauri/src/commands.rs` | All Tauri commands (1063 lines) | `start_training_job`, `execute_agent_command` |
| `client/src-tauri/src/flow_manager.rs` | Flows automation backend | `FlowExecutor`, `FlowLogger` |
| `client/src-tauri/src/training_manager.rs` | PULSE v1 implementation | `TrainingStatus`, polling logic |
| `client/src/lib/api/client.ts` | Frontend API client | All `invoke()` wrappers |
| `client/src/lib/components/FlowsPanel.svelte` | Flows UI | Flow cards, execution history |
| `automation/flows/*.json` | Flow definitions | `quick_status.json`, `index_and_train.json` |
| `services/lightrag/lightrag_server.py` | CORTEX FastAPI server (756 lines) | `/query`, `/insert`, `/health` |
| `services/llama_factory/config/*.yaml` | Training configurations | `llama3_lora_sft.yaml` |
| `docs/tasks/TASKS_CONSOLIDATED_REPORT.md` | TASK 4-16 complete reference | UI implementation details |
| `docs/models/MODELS_CONSOLIDATED_REPORT.md` | Model training reports | TD-010v2 eval_loss: 0.8591 |
| `docs/infrastructure/INFRASTRUCTURE_CONSOLIDATED_REPORT.md` | CORTEX, Security, RAG | Configuration reference |

---

## üõ†Ô∏è Build & Release

### Development Build

```powershell
cd E:\WORLD_OLLAMA\client

# Frontend only (no Rust rebuild)
npm run dev

# Full Tauri dev (hot reload)
npm run tauri dev
```

### Production Build

```powershell
# Automated build script (recommended)
pwsh E:\WORLD_OLLAMA\scripts\BUILD_RELEASE.ps1

# Manual (if script fails)
cd E:\WORLD_OLLAMA\client
npm run tauri build

# Output: client/src-tauri/target/release/tauri_fresh.exe
```

**Requirements:**
- Windows SDK 10.0+
- MSVC Build Tools 2022
- Rust 1.75+

### Testing

```powershell
# Core Bridge integration tests
pwsh E:\WORLD_OLLAMA\client\run_auto_tests.ps1

# System Status (3 scenarios)
pwsh E:\WORLD_OLLAMA\client\test_task4_scenarios.ps1

# Settings (5 scenarios)
pwsh E:\WORLD_OLLAMA\client\test_task5_settings.ps1

# E2E smoke test
pwsh E:\WORLD_OLLAMA\USER\TEST_E2E.ps1
```

---

## üí° Critical Learnings (Hard-Won Knowledge)

### 1. Micro-Chunking for Large Files
**Problem:** Ollama limit 4096 tokens (~15K chars), but files were 3.3MB  
**Solution:** 10KB chunks (~3-4K tokens, 25% safety margin)  
**Result:** 3.3MB file = 330 chunks, ~15 min indexing, 100% reliable

See: `services/lightrag/lightrag_server.py` chunking logic

### 2. GPU Memory Discovery (MSI Afterburner)
**Problem:** RTX 5060 Ti showed 13GB VRAM instead of 16GB  
**Root Cause:** Memory Clock not overclocked  
**Solution:** +2000 MHz Memory Clock ‚Üí full 16GB available  

Files: `docs/gpu-optimization-todo.md`, `docs/rtx-5060ti-16gb-safe-tuning-roadmap.md`

### 3. Docker Ollama Breaks GPU (Windows)
**Symptoms:** Logs show `/usr/bin/ollama runner`, low GPU utilization (<10%)  
**Cause:** Docker Ollama (Linux) can't access Windows GPU properly  
**Solution:** Stop Docker Ollama, use only local Windows Ollama

```powershell
docker stop ollama; docker rm ollama
curl http://localhost:11434/api/tags  # Verify local works
```

### 4. LightRAG State Persistence
**WRONG:** Assuming restart = data loss, reindexing from scratch  
**CORRECT:** `data/kv_store_doc_status.json` survives restarts

```powershell
# Check progress before reindexing
$docs = Get-Content services\lightrag\data\kv_store_doc_status.json | ConvertFrom-Json
$docs.PSObject.Properties.Name.Count  # Shows indexed documents
```

---

## üö¶ Quick Commands Reference

```powershell
# === SERVICE MANAGEMENT ===
pwsh scripts\START_ALL.ps1              # Start Ollama + CORTEX (+ optional Neuro-Terminal)
pwsh scripts\STOP_ALL.ps1               # Stop all services
pwsh scripts\CHECK_STATUS.ps1           # Health check (single)
pwsh scripts\CHECK_STATUS.ps1 -Detailed # With response times

# === MONITORING ===
nvidia-smi --query-gpu=memory.used,utilization.gpu --format=csv  # GPU stats
netstat -ano | Select-String ":8004"    # Check CORTEX port
Get-Content services\lightrag\logs\cortex.log -Tail 20  # CORTEX logs

# === DEVELOPMENT ===
cd client; npm run tauri dev            # Start Desktop Client dev mode
pwsh client\run_auto_tests.ps1          # Run tests

# === TRAINING ===
pwsh scripts\start_training_ui.ps1      # Launch LLaMA Board (web UI)
Get-Content services\llama_factory\training_status.json  # PULSE v1 status

# === TROUBLESHOOTING ===
Get-Process python | Where-Object {$_.CommandLine -like "*lightrag*"} | Stop-Process
ollama list | Select-String "qwen|nomic"  # Check models
```

---

## üîÑ Hybrid Execution Strategy (MCP + Terminal)

**Principle:** Use the right tool for the right job

### Use `myshell/execute_command` (MCP) for:

‚úÖ **Automated Testing & Validation** (TASK 51 pattern)
```powershell
# Structured output needed for analysis
myshell/execute_command: "cargo check"
myshell/execute_command: "npm run check"
myshell/execute_command: "Get-Command script.ps1"
```

‚úÖ **Health Checks** (agent needs exitCode for logic)
```powershell
myshell/execute_command: "Test-NetConnection localhost -Port 8004"
myshell/execute_command: "ollama list | Select-String qwen"
```

‚úÖ **Quick Information Retrieval** (<2 min, for agent parsing)
```powershell
myshell/execute_command: "git status --porcelain"
myshell/execute_command: "Get-Content config.json | ConvertFrom-Json"
```

**Why MCP:** Structured JSON output (`exitCode`, `stdout`, `stderr`), isolated process, agent can parse results.

---

### Use `run_in_terminal` (VS Code) for:

‚úÖ **Presentation & Demonstration** (user observes)
```powershell
run_in_terminal: "pwsh scripts/START_ALL.ps1"
run_in_terminal: "nvidia-smi"
run_in_terminal: "cargo build --release"  # Show progress bars
```

‚úÖ **Background Processes** (services, dev servers)
```powershell
run_in_terminal(isBackground=true): "npm run tauri dev"
run_in_terminal(isBackground=true): "pwsh scripts/start_lightrag.ps1"
```

‚úÖ **Long Operations** (>2 min, avoid MCP timeout)
```powershell
run_in_terminal: "npm install"
run_in_terminal: "docker build ."
run_in_terminal: "pwsh scripts/train_model.ps1"
```

‚úÖ **Interactive/Debugging** (requires user input)
```powershell
run_in_terminal: "python -m pdb script.py"
```

**Why Terminal:** Visual output, colors, progress bars, no timeout risk, user control.

---

### Decision Tree:

```
Command execution needed?
‚îú‚îÄ Result needed for agent logic? ‚Üí Time < 2 min?
‚îÇ  ‚îú‚îÄ YES ‚Üí ‚úÖ MCP (structured output)
‚îÇ  ‚îî‚îÄ NO ‚Üí ‚ö†Ô∏è Terminal (avoid timeout)
‚îî‚îÄ User observes execution? OR Background process?
   ‚îî‚îÄ YES ‚Üí ‚úÖ Terminal (presentation/services)
```

**Reference:** `docs/tasks/HYBRID_EXECUTION_STRATEGY_ANALYSIS.md`

---

## üîß MCP Auto-Activation (Phase 1 v0.4.0)

**CRITICAL:** MCP Server settings apply **AUTOMATICALLY** ‚Äî no manual activation needed.

### What Works Automatically (02.12.2025)

‚úÖ **Terminal Settings** (from `.vscode/settings.json`):
- Persistent Sessions enabled
- Shell Integration enabled  
- Environment variable `VSCODE_AGENT_ENABLED=1` set in new terminals

‚úÖ **MCP Server** (`myshell/execute_command`):
- Registered via `github.copilot.chat.mcp.servers` in settings
- Starts automatically on first Copilot request
- **NO NEED to check availability** ‚Äî tool is always ready

‚úÖ **Base64 Encoding** (v1.2.0):
- Auto-detects problematic characters: `|`, `{}`, `$`, `"`, `'`, `` ` ``
- Applies encoding automatically (100% accuracy, 17/17 tests passed)
- **NO NEED to specify `useEncodedCommand`** ‚Äî regex handles it

### What Agent MUST NOT Do

‚ùå **DON'T** ask "Should I activate MCP server?"  
‚ùå **DON'T** check if `execute_command` tool exists before using it  
‚ùå **DON'T** ask "Should I use Base64 Encoding for this command?"  
‚ùå **DON'T** manually specify `useEncodedCommand` parameter

### What Agent SHOULD Do

‚úÖ **DO** call `myshell/execute_command` directly for any PowerShell command  
‚úÖ **DO** trust auto-detection for complex commands (pipes, braces, variables)  
‚úÖ **DO** use MCP for structured output (Exit Code, stdout, stderr parsing)  
‚úÖ **DO** use Terminal for visual demos or background processes

### Example (Correct Usage)

```typescript
// ‚úÖ CORRECT - Direct call, auto-detection handles encoding
myshell/execute_command: "Get-Process | Where-Object { $_.CPU -gt 1 } | Select-Object -First 5"

// Result: Exit Code 0 (Base64 applied automatically)
```

```typescript
// ‚ùå WRONG - Unnecessary manual check
Agent: "Should I use Base64 Encoding for this command?"
User: (confused ‚Äî it's automatic)
```

### Restart Required (One Time Only)

After updating `.vscode/settings.json` or `mcp-shell/server.ts`:
1. `Ctrl+Shift+P` ‚Üí `Developer: Reload Window`
2. Wait 2-3 seconds
3. ‚úÖ All settings active automatically

**Reference:** `docs/infra/MCP_AUTO_ACTIVATION_VERIFICATION.md`

---

## ‚ö° Task Tracking & Versioning

**Current Status:** v0.3.0-alpha (ORDER 42 complete, ORDER 43 pending)

| Version | Status | Key Features |
|---------|--------|--------------|
| **v0.1.0** | ‚úÖ Released 27.11.2025 | Desktop Client MVP (TASK 4-15) |
| **v0.2.0** | ‚úÖ Released 29.11.2025 | PULSE v1, Safe Git Assistant |
| **v0.3.0-alpha** | ‚úÖ Released 30.11.2025 | Flows Automation (ORDER 35-38, 42) |
| **v0.3.1** | üìã Planned | ORDER 37 fix (INDEX paths), ORDER 43 (HF auth) |

**Active Blockers:**
- üî¥ ORDER 37: INDEX path resolution (production blocker)
- üü° ORDER 43: HuggingFace authentication (optional for training)

See `PROJECT_STATUS_SNAPSHOT_v4.0.md` for detailed status.

---

## üìñ Further Reading

- **Full Architecture:** `PROJECT_MAP.md`
- **User Manual:** `MANUAL.md`
- **All Tasks:** `docs/tasks/TASKS_CONSOLIDATED_REPORT.md`
- **All Changes:** `CHANGELOG.md`, `CHANGELOG_v0.3.0.md`
- **Complete Index:** `DOCUMENTATION_INDEX.md` (68 markdown files)

---

_This guide prioritizes actionable technical knowledge over aspirational practices. Focus on discoverable patterns, not documentation-only claims._
