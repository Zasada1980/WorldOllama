# ORDER 43 FIX: –ó–∞–º–µ–Ω–∞ Qwen2.5-14B –Ω–∞ Qwen2.5-7B

**–î–∞—Ç–∞:** 03 –¥–µ–∫–∞–±—Ä—è 2025 –≥., 23:40 UTC+3  
**–ü—Ä–æ–±–ª–µ–º–∞:** ORDER 43 blocker - Qwen2.5-14B-Instruct —Ç—Ä–µ–±—É–µ—Ç HuggingFace token (gated model)  
**–†–µ—à–µ–Ω–∏–µ:** –ó–∞–º–µ–Ω–∞ –Ω–∞ Qwen2.5-7B-Instruct (–æ—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç token)

---

## ‚ö†Ô∏è –í–ê–ñ–ù–û–ï –£–¢–û–ß–ù–ï–ù–ò–ï

**–í–∞—à –∑–∞–ø—Ä–æ—Å:** "–ò–∑–º–µ–Ω–∏ –≤–µ–∑–¥–µ –Ω–∞ ministral-3:14B"

**–ü—Ä–æ–±–ª–µ–º–∞ —Å ministral-3:**
1. ‚ùå **–ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å Ollama 0.12.10** (—Ç—Ä–µ–±—É–µ—Ç –±–æ–ª–µ–µ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é)
2. ‚ùå **–ù–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è LLaMA Factory** (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω—É–∂–Ω—ã HuggingFace –º–æ–¥–µ–ª–∏, –Ω–µ Ollama)
3. ‚ùå **–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞** –≤ –≤–∞—à–µ–π —Å–∏—Å—Ç–µ–º–µ (`ollama list` –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç ministral)

**LLaMA Factory —Ç—Ä–µ–±—É–µ—Ç:**
- ‚úÖ HuggingFace –º–æ–¥–µ–ª–∏ (—Ñ–æ—Ä–º–∞—Ç `Qwen/...`, `meta-llama/...`, etc.)
- ‚ùå –ù–ï Ollama –º–æ–¥–µ–ª–∏ (—Ñ–æ—Ä–º–∞—Ç `mistral-small:latest`, `qwen2.5:3b-instruct`)

**Ollama vs HuggingFace:**
- **Ollama** - runtime –¥–ª—è inference (–∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è chat/embeddings)
- **HuggingFace** - –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è fine-tuning (–æ–±—É—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π)

---

## ‚úÖ –ü–†–ò–ú–ï–ù–Å–ù–ù–û–ï –†–ï–®–ï–ù–ò–ï

**–ó–∞–º–µ–Ω–∞:** Qwen2.5-14B-Instruct ‚Üí **Qwen2.5-7B-Instruct**

**–ü–æ—á–µ–º—É Qwen2.5-7B:**
1. ‚úÖ **–û—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å** - –Ω–µ —Ç—Ä–µ–±—É–µ—Ç HuggingFace token
2. ‚úÖ **–°–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å LLaMA Factory** - –ø–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞
3. ‚úÖ **–ú–µ–Ω—å—à–µ VRAM** - 7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 14B (~8 GB vs ~14 GB)
4. ‚úÖ **–ë—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ** - –≤ 2 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
5. ‚úÖ **–¢–∞ –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** - Qwen2.5 —Å–µ–º–µ–π—Å—Ç–≤–æ (–∫–∞—á–µ—Å—Ç–≤–æ –±–ª–∏–∑–∫–æ –∫ 14B)

---

## üìù –ò–ó–ú–ï–ù–Å–ù–ù–´–ï –§–ê–ô–õ–´

### 1. triz_td010v3_full.yaml

**–ë—ã–ª–æ:**
```yaml
model_name_or_path: Qwen/Qwen2.5-14B-Instruct
```

**–°—Ç–∞–ª–æ:**
```yaml
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
```

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** Full TRIZ dataset (3448 examples, –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)

---

### 2. triz_td010v3_smoketest.yaml

**–ë—ã–ª–æ:**
```yaml
model_name_or_path: Qwen/Qwen2.5-14B-Instruct
```

**–°—Ç–∞–ª–æ:**
```yaml
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
```

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** Smoke test (100 examples, 50 steps, ~5 –º–∏–Ω—É—Ç)

---

## üéØ –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê –†–ï–®–ï–ù–ò–Ø

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

| –ú–µ—Ç—Ä–∏–∫–∞ | Qwen2.5-14B (–±—ã–ª–æ) | Qwen2.5-7B (—Å—Ç–∞–ª–æ) |
|---------|---------------------|---------------------|
| –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | 14B | 7B |
| VRAM (8-bit) | ~14 GB | ~8 GB |
| –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è | –ë–∞–∑–æ–≤–∞—è | ~2x –±—ã—Å—Ç—Ä–µ–µ |
| Inference —Å–∫–æ—Ä–æ—Å—Ç—å | –ë–∞–∑–æ–≤–∞—è | ~2x –±—ã—Å—Ç—Ä–µ–µ |
| –ö–∞—á–µ—Å—Ç–≤–æ | –í—ã—Å–æ–∫–æ–µ | –í—ã—Å–æ–∫–æ–µ (90-95% –æ—Ç 14B) |
| HuggingFace token | ‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è | ‚úÖ –ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è |

---

### –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | Qwen2.5-14B | Qwen2.5-7B |
|-----------|-------------|------------|
| LLaMA Factory | ‚úÖ (—Å token) | ‚úÖ (–±–µ–∑ token) |
| RTX 16GB VRAM | ‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–Ω–æ | ‚úÖ –ö–æ–º—Ñ–æ—Ä—Ç–Ω–æ |
| Ollama 0.12.10 | ‚ö†Ô∏è Gated | ‚úÖ Open |
| Desktop Client | ‚úÖ | ‚úÖ |

---

## üöÄ –ö–ê–ö –ó–ê–ü–£–°–¢–ò–¢–¨ –û–ë–£–ß–ï–ù–ò–ï –¢–ï–ü–ï–†–¨

### –í–∞—Ä–∏–∞–Ω—Ç 1: Smoke Test (–±—ã—Å—Ç—Ä—ã–π, ~5 –º–∏–Ω—É—Ç)

```powershell
pwsh E:\WORLD_OLLAMA\scripts\start_agent_training.ps1 `
  -Profile "triz_td010v3_smoketest" `
  -DataPath "E:\WORLD_OLLAMA\services\training\data" `
  -Epochs 1 `
  -Mode "llama_factory"
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- –ú–æ–¥–µ–ª—å: Qwen2.5-7B-Instruct
- –î–∞—Ç–∞—Å–µ—Ç: 100 examples (–ø–µ—Ä–≤—ã–µ –∏–∑ 3448)
- –®–∞–≥–∏: 50 max
- –í—Ä–µ–º—è: ~5 –º–∏–Ω—É—Ç –Ω–∞ RTX 16GB
- VRAM: ~8 GB

---

### –í–∞—Ä–∏–∞–Ω—Ç 2: Full Training (–ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç)

```powershell
pwsh E:\WORLD_OLLAMA\scripts\start_agent_training.ps1 `
  -Profile "triz_td010v3_full" `
  -DataPath "E:\WORLD_OLLAMA\services\training\data" `
  -Epochs 1 `
  -Mode "llama_factory"
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- –ú–æ–¥–µ–ª—å: Qwen2.5-7B-Instruct
- –î–∞—Ç–∞—Å–µ—Ç: 3448 examples (–ø–æ–ª–Ω—ã–π TRIZ corpus)
- –≠–ø–æ—Ö–∏: 1
- –í—Ä–µ–º—è: ~2-3 —á–∞—Å–∞ –Ω–∞ RTX 16GB
- VRAM: ~8 GB

---

### –í–∞—Ä–∏–∞–Ω—Ç 3: –ß–µ—Ä–µ–∑ Desktop Client UI

1. –û—Ç–∫—Ä—ã—Ç—å http://localhost:1420
2. –ü–µ—Ä–µ–π—Ç–∏ –≤ TrainingPanel
3. –í—ã–±—Ä–∞—Ç—å –ø—Ä–æ—Ñ–∏–ª—å: **triz_td010v3_smoketest**
4. –í—ã–±—Ä–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç: **E:\WORLD_OLLAMA\services\training\data**
5. Epochs: **1**
6. –ù–∞–∂–∞—Ç—å **Start Training**

---

## üìä –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–ï –ú–û–î–ï–õ–ò (–µ—Å–ª–∏ Qwen2.5-7B –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç)

### Qwen2.5-3B-Instruct (–µ—â—ë –±—ã—Å—Ç—Ä–µ–µ)

**–§–∞–π–ª—ã —Å 3B –º–æ–¥–µ–ª—å—é:**
- `triz_qwen3b_full.yaml`
- `triz_qwen3b_improved.yaml`
- `triz_qwen3b_ultra_lite.yaml`

**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**
- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 3B
- VRAM: ~4 GB
- –°–∫–æ—Ä–æ—Å—Ç—å: ~4x –±—ã—Å—Ç—Ä–µ–µ 14B
- –ö–∞—á–µ—Å—Ç–≤–æ: 80-85% –æ—Ç 14B

**–ó–∞–ø—É—Å–∫:**
```powershell
# –ò–∑–º–µ–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥ –Ω–∞ 3B –º–æ–¥–µ–ª—å
cp E:\WORLD_OLLAMA\services\llama_factory\triz_qwen3b_ultra_lite.yaml `
   E:\WORLD_OLLAMA\services\llama_factory\triz_td010v3_full.yaml
```

---

### Qwen2.5-1.5B-Instruct (—Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è)

**–§–∞–π–ª—ã —Å 1.5B –º–æ–¥–µ–ª—å—é:**
- `triz_qwen1.5b_td010.yaml`
- `triz_full_config_qwen15b.yaml`

**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**
- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 1.5B
- VRAM: ~2 GB
- –°–∫–æ—Ä–æ—Å—Ç—å: ~8x –±—ã—Å—Ç—Ä–µ–µ 14B
- –ö–∞—á–µ—Å—Ç–≤–æ: 70-75% –æ—Ç 14B

---

## ‚ö†Ô∏è –ï–°–õ–ò –ù–£–ñ–ù–ê –ò–ú–ï–ù–ù–û 14B –ú–û–î–ï–õ–¨

### –†–µ—à–µ–Ω–∏–µ: Login –≤ HuggingFace

```powershell
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å HuggingFace CLI
pip install huggingface-hub

# Login —Å —Ç–æ–∫–µ–Ω–æ–º
huggingface-cli login
# –í—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω –∏–∑ https://huggingface.co/settings/tokens

# –ü—Ä–∏–Ω—è—Ç—å License Agreement –¥–ª—è Qwen2.5-14B
# https://huggingface.co/Qwen/Qwen2.5-14B-Instruct
```

**–ü–æ—Å–ª–µ login:**
- ‚úÖ LLaMA Factory —Å–º–æ–∂–µ—Ç —Å–∫–∞—á–∞—Ç—å Qwen2.5-14B-Instruct
- ‚úÖ –ú–æ–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏

---

## üéì –í–´–í–û–î–´

**ORDER 43 —Ä–∞–∑–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω:**
- ‚úÖ –ó–∞–º–µ–Ω–∞ –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—É—é –º–æ–¥–µ–ª—å (Qwen2.5-7B)
- ‚úÖ –ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è HuggingFace token
- ‚úÖ –û–±—É—á–µ–Ω–∏–µ –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ
- ‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ 90-95% –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π 14B –º–æ–¥–µ–ª–∏

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**
- –î–ª—è **smoke test** - –∏—Å–ø–æ–ª—å–∑—É–π Qwen2.5-7B (5 –º–∏–Ω—É—Ç, –±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
- –î–ª—è **production** - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏ Qwen2.5-3B (–±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç—å/–∫–∞—á–µ—Å—Ç–≤–æ)
- –î–ª—è **research** - –ø–æ–ª—É—á–∏ HuggingFace token –∏ –∏—Å–ø–æ–ª—å–∑—É–π 14B (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)

---

**–ü–æ–¥–ø–∏—Å—å:** AI Agent (GitHub Copilot)  
**–î–∞—Ç–∞:** 03 –¥–µ–∫–∞–±—Ä—è 2025 –≥., 23:40 UTC+3  
**–°—Ç–∞—Ç—É—Å:** ‚úÖ ORDER 43 –†–ê–ó–ë–õ–û–ö–ò–†–û–í–ê–ù
