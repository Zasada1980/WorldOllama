# TD-010v3 (Qwen2.5-3B) Research-Based Training Analysis

**–î–∞—Ç–∞:** 27 –Ω–æ—è–±—Ä—è 2025 –≥.  
**–°—Ç–∞—Ç—É—Å:** ‚úÖ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û  
**–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:** 2:54 (174 —Å–µ–∫—É–Ω–¥—ã)

---

## üìä –§–∏–Ω–∞–ª—å–Ω—ã–µ –ú–µ—Ç—Ä–∏–∫–∏

| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ | –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å TD-010v2 (1.5B) |
|---------|----------|----------------------------|
| **train_loss** | 0.9826 | -0.0420 (–ª—É—á—à–µ) |
| **eval_loss** | 1.0026 | +0.0668 (—Ö—É–∂–µ) |
| **–†–∞–∑–º–µ—Ä –∞–¥–∞–ø—Ç–µ—Ä–∞** | 7.05 MB | -28.22 MB (-80%) |
| **Trainable params** | 1,843,200 | –ú–µ–Ω—å—à–µ (—Ç–æ–ª—å–∫–æ q_proj, v_proj) |
| **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è** | 174 —Å–µ–∫ | –ë—ã—Å—Ç—Ä–µ–µ (–º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) |
| **VRAM peak** | ~3.1 GB | –ù–∞–º–Ω–æ–≥–æ –º–µ–Ω—å—à–µ 16 GB |

---

## üéØ –ß—Ç–æ –ò–∑–º–µ–Ω–∏–ª–æ—Å—å vs –ü—Ä–æ–≤–∞–ª–∏–≤—à–∏–µ—Å—è –ü–æ–ø—ã—Ç–∫–∏?

### ‚ùå Ultra-Lite Config (OOM –Ω–∞ step 2/51)

```yaml
optim: adamw_torch_fused  # WRONG!
lora_target: q_proj,v_proj,o_proj,gate_proj,up_proj  # 5 –º–æ–¥—É–ª–µ–π
lora_rank: 4
cutoff_len: 384
per_device_train_batch_size: 1
trainable_params: 5,271,552
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** OOM (Out of Memory)

### ‚úÖ Research-Based Config (SUCCESS!)

```yaml
optim: adamw_8bit  # CRITICAL CHANGE!
lora_target: q_proj,v_proj  # –¢–æ–ª—å–∫–æ 2 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥—É–ª—è
lora_rank: 8  # –í—ã—à–µ, –Ω–æ –º–µ–Ω—å—à–µ –º–æ–¥—É–ª–µ–π
cutoff_len: 512
per_device_train_batch_size: 2  # –£–¥–≤–æ–µ–Ω–∏–µ batch size
trainable_params: 1,843,200  # -65% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤!
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ 51/51 —à–∞–≥–æ–≤, eval_loss 1.0026

---

## üî¨ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ò–Ω—Å–∞–π—Ç: –†–æ–ª—å –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞

**–ö–ª—é—á–µ–≤–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ:** `adamw_8bit` vs `adamw_torch_fused`

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤:

| –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä | VRAM State Buffers | –¢–æ—á–Ω–æ—Å—Ç—å | –°–∫–æ—Ä–æ—Å—Ç—å | –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å |
|-------------|-------------------|----------|----------|--------------|
| **adamw_torch_fused** | ~0.12-0.15 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **adamw_8bit** | ~0.08 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**–≠–∫–æ–Ω–æ–º–∏—è:** **2-3 GB VRAM** –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ `adamw_8bit`

**–ü–æ—á–µ–º—É —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è RTX 5060 Ti 16GB?**

- Mini-test (10 samples) –≤–ª–µ–∑–∞–ª –≤ VRAM —Å fused optimizer
- Full dataset (300 samples) —Å–æ–∑–¥–∞–≤–∞–ª **VRAM pressure** ‚Üí OOM
- adamw_8bit –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç **–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞** –¥–ª—è —Ä–∞–±–æ—Ç—ã

**–ò—Å—Ç–æ—á–Ω–∏–∫:** RTX 5060 Ti Technical Report, Section 3.2:  
> "8-bit optimizers significantly reduce VRAM for optimizer states in QLoRA training"

---

## üÜö –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Baseline (TD-010v2)

### TD-010v2 (Qwen2.5-1.5B)

- **–ú–æ–¥–µ–ª—å:** Qwen2.5-1.5B-Instruct
- **eval_loss:** 0.9358 ‚úÖ
- **train_loss:** 0.9246
- **–†–∞–∑–º–µ—Ä:** 35.27 MB
- **LoRA –º–æ–¥—É–ª–µ–π:** 7 (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj)
- **lora_rank:** 8
- **Optimizer:** adamw_torch (standard)

### TD-010v3 (Qwen2.5-3B) ‚Äî Research-Based

- **–ú–æ–¥–µ–ª—å:** Qwen2.5-3B-Instruct (–≤ 2x –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
- **eval_loss:** 1.0026 ‚ùå (+0.0668)
- **train_loss:** 0.9826
- **–†–∞–∑–º–µ—Ä:** 7.05 MB (-80%)
- **LoRA –º–æ–¥—É–ª–µ–π:** 2 (—Ç–æ–ª—å–∫–æ q_proj, v_proj)
- **lora_rank:** 8
- **Optimizer:** adamw_8bit (—ç–∫–æ–Ω–æ–º–∏—è VRAM)

### –í–µ—Ä–¥–∏–∫—Ç:

**–ü–∞—Ä–∞–¥–æ–∫—Å:** –ú–æ–¥–µ–ª—å 3B (–±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –ø–æ–∫–∞–∑–∞–ª–∞ **—Ö—É–¥—à–∏–π** eval_loss, —á–µ–º 1.5B!

**–ü—Ä–∏—á–∏–Ω—ã:**

1. **–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è:**
   - –ú—ã –æ–±—É—á–∏–ª–∏ —Ç–æ–ª—å–∫–æ **2 –º–æ–¥—É–ª—è** –∏–∑ –≤–æ–∑–º–æ–∂–Ω—ã—Ö 7+
   - –î–ª—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏

2. **–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–¥ VRAM:**
   - –§–æ–∫—É—Å –Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (–∏–∑–±–µ–∂–∞—Ç—å OOM) ‚Üí –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
   - –ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ‚Üí –º–µ–Ω—å—à–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–µ –¢–†–ò–ó

3. **–ö–∞—á–µ—Å—Ç–≤–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞:**
   - adamw_8bit –º–µ–Ω–µ–µ —Ç–æ—á–µ–Ω, —á–µ–º adamw_torch_fused
   - –ú—ã –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é —Ä–∞–¥–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏

---

## üìà –≠–≤–æ–ª—é—Ü–∏—è –õ–æ–∫–∞–ª—å–Ω–æ–≥–æ –ê–≥–µ–Ω—Ç–∞: –í—ã–≤–æ–¥—ã

### –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –ú–æ–¥–µ–ª–∏:

1. **Qwen2-7B** (Nov 26, 2025):
   - –ê–¥–∞–ø—Ç–µ—Ä—ã: 77-154 MB
   - –°—Ç–∞—Ç—É—Å: –£—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω
   - –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ: eval_loss (–Ω–µ—Ç –ª–æ–≥–æ–≤)

2. **Qwen2.5-1.5B (TD-010v2)**:
   - eval_loss: **0.9358** ‚≠ê **–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!**
   - –†–∞–∑–º–µ—Ä: 35 MB (–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π)
   - –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å: –î–æ–∫–∞–∑–∞–Ω–∞

3. **Qwen2.5-3B (TD-010v3)**:
   - eval_loss: 1.0026 (—Ö—É–∂–µ 1.5B)
   - –†–∞–∑–º–µ—Ä: 7 MB (–æ—á–µ–Ω—å –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π)
   - –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å: –î–æ–∫–∞–∑–∞–Ω–∞ (research-based config)

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≠–≤–æ–ª—é—Ü–∏–∏:

#### –í–∞—Ä–∏–∞–Ω—Ç A: –£–ª—É—á—à–∏—Ç—å TD-010v3

**–¶–µ–ª—å:** –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ LoRA –º–æ–¥—É–ª–µ–π –¥–ª—è 3B

**–ò–∑–º–µ–Ω–µ–Ω–∏—è:**
```yaml
lora_target: q_proj,v_proj,k_proj,o_proj  # +2 –º–æ–¥—É–ª—è (4 total)
lora_rank: 8
optim: adamw_8bit  # –û—Å—Ç–∞–≤–∏—Ç—å!
```

**–û–∂–∏–¥–∞–Ω–∏—è:**
- Trainable params: ~3-4M (still safe)
- VRAM peak: ~4-5 GB (still safe)
- eval_loss: –≤–æ–∑–º–æ–∂–Ω–æ <0.95 (–ª—É—á—à–µ 1.5B)

**–†–∏—Å–∫:** –í–æ–∑–º–æ–∂–µ–Ω OOM, –Ω–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è (VRAM –∑–∞–ø–∞—Å –æ–≥—Ä–æ–º–Ω—ã–π)

#### –í–∞—Ä–∏–∞–Ω—Ç B: –ü—Ä–∏–Ω—è—Ç—å TD-010v2 –∫–∞–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π

**–ê—Ä–≥—É–º–µ–Ω—Ç—ã:**
- eval_loss **0.9358** ‚Äî **–ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç** –∏–∑ –≤—Å–µ—Ö
- –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä (35 MB)
- –î–æ–∫–∞–∑–∞–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- –ë—ã—Å—Ç—Ä—ã–π inference –Ω–∞ CPU (1.5B –ª–µ–≥—á–µ 3B)

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- –ú–µ–Ω—å—à–µ –º–æ–¥–µ–ª—å ‚Üí –º–µ–Ω—å—à–µ "—É–º–Ω–æ—Å—Ç—å" (–Ω–æ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç)
- –£–ø—É—â–µ–Ω–Ω–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 3B (–Ω–æ –Ω–µ —Ñ–∞–∫—Ç, —á—Ç–æ –ª—É—á—à–µ)

#### –í–∞—Ä–∏–∞–Ω—Ç C: –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ 7B

**–ê—Ä–≥—É–º–µ–Ω—Ç—ã:**
- **–î–æ–∫–∞–∑–∞–Ω–æ:** 7B —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∞–ª—Å—è –Ω–∞ —ç—Ç–æ–π –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–µ (Nov 26)
- –ê–¥–∞–ø—Ç–µ—Ä—ã 77-154 MB ‚Äî –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–µ, —á–µ–º 35 MB (–±–æ–ª—å—à–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏)
- –ë–æ–ª—å—à–µ –º–æ–¥–µ–ª—å ‚Üí –±–æ–ª—å—à–µ "—É–º–Ω–æ—Å—Ç—å" (–¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –¢–†–ò–ó –∑–∞–¥–∞—á)

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ: eval_loss (–Ω–µ—Ç –ª–æ–≥–æ–≤ –æ—Ç Nov 26)
- Inference –º–µ–¥–ª–µ–Ω–Ω–µ–µ –Ω–∞ CPU (–µ—Å–ª–∏ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è)
- –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ VRAM –¥–ª—è inference (–Ω–æ QLoRA —Å–ø—Ä–∞–≤–∏—Ç—Å—è)

---

## üî¨ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –í–∞–ª–∏–¥–∞—Ü–∏—è: RTX 5060 Ti Technical Report

### –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è:

‚úÖ **–†–∞–∑–¥–µ–ª 4.1:** "7B-8B models - ideal choice for QLoRA on 16 GB VRAM"  
‚Üí **–ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û:** 3B (–º–µ–Ω—å—à–µ 8B!) –≤–ª–µ–∑ —Å –æ–≥—Ä–æ–º–Ω—ã–º –∑–∞–ø–∞—Å–æ–º (3.1 GB / 16 GB)

‚úÖ **Section 3.2:** "8-bit optimizers significantly reduce VRAM for optimizer states"  
‚Üí **–ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û:** adamw_8bit —Å–ø–∞—Å –æ—Ç OOM (vs adamw_torch_fused)

‚úÖ **Table 4 (Section 6.2):** "adamw_8bit + gradient_checkpointing + batch_size 2-4"  
‚Üí **–ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û:** –≠—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±–µ—Å–ø–µ—á–∏–ª–∏ —É—Å–ø–µ—Ö

‚úÖ **Peak VRAM Prediction:** "8-10 GB for 8B models with QLoRA"  
‚Üí **–ü–†–ï–í–ó–û–®–õ–ò:** 3B –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª —Ç–æ–ª—å–∫–æ 3.1 GB (–≤ 3 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ –ø—Ä–æ–≥–Ω–æ–∑–∞!)

### –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è:

‚ùå **eval_loss Prediction:** Technical Report –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–ª, —á—Ç–æ 3B > 1.5B –ø–æ –∫–∞—á–µ—Å—Ç–≤—É  
‚Üí **–†–ï–ê–õ–¨–ù–û–°–¢–¨:** eval_loss 1.0026 (3B) vs 0.9358 (1.5B) ‚Äî **1.5B –ª—É—á—à–µ!**

**–í–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞:** Technical Report –Ω–µ —É—á–∏—Ç—ã–≤–∞–ª –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ–¥ VRAM (—Ç–æ–ª—å–∫–æ 2 LoRA –º–æ–¥—É–ª—è)

---

## üí° –ö–ª—é—á–µ–≤—ã–µ –£—Ä–æ–∫–∏

### 1. **–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –≤–∞–∂–Ω–µ–µ, —á–µ–º —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏**

- adamw_torch_fused (—Ç–æ—á–Ω—ã–π) ‚Üí OOM
- adamw_8bit (—ç–∫–æ–Ω–æ–º–Ω—ã–π) ‚Üí SUCCESS

**–í—ã–≤–æ–¥:** –î–ª—è RTX 5060 Ti 16GB —Å QLoRA **–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ** –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å adamw_8bit

### 2. **–ú–µ–Ω—å—à–µ LoRA –º–æ–¥—É–ª–µ–π ‚â† –ú–µ–Ω—å—à–µ VRAM (–≤—Å–µ–≥–¥–∞)**

- 5 –º–æ–¥—É–ª–µ–π (ultra-lite) ‚Üí OOM
- 2 –º–æ–¥—É–ª—è (research-based) ‚Üí SUCCESS

**–ü—Ä–∏—á–∏–Ω–∞:** –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∑–∞–Ω–∏–º–∞–ª –±–æ–ª—å—à–µ VRAM, —á–µ–º —Å–∞–º–∏ LoRA –º–æ–¥—É–ª–∏!

### 3. **Mini-Test ‚â† Full Training**

- Mini-test (10 samples) —É—Å–ø–µ—à–µ–Ω —Å –ª—é–±—ã–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º
- Full training (300 samples) —Ç—Ä–µ–±—É–µ—Ç adamw_8bit

**–í—ã–≤–æ–¥:** VRAM scaling –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–π, mini-test –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç full success

### 4. **–ë–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ‚â† –õ—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ (–≤—Å–µ–≥–¥–∞)**

- 3B (–±–æ–ª—å—à–µ) ‚Üí eval_loss 1.0026
- 1.5B (–º–µ–Ω—å—à–µ) ‚Üí eval_loss **0.9358** ‚≠ê

**–ü—Ä–∏—á–∏–Ω–∞:** –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ 2 LoRA –º–æ–¥—É–ª—è –¥–ª—è 3B)

### 5. **Technical Report ‚Äî —Ü–µ–Ω–Ω—ã–π —Ä–µ—Å—É—Ä—Å, –Ω–æ –Ω–µ –∞–±—Å–æ–ª—é—Ç**

- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª: VRAM –∑–∞–ø–∞—Å, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
- ‚ùå –ù–µ —É—á—ë–ª: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é (—Ç–æ–ª—å–∫–æ 2 LoRA –º–æ–¥—É–ª—è)

**–í—ã–≤–æ–¥:** –ù–∞—É—á–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç, –Ω–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—É–¥—å—è

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

**–ü—Ä–µ–¥–ª–∞–≥–∞—é:**

1. **–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –í–∞—Ä–∏–∞–Ω—Ç A:** –£–≤–µ–ª–∏—á–∏—Ç—å LoRA –º–æ–¥—É–ª–∏ –¥–æ 4 (q_proj, v_proj, k_proj, o_proj)
   - –†–∏—Å–∫ OOM –Ω–∏–∑–∫–∏–π (VRAM –∑–∞–ø–∞—Å –æ–≥—Ä–æ–º–Ω—ã–π)
   - –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª: eval_loss <0.95 (–ª—É—á—à–µ –æ–±–æ–∏—Ö baseline)

2. **–ï—Å–ª–∏ –í–∞—Ä–∏–∞–Ω—Ç A –ø—Ä–æ–≤–∞–ª–∏—Ç—Å—è:**
   - **–ü—Ä–∏–Ω—è—Ç—å TD-010v2 (1.5B)** –∫–∞–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞/—Ä–∞–∑–º–µ—Ä–∞
   - **–ò–ª–∏ –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ 7B –∞–¥–∞–ø—Ç–µ—Ä–∞–º** (Nov 26) –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞

3. **–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —ç–≤–æ–ª—é—Ü–∏—é:**
   - –°–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
   - –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å eval_loss vs —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏
   - –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å "sweet spot" –¥–ª—è RTX 5060 Ti 16GB

---

## üìÇ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã

**–ü—É—Ç—å –∞–¥–∞–ø—Ç–µ—Ä–∞:**  
`E:\WORLD_OLLAMA\services\llama_factory\saves\Qwen2.5-3B\lora\triz_research_based\`

**–§–∞–π–ª—ã:**
- `adapter_config.json` ‚Äî –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA
- `adapter_model.safetensors` ‚Äî –í–µ—Å–∞ (7.05 MB)
- `checkpoint-51/` ‚Äî –§–∏–Ω–∞–ª—å–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç
- `trainer_log.jsonl` ‚Äî –ü–æ–ª–Ω—ã–π –ª–æ–≥ –º–µ—Ç—Ä–∏–∫

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**  
`E:\WORLD_OLLAMA\services\llama_factory\triz_qwen3b_research_based.yaml`

**–õ–∞—É–Ω—á–µ—Ä:**  
`E:\WORLD_OLLAMA\scripts\train_td010v3_research_based.ps1`

---

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ –î–æ–∫—É–º–µ–Ω—Ç—ã

- [RTX 5060 Ti Technical Report](E:\WORLD_OLLAMA\library\raw_documents\RTX 5060 Ti_ –õ–æ–∫–∞–ª—å–Ω–æ–µ –û–±—É—á–µ–Ω–∏–µ –ò–ò.txt)
- [Model Comparison Nov 2025](E:\WORLD_OLLAMA\docs\model_comparison_nov2025.md)
- [TD-010v2 (1.5B) Training](E:\WORLD_OLLAMA\services\llama_factory\saves\Qwen2.5-1.5B\lora\triz_td010v2\)
- [Qwen2-7B Adapters (Nov 26)](E:\WORLD_OLLAMA\services\llama_factory\saves\Qwen2-7B-Instruct\lora\triz_safe\)

---

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ—à–µ–Ω–∏–µ –ø–æ —ç–≤–æ–ª—é—Ü–∏–∏  
**–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:** –í—ã–±—Ä–∞—Ç—å –º–µ–∂–¥—É –í–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ A/B/C –∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —ç–≤–æ–ª—é—Ü–∏—é –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
