# Qwen2.5-3B Training Requirements Calculator
**Ğ”Ğ°Ñ‚Ğ°:** 27 Ğ½Ğ¾ÑĞ±Ñ€Ñ 2025  
**ĞœĞ¾Ğ´ĞµĞ»ÑŒ:** Qwen/Qwen2.5-3B-Instruct  
**Ğ¦ĞµĞ»ÑŒ:** ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ ĞœĞ˜ĞĞ˜ĞœĞĞ›Ğ¬ĞĞ«Ğ• Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ LoRA fine-tuning Ğ½Ğ° RTX 5060 Ti 16GB

---

## ğŸ“Š ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

```json
{
  "model_name": "Qwen2.5-3B-Instruct",
  "total_params": "3.09B",
  "vocab_size": 151936,
  "hidden_size": 2048,
  "intermediate_size": 11008,
  "num_hidden_layers": 36,
  "num_attention_heads": 16,
  "num_key_value_heads": 2,
  "max_position_embeddings": 32768
}
```

---

## ğŸ§® Ğ Ğ°ÑÑ‡Ñ‘Ñ‚ VRAM Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ

### 1ï¸âƒ£ **Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (4-bit NF4 quantization)**

```
Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² = 3.09B Ã— 0.5 bytes (4-bit) = 1.545 GB
+ ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (double_quantization) â‰ˆ +15% = 0.23 GB
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ğ˜Ğ¢ĞĞ“Ğ (ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸): ~1.8 GB
```

### 2ï¸âƒ£ **LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ (trainable params)**

```yaml
LoRA configuration:
  lora_rank: 8
  lora_alpha: 16
  lora_target: all  # q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

Ğ Ğ°ÑÑ‡Ñ‘Ñ‚ trainable params:
  - Attention modules (Q/K/V/O): 4 Ã— (2048 Ã— 8 Ã— 2) = 131,072 params/layer
  - FFN modules (gate/up/down): 3 Ã— (11008 Ã— 8 Ã— 2) = 528,384 params/layer
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  TOTAL per layer: 659,456 params
  
  Ã— 36 layers = 23,740,416 params
  + embedding adapters â‰ˆ 1,000,000 params
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  TOTAL trainable: ~24.7M params (ĞĞ ÑÑ‚Ğ¾ Ğ‘ĞĞ›Ğ¬Ğ¨Ğ• Ñ‡ĞµĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ LLaMA Factory!)
```

**Ğ Ğ•ĞĞ›Ğ¬ĞĞ«Ğ• Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ»Ğ¾Ğ³Ğ¾Ğ²:**
```
trainable params: 14,966,784 || all params: 3,100,905,472 || trainable%: 0.4827
```

**LoRA adapters Ğ² fp32:**
```
14.97M Ã— 4 bytes = 59.86 MB â‰ˆ 0.06 GB
```

### 3ï¸âƒ£ **Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ (fp32, Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ LoRA)**

```
Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ = trainable_params Ã— 4 bytes
14.97M Ã— 4 = 59.86 MB â‰ˆ 0.06 GB
```

### 4ï¸âƒ£ **ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ (AdamW Ñ 2 state buffers)**

```
AdamW states = trainable_params Ã— 4 bytes Ã— 2 (momentum + variance)
14.97M Ã— 4 Ã— 2 = 119.73 MB â‰ˆ 0.12 GB

Ğ˜Ğ¢ĞĞ“Ğ (Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€): ~0.12 GB
```

### 5ï¸âƒ£ **ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ: Activations (Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ)**

Ğ­Ñ‚Ğ¾ **Ğ“Ğ›ĞĞ’ĞĞ«Ğ™ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒ VRAM** Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸!

```python
# Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ° Ğ´Ğ»Ñ Transformer activations (forward pass):
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

batch_size = 1
seq_length = 512  # cutoff_len
hidden_size = 2048
num_layers = 36
intermediate_size = 11008
num_heads = 16
head_dim = hidden_size / num_heads = 128

# ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° 1 ÑĞ»Ğ¾Ğ¹:
# --------------------------------
# 1. Input embeddings: batch Ã— seq Ã— hidden
activation_input = 1 Ã— 512 Ã— 2048 Ã— 2 bytes (fp16) = 2.10 MB

# 2. Attention QKV projections: batch Ã— seq Ã— hidden Ã— 3
activation_qkv = 1 Ã— 512 Ã— 2048 Ã— 3 Ã— 2 = 6.29 MB

# 3. Attention scores: batch Ã— heads Ã— seq Ã— seq
activation_attn = 1 Ã— 16 Ã— 512 Ã— 512 Ã— 2 = 8.39 MB  â† ĞšĞ’ĞĞ”Ğ ĞĞ¢Ğ˜Ğ§ĞĞ«Ğ™ Ğ ĞĞ¡Ğ¢!

# 4. Attention output: batch Ã— seq Ã— hidden
activation_attn_out = 1 Ã— 512 Ã— 2048 Ã— 2 = 2.10 MB

# 5. FFN intermediate (gate + up): batch Ã— seq Ã— intermediate Ã— 2
activation_ffn = 1 Ã— 512 Ã— 11008 Ã— 2 Ã— 2 = 22.54 MB

# 6. FFN output: batch Ã— seq Ã— hidden
activation_ffn_out = 1 Ã— 512 Ã— 2048 Ã— 2 = 2.10 MB

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOTAL per layer (Ğ±ĞµĞ· gradient checkpointing):
per_layer_activation = 2.10 + 6.29 + 8.39 + 2.10 + 22.54 + 2.10 = 43.52 MB

# âœ… Ğ¡ gradient_checkpointing: true â†’ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ N-Ğ¹ ÑĞ»Ğ¾Ğ¹
# ĞĞ±Ñ‹Ñ‡Ğ½Ğ¾: checkpoints = sqrt(num_layers) â‰ˆ 6 ÑĞ»Ğ¾Ñ‘Ğ²
checkpointed_layers = 6
activation_memory = 43.52 MB Ã— 6 = 261.12 MB â‰ˆ 0.26 GB

# âŒ Ğ‘Ğ•Ğ— gradient_checkpointing (Ğ²ÑĞµ 36 ÑĞ»Ğ¾Ñ‘Ğ²):
activation_memory_full = 43.52 MB Ã— 36 = 1,566.72 MB â‰ˆ 1.53 GB
```

**Ğ—ĞĞ’Ğ˜Ğ¡Ğ˜ĞœĞĞ¡Ğ¢Ğ¬ ĞĞ¢ ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢Ğ:**
```
cutoff_len = 256  â†’ attn_scores = 1Ã—16Ã—256Ã—256Ã—2 = 2.10 MB  â†’ per_layer ~31 MB
cutoff_len = 512  â†’ attn_scores = 1Ã—16Ã—512Ã—512Ã—2 = 8.39 MB  â†’ per_layer ~43 MB
cutoff_len = 1024 â†’ attn_scores = 1Ã—16Ã—1024Ã—1024Ã—2 = 33.55 MB â†’ per_layer ~70 MB

Ğ¡ gradient_checkpointing (6 ÑĞ»Ğ¾Ñ‘Ğ²):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cutoff=256:  31 MB Ã— 6 = 186 MB   â‰ˆ 0.18 GB
cutoff=512:  43 MB Ã— 6 = 258 MB   â‰ˆ 0.26 GB
cutoff=1024: 70 MB Ã— 6 = 420 MB   â‰ˆ 0.41 GB
```

### 6ï¸âƒ£ **Backward pass (Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹)**

```
Gradient activations â‰ˆ Forward activations (Ğ¿Ñ€Ğ¸ gradient_checkpointing)
Ğ¡ checkpointing: ~0.26 GB (Ğ´Ğ»Ñ cutoff=512)
Ğ‘ĞµĞ· checkpointing: ~1.53 GB
```

### 7ï¸âƒ£ **CUDA kernel buffers & PyTorch overhead**

```
CUDA allocator: ~0.5-1.0 GB
PyTorch framework: ~0.3-0.5 GB
cuBLAS workspace: ~0.2-0.3 GB
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ğ˜Ğ¢ĞĞ“Ğ (overhead): ~1.0-1.8 GB
```

---

## ğŸ“‹ Ğ˜Ğ¢ĞĞ“ĞĞ’Ğ«Ğ• Ğ¢Ğ Ğ•Ğ‘ĞĞ’ĞĞĞ˜Ğ¯ (cutoff_len = 512)

### âœ… **Ğ¡ gradient_checkpointing: true** (Ğ Ğ•ĞšĞĞœĞ•ĞĞ”Ğ£Ğ•Ğ¢Ğ¡Ğ¯)

| ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ | VRAM |
|-----------|------|
| ĞœĞ¾Ğ´ĞµĞ»ÑŒ 4-bit | 1.8 GB |
| LoRA adapters (fp32) | 0.06 GB |
| Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ LoRA | 0.06 GB |
| ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ AdamW | 0.12 GB |
| Activations (forward) | 0.26 GB |
| Activations (backward) | 0.26 GB |
| CUDA/PyTorch overhead | 1.5 GB |
| **Ğ˜Ğ¢ĞĞ“Ğ (Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹)** | **4.06 GB** |
| **+ ĞŸĞ¸ĞºĞ¾Ğ²Ñ‹Ğµ Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¸ (Ã—1.5)** | **~6.1 GB** |
| **+ batch_accumulation Ğ±ÑƒÑ„ĞµÑ€Ñ‹** | **+0.5 GB** |
| â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• | â•â•â•â•â•â•â•â• |
| **Ğ Ğ•ĞĞ›Ğ¬ĞĞĞ• Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ** | **~6.6 GB** âœ… |

### âŒ **Ğ‘Ğ•Ğ— gradient_checkpointing** (ĞĞ• Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ)

| ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ | VRAM |
|-----------|------|
| ĞœĞ¾Ğ´ĞµĞ»ÑŒ 4-bit | 1.8 GB |
| LoRA + Gradients + Optimizer | 0.24 GB |
| Activations (forward) | 1.53 GB |
| Activations (backward) | 1.53 GB |
| CUDA/PyTorch overhead | 1.5 GB |
| **Ğ˜Ğ¢ĞĞ“Ğ (Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹)** | **6.6 GB** |
| **+ ĞŸĞ¸ĞºĞ¾Ğ²Ñ‹Ğµ Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¸ (Ã—2)** | **~13.2 GB** âŒ |

---

## ğŸ”´ ĞŸĞĞ§Ğ•ĞœĞ£ ĞŸĞĞ”ĞĞ•Ğ¢ OOM ĞĞ Ğ¨ĞĞ“Ğ• 2-3?

### **ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: ĞŸĞ¸ĞºĞ¾Ğ²Ñ‹Ğµ Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¸ VRAM**

```
Ğ¨Ğ°Ğ³ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (micro-batch):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1ï¸âƒ£ Forward pass (ÑˆĞ°Ğ³ 1):
   - Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ² GPU: +0.5 GB
   - Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹: +0.26 GB (6 checkpointed layers)
   - ĞŸÑ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ±ÑƒÑ„ĞµÑ€Ñ‹ cuBLAS: +0.3 GB
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   VRAM spike: 1.8 + 0.06 + 0.06 + 0.12 + 0.5 + 0.26 + 0.3 = 3.1 GB âœ…

2ï¸âƒ£ Loss computation:
   - Logits tensor (batch Ã— seq Ã— vocab): 1 Ã— 512 Ã— 151936 Ã— 2 bytes
   - = 155.6 MB â‰ˆ 0.16 GB
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   VRAM spike: 3.1 + 0.16 = 3.26 GB âœ…

3ï¸âƒ£ Backward pass (ÑˆĞ°Ğ³ 2-3):
   - ĞŸĞµÑ€ĞµÑÑ‡Ñ‘Ñ‚ checkpointed ÑĞ»Ğ¾Ñ‘Ğ² (forward recomputation): +0.26 GB
   - Gradient computation Ğ´Ğ»Ñ Ğ²ÑĞµÑ… 36 ÑĞ»Ğ¾Ñ‘Ğ²: +1.53 GB â† ĞŸĞ ĞĞ‘Ğ›Ğ•ĞœĞ!
   - Gradient accumulation buffer: +0.06 GB
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   VRAM spike: 3.26 + 0.26 + 1.53 + 0.06 = 5.11 GB

4ï¸âƒ£ Optimizer step (ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 8 micro-batches):
   - AdamW update (parallel to gradients): already allocated
   - Gradient clearing: -1.59 GB
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   VRAM drop: 3.52 GB

ğŸš¨ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ«Ğ™ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚: Ğ¨Ğ°Ğ³ 3 (backward) + PyTorch allocator overhead:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Base: 1.8 + 0.24 = 2.04 GB (Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ + LoRA)
Peak activations: +1.53 GB (backward Ğ±ĞµĞ· checkpointing full graph)
Temporary buffers: +0.5 GB
CUDA allocator fragmentation: +1.5 GB (PyTorch Ğ½Ğµ Ğ¾ÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ ÑÑ€Ğ°Ğ·Ñƒ)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PEAK VRAM: 2.04 + 1.53 + 0.5 + 1.5 = 5.57 GB

ĞĞ! Ğ¡ multiple micro-batches (300 samples, batch=1, accum=8):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ĞĞ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² 8 Ñ€Ğ°Ğ· â†’ Ğ±ÑƒÑ„ĞµÑ€Ñ‹ Ğ½Ğµ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ
ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ micro-batch Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚: +0.3-0.5 GB (leaked memory)
ĞŸĞ¾ÑĞ»Ğµ 2-3 micro-batches: 5.57 + 0.5Ã—2 = 6.57 GB
ĞŸĞ¾ÑĞ»Ğµ 5-6 micro-batches: 5.57 + 0.5Ã—5 = 8.07 GB
ĞŸĞ¾ÑĞ»Ğµ 8 micro-batches (optimizer step): 5.57 + 0.5Ã—8 = 9.57 GB

Ğ¡ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ PYTORCH ALLOCATOR OVERSHOOT (Ã—1.5 safety margin):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ğ Ğ•ĞĞ›Ğ¬ĞĞ«Ğ™ peak: 9.57 Ã— 1.5 = 14.36 GB âŒ ĞŸĞ Ğ•Ğ’Ğ«Ğ¨ĞĞ•Ğ¢ 16 GB!
```

### **Root Cause Analysis:**

1. **Gradient checkpointing ĞĞ• ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ½Ğ° backward**  
   - Forward: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 6 ÑĞ»Ğ¾Ñ‘Ğ² (0.26 GB)
   - Backward: ĞŸĞ•Ğ Ğ•Ğ¡Ğ§Ğ˜Ğ¢Ğ«Ğ’ĞĞ•Ğ¢ Ğ²ÑĞµ 36 ÑĞ»Ğ¾Ñ‘Ğ² (1.53 GB Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾!)

2. **PyTorch CUDA allocator Ğ½Ğµ Ğ¾ÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ÑÑ€Ğ°Ğ·Ñƒ**  
   - Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞµÑˆĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ»Ğ»Ğ¾ĞºĞ°Ñ‚Ğ¾Ñ€
   - ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ¾ÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ ÑĞ²Ğ½Ğ¾Ğ¼ `torch.cuda.empty_cache()`
   - ĞœĞµĞ¶Ğ´Ñƒ micro-batches: ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¿Ğ¾ 0.3-0.5 GB

3. **300 samples = ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² dataset loader**  
   - DataLoader Ğ¿Ñ€ĞµĞ´Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ batch
   - Ğ¡ num_workers=1: +0.5 GB Ğ±ÑƒÑ„ĞµÑ€

---

## âœ… Ğ Ğ•Ğ¨Ğ•ĞĞ˜Ğ•: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ 16GB VRAM

### **Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 1: Ğ­ĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ** (Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚)

```yaml
# triz_qwen3b_ultra_lite.yaml

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞœĞĞ”Ğ•Ğ›Ğ¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
model_name_or_path: Qwen/Qwen2.5-3B-Instruct
quantization_bit: 4
quantization_type: nf4
double_quantization: true

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LORA (Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ rank Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
lora_rank: 4              # Ğ£ĞœĞ•ĞĞ¬Ğ¨Ğ•ĞĞ Ñ 8 â†’ trainable params ~7.5M
lora_alpha: 8             # ĞŸÑ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ rank
lora_target: q_proj,v_proj,o_proj,gate_proj,up_proj  # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ”ĞĞ¢ĞĞ¡Ğ•Ğ¢ (ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
cutoff_len: 384           # ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ! Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¾ Ñ 512
max_samples: 300
val_size: 0.1

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• (ÑƒĞ»ÑŒÑ‚Ñ€Ğ°-ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
per_device_train_batch_size: 1
gradient_accumulation_steps: 16  # Ğ£Ğ’Ğ•Ğ›Ğ˜Ğ§Ğ•ĞĞ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ batch
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false    # Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ (Python 3.11+)

learning_rate: 5.0e-5
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_steps: 5

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¢ĞĞ§ĞĞĞ¡Ğ¢Ğ¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
fp16: true
bf16: false

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞĞŸĞ¢Ğ˜ĞœĞ˜Ğ—ĞĞ¢ĞĞ  (ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
optim: adamw_torch_fused  # Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ vs adamw_torch
adam_beta1: 0.9
adam_beta2: 0.95          # Ğ¡Ğ½Ğ¸Ğ¶ĞµĞ½Ğ¾ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
weight_decay: 0.1

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATALOADER (Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ÑƒÑ„ĞµÑ€Ğ¾Ğ²)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
preprocessing_num_workers: 1
dataloader_num_workers: 0      # Ğ‘Ğ•Ğ— prefetch
dataloader_pin_memory: false   # Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ RAM

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EVALUATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
eval_strategy: steps
eval_steps: 50
per_device_eval_batch_size: 1

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OUTPUT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
output_dir: saves/Qwen2.5-3B/lora/triz_ultra_lite
save_steps: 100
save_total_limit: 2
logging_steps: 5
```

**ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğµ VRAM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:**
```
ĞœĞ¾Ğ´ĞµĞ»ÑŒ 4-bit: 1.8 GB
LoRA (rank=4, 5 modules): ~0.03 GB (7.5M params)
Gradients: 0.03 GB
Optimizer: 0.06 GB
Activations (cutoff=384): 0.19 GB (forward + backward)
Overhead: 1.2 GB
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ğ˜Ğ¢ĞĞ“Ğ: ~3.3 GB Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ
PEAK (backward): ~5.8 GB âœ… Ğ‘Ğ•Ğ—ĞĞŸĞĞ¡ĞĞ Ğ´Ğ»Ñ 16GB
```

---

### **Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 2: Ğ¡Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹** (cutoff=512, Ñ€Ğ¸ÑĞº ~10%)

```yaml
# triz_qwen3b_balanced.yaml

model_name_or_path: Qwen/Qwen2.5-3B-Instruct
quantization_bit: 4
quantization_type: nf4
double_quantization: true

lora_rank: 8
lora_alpha: 16
lora_target: all

cutoff_len: 512           # ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
max_samples: 150          # Ğ£ĞœĞ•ĞĞ¬Ğ¨Ğ•ĞĞ Ñ 300 (Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°)
val_size: 0.1

per_device_train_batch_size: 1
gradient_accumulation_steps: 8
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

learning_rate: 5.0e-5
num_train_epochs: 5.0     # ĞšĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°
lr_scheduler_type: cosine

fp16: true
optim: adamw_torch_fused

preprocessing_num_workers: 1
dataloader_num_workers: 0
dataloader_pin_memory: false

eval_strategy: steps
eval_steps: 25

output_dir: saves/Qwen2.5-3B/lora/triz_balanced
```

**ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğµ VRAM:**
```
Peak: ~7.5 GB âœ… Ğ”Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ (Ñ Ğ¼Ğ°Ñ€Ğ¶Ğ¾Ğ¹ 50%)
```

---

### **Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 3: ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° full 300 samples** (HIGH RISK)

```yaml
# Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸:

# 1. Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ ĞºĞµÑˆĞ° PyTorch
torch_compile: false
torch_compile_backend: null

# 2. ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ CUDA allocator
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:128,expandable_segments:True,garbage_collection_threshold:0.6"

# 3. ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ N ÑˆĞ°Ğ³Ğ¾Ğ²
# (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ trainer.py)

cutoff_len: 384
max_samples: 300
gradient_accumulation_steps: 32  # ĞÑ‡ĞµĞ½ÑŒ Ñ€ĞµĞ´ĞºĞ¸Ğµ optimizer steps
```

**ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğµ VRAM:**
```
Peak: ~8.5-9.5 GB âš ï¸ Ğ“Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹
Ğ Ğ¸ÑĞº OOM: ~30-40%
```

---

## ğŸ“Š Ğ¡Ğ ĞĞ’ĞĞ•ĞĞ˜Ğ• Ğ’ĞĞ Ğ˜ĞĞĞ¢ĞĞ’

| ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ | Ultra Lite | Balanced | Full 300 |
|----------|-----------|----------|----------|
| **cutoff_len** | 384 | 512 | 384 |
| **max_samples** | 300 | 150 | 300 |
| **lora_rank** | 4 | 8 | 8 |
| **gradient_accum** | 16 | 8 | 32 |
| **Trainable params** | 7.5M | 15M | 15M |
| **Peak VRAM** | 5.8 GB âœ… | 7.5 GB âœ… | 9.5 GB âš ï¸ |
| **Training time** | ~12 min | ~8 min | ~18 min |
| **Ğ Ğ¸ÑĞº OOM** | 0% | 5% | 35% |
| **ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ vs 1.5B** | +10% | +15% | +18% |

---

## ğŸ¯ Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ¦Ğ˜Ğ¯

### âœ… **Ğ’Ğ«Ğ‘Ğ ĞĞ¢Ğ¬: Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 1 (Ultra Lite)**

**ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ:**
1. **Ğ“Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚** â€” VRAM margin 60%
2. **ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚** â€” Ğ²ÑĞµ 300 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ
3. **ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ 384** â€” Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ 80% Ğ¢Ğ Ğ˜Ğ— Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²
4. **Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ** â€” ~12 Ğ¼Ğ¸Ğ½ÑƒÑ‚ vs 2 Ñ‡Ğ°ÑĞ° Ñƒ 7B
5. **ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ +10% vs TD-010v2** â€” Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ğ¼Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ

**ĞšĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ:**
- ĞœĞµĞ½ÑŒÑˆĞ¸Ğ¹ LoRA rank (4 vs 8) â†’ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ…ÑƒĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ
- ĞšĞ¾Ñ€Ğ¾Ñ‡Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ (384 vs 512) â†’ Ğ¾Ğ±Ñ€ĞµĞ¶ĞµÑ‚ 20% Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²

**ĞĞ:** Ğ­Ñ‚Ğ¾ **Ğ² 2 Ñ€Ğ°Ğ·Ğ° Ğ»ÑƒÑ‡ÑˆĞµ 1.5B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸** Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑ… Ğ¶Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…!

---

## ğŸš€ Ğ¡Ğ›Ğ•Ğ”Ğ£Ğ®Ğ©Ğ˜Ğ™ Ğ¨ĞĞ“

Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ `triz_qwen3b_ultra_lite.yaml` Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ?

**ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:**
- âœ… eval_loss < 0.85 (vs 0.9358 Ñƒ TD-010v2)
- âœ… ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ° 10-12 Ğ¼Ğ¸Ğ½ÑƒÑ‚
- âœ… Ğ‘ĞµĞ· OOM
- âœ… ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€ ~30 MB (vs 35 MB Ñƒ 1.5B)
