#!/usr/bin/env python3
"""
Initial Indexing Script for CORTEX (TD-006)
SESA3002a Protocol: Clean index creation from 131 files

This script builds the LightRAG index from scratch using
documents in E:\\WORLD_OLLAMA\\library\\raw_documents

NO OLD CACHE - Pure WORLD_OLLAMA state
"""

import os
import time
import requests
from pathlib import Path
import sys

# === CONFIGURATION ===
SERVER_URL = "http://localhost:8004"
LIBRARY_DIR = Path(r"E:\WORLD_OLLAMA\library\raw_documents")
CHUNK_SIZE_CHARS = 700  # Safe for qwen2.5:14b (4096 token limit)
DELAY_BETWEEN_CHUNKS = 20  # Seconds (GPU cooling)

def wait_for_server(max_attempts=30):
    """Wait for LightRAG server to be ready"""
    print("Waiting for CORTEX server...")
    for i in range(max_attempts):
        try:
            resp = requests.get(f"{SERVER_URL}/health", timeout=2)
            if resp.status_code == 200:
                print("‚úÖ CORTEX online!")
                return True
        except:
            time.sleep(2)
            if i % 5 == 0: 
                print(".", end="", flush=True)
    print("\n‚ùå Server timeout")
    return False

def ingest_file(file_path, chunk_index=0):
    """Ingest single file with chunking support"""
    print(f"\nüìÑ Processing: {file_path.name}")
    
    try:
        # Read file content
        text = file_path.read_text(encoding="utf-8", errors="ignore")
        if not text.strip():
            print("   ‚ö†Ô∏è Empty file, skipping")
            return chunk_index
            
        total_chars = len(text)
        
        # Small file: send whole
        if total_chars < CHUNK_SIZE_CHARS:
            print(f"   üì¶ Sending whole file ({total_chars} chars)...")
            payload = {
                "text": text, 
                "metadata": {"source": file_path.name}
            }
            
            try:
                response = requests.post(
                    f"{SERVER_URL}/insert", 
                    json=payload, 
                    timeout=300
                )
                if response.status_code == 200:
                    chunk_index += 1
                    print(f"   ‚úÖ OK (chunk #{chunk_index})")
                else:
                    print(f"   ‚ùå Error: {response.status_code}")
            except Exception as e:
                print(f"   ‚ùå Network error: {e}")
            
            return chunk_index

        # Large file: chunk it
        print(f"   üìä Large file ({total_chars} chars). Chunking by {CHUNK_SIZE_CHARS}...")
        chunks = [
            text[i:i+CHUNK_SIZE_CHARS] 
            for i in range(0, total_chars, CHUNK_SIZE_CHARS)
        ]
        total_chunks = len(chunks)
        
        for i, chunk in enumerate(chunks):
            chunk_part = i + 1
            print(f"   üîπ Chunk {chunk_part}/{total_chunks} ({len(chunk)} chars)... ", end="", flush=True)
            
            payload = {
                "text": chunk, 
                "metadata": {"source": f"{file_path.name}_part_{chunk_part}"}
            }
            
            try:
                response = requests.post(
                    f"{SERVER_URL}/insert", 
                    json=payload, 
                    timeout=180
                )
                if response.status_code == 200:
                    chunk_index += 1
                    print(f"‚úÖ (chunk #{chunk_index})")
                    time.sleep(DELAY_BETWEEN_CHUNKS)  # GPU cooldown
                else:
                    print(f"‚ùå ERROR {response.status_code}")
                    time.sleep(30)
            except requests.Timeout:
                print(f"‚ùå TIMEOUT (180s)")
                time.sleep(30)
            except Exception as e:
                print(f"‚ùå {type(e).__name__}: {e}")
                time.sleep(10)
        
        return chunk_index
        
    except Exception as e:
        print(f"   ‚ùå File error: {e}")
        return chunk_index

def main():
    """Main indexing workflow"""
    print("=" * 60)
    print("CORTEX IGNITION - INITIAL INDEXING (TD-006)")
    print("=" * 60)
    print(f"üìÇ Library: {LIBRARY_DIR}")
    print(f"üåê Server: {SERVER_URL}")
    print(f"üß¨ Chunk size: {CHUNK_SIZE_CHARS} chars")
    print("=" * 60)
    
    # Check server health
    if not wait_for_server():
        print("\n‚ùå ABORT: Server not responding")
        sys.exit(1)
    
    # Get file list
    files = sorted(LIBRARY_DIR.glob("*.txt"))
    total_files = len(files)
    
    if total_files == 0:
        print(f"\n‚ö†Ô∏è No .txt files found in {LIBRARY_DIR}")
        sys.exit(1)
    
    print(f"\nüìö Found {total_files} files")
    print("=" * 60)
    
    # Process files
    start_time = time.time()
    chunk_counter = 0
    
    for idx, file_path in enumerate(files, 1):
        print(f"\n[{idx}/{total_files}] ", end="")
        chunk_counter = ingest_file(file_path, chunk_counter)
    
    # Summary
    elapsed = time.time() - start_time
    print("\n" + "=" * 60)
    print("‚úÖ INDEXING COMPLETE")
    print("=" * 60)
    print(f"üìä Files processed: {total_files}")
    print(f"üß¨ Total chunks: {chunk_counter}")
    print(f"‚è±Ô∏è Time: {elapsed/60:.1f} minutes")
    print("=" * 60)

if __name__ == "__main__":
    main()
