Стратегическая Дорожная Карта для Оптимального Local-First Web UI: Миграция WORLD_OLLAMA на Архитектуру Tauri/Rust для Высокой Производительности




1. Исполнительное Резюме и Обоснование Перехода




1.1. Проблемы Текущей Архитектуры (Chainlit/Python)


Текущий локальный веб-интерфейс Neuro-Terminal, построенный на Python-центричных фреймворках, таких как Chainlit, демонстрирует фундаментальные архитектурные ограничения, которые препятствуют достижению целей проекта WORLD_OLLAMA по созданию высокопроизводительного, автономного десктопного приложения. Основные проблемы сосредоточены в трех ключевых областях.
Во-первых, наблюдается существенный недостаток производительности и отсутствие надежного стриминга токенов. Python-центричные инструменты, включая Streamlit и Gradio, изначально разработаны для быстрого прототипирования ML-интерфейсов, а не для производственных систем с низкой задержкой.1 Streamlit, например, имеет архитектурный недостаток, заключающийся в перезапуске всего Python-скрипта при каждом взаимодействии, что создает неприемлемый оверхед для интерактивного LLM-терминала.1 В то время как Gradio ориентирован исключительно на демонстрации моделей и не подходит для создания полнофункциональных приложений, управляющих сложными процессами (как CORTEX GraphRAG).1
Во-вторых, существует критическая проблема упаковки (Packaging). Создание нативного исполняемого файла (.exe, .dmg) для Python-приложения требует включения всей среды выполнения Python (Python Runtime). Это неизбежно приводит к чрезмерно большим дистрибутивам. Эта проблема напрямую противоречит требованию к минималистичному и легковесному десктопному приложению, которое должно сосуществовать с ресурсоемкими локальными LLM-движками, такими как Ollama.


1.2. Стратегическое Решение: Переход на Нативный Local-First Стек


Для устранения указанных проблем необходимо стратегическое смещение архитектурного фокуса от монолитного Python-стека к архитектуре, основанной на выделенном, легковесном нативном веб-стеке. Этот подход предполагает четкое разделение между высокопроизводительным нативным ядром (Core) и отзывчивым интерфейсом (Frontend), заключенным в минималистичную оболочку.
Переход на выделенный стек, использующий Tauri и Rust, обеспечивает решение трех критических вызовов:
1. Минимальный Размер и Потребление Ресурсов: Гарантируется минимальный размер дистрибутива и потребление оперативной памяти, что является приоритетом для LLM-приложений.
2. Нативный IPC для Стриминга: Использование нативных механизмов межпроцессного взаимодействия (IPC) для передачи данных LLM с минимальной задержкой.
3. Полный Контроль над UX/UI: Предоставляется возможность реализации истинных Local-First паттернов взаимодействия, включая детальную визуальную обратную связь о состоянии локальных сервисов.


1.3. Ключевая Рекомендация: Принятие Tauri/Rust


Анализ архитектур однозначно указывает на Tauri (Rust) в качестве оптимального нативного контейнера, дополненного современным веб-фреймворком (рекомендуется Svelte/SvelteKit). Эта комбинация обеспечивает максимальную производительность при минимальном оверхеде, являясь стратегически верным выбором для долговременного развития проекта.


2. Сравнительный Анализ Архитектур для Local-First Приложений




2.1. Оценка Python-Центричных Web UI


Фреймворки типа Chainlit, Streamlit и Gradio являются неоптимальными для LLM-интерфейсов, требующих высокой степени интерактивности и низких накладных расходов.
В дополнение к проблемам с производительностью, эти инструменты накладывают серьезные ограничения на кастомизацию и UI/UX.1 Для создания высокополированного пользовательского интерфейса, который сможет конкурировать с нативными десктопными приложениями, необходим полный контроль над HTML/CSS/JS, что недостижимо в рамках фиксированных наборов виджетов, предлагаемых, например, Gradio или Streamlit.1


2.2. Анализ Нативных Веб-Оболочек (Tauri vs. Electron)


Сравнение двух основных подходов к упаковке веб-приложений в десктопные контейнеры показывает критическое превосходство Tauri, особенно в контексте ресурсоемких LLM-систем.


Ресурсоэффективность и Размер Дистрибутива


Ключевым преимуществом Tauri является его легковесность, достигнутая за счет использования системного WebView вместо упаковки полного дистрибутива Chromium, как это делает Electron.


Критерий
	Electron (Chromium)
	Tauri (Rust Core + Native WebView)
	Размер Инсталлятора
	$\sim$85+ МБ (Требует Chromium) 3
	$\sim$2.5 МБ (Использует системный WebView) 3
	Потребление Памяти
	Высокое (Из-за Chromium)
	Низкое ($\sim$58% меньше) 4
	Сложность Обратной Инженерии
	Низкая (легко распаковывается NPM-командой) 3
	Высокая (компилируется в сложный бинарный файл) 3
	Tauri создает инсталлятор размером всего $\sim$2.5 МБ, тогда как размер пакета Electron составляет порядка $\sim$85 МБ.3 Кроме того, бенчмарки показывают, что Tauri потребляет до 58% меньше памяти по сравнению с Electron.4
В контексте Local-First LLM-приложений, такое сокращение потребления ресурсов интерфейсом имеет стратегическое значение. Выбор легковесного инструмента, как Tauri, не просто уменьшает размер приложения, но и освобождает критические системные ресурсы (RAM/VRAM) для самого LLM-движка Ollama. Более низкий оверхед UI означает, что больше ресурсов доступно для обработки запросов, загрузки моделей и RAG-индексации, напрямую повышая конечную производительность системы.
Кроме того, переход на Tauri обеспечивает дополнительную защиту интеллектуальной собственности. Tauri компилирует основную логику (Rust Core) в нативный бинарный файл, что значительно усложняет поверхностный анализ и обратную инженерию, в отличие от Electron, который может быть распакован с помощью простой команды NPM, открывая исходный код.3


3. Фаза 1: Принципы UX/UI Дизайна Local-First


Архитектура Local-First требует пересмотра стандартных веб-паттернов UX/UI. Дизайн Neuro-Terminal должен фокусироваться на подтверждении автономности и прозрачном управлении локальными сервисами, а не на индикации сетевого подключения.


3.1. Реализация Идеалов Local-First


В соответствии с идеалами Local-First, приложение должно быть разработано так, чтобы локальная функциональность была приоритетом.6
* Мгновенная Реакция ("No Spinners"): Приложение должно быть мгновенно отзывчивым, поскольку все данные хранятся локально, и синхронизация, если она потребуется в будущем, является фоновым процессом.6
* Персистенция и Источник Истины: Локальная база данных (например, клиентский SQLite) должна рассматриваться как основной источник истины для истории чатов и настроек.
* Подготовка к Коллаборации и Синхронизации (CRDTs): Для обеспечения готовности к будущему функционалу синхронизации между устройствами или совместной работы, необходимо рассмотреть применение Conflict-Free Replicated Datatypes (CRDT).8 Библиотеки, такие как Yjs и связанные с ней инструменты (например, SyncedStore для Svelte/React), обеспечивают автоматическое, безошибочное разрешение конфликтов, что устраняет сложные ручные процедуры, необходимые при работе с обычными базами данных в условиях офлайн-работы.10


3.2. Визуальная Обратная Связь при Работе с Локальными Сервисами


Поскольку приложение зависит от локальной микросервисной архитектуры (Ollama и CORTEX GraphRAG), UX должен сфокусироваться на статусах LAN-сервисов, а не WAN.
* Индикация Доступности Сервисов: Пользователь должен получать четкие, постоянные визуальные индикаторы, отображающие статус локальных эндпоинтов (Ollama:11434, CORTEX:8004). Эти индикаторы должны отличать критические ошибки локальных сервисов (Service Down) от обычного отсутствия интернета.12
   * Необходимые состояния: LLM-движок (Запущен, Ошибка инициализации, Загрузка тяжелой модели), RAG-сервис (Индексирование, Доступен).
* UX-паттерны для RAG-Индексации: Индексация документов для RAG является долгой, асинхронной операцией ввода-вывода.13 Интерфейс должен использовать "Защитный UX" 15, предоставляя четкие прогресс-бары, уведомления и возможность возобновления прерванных операций.16 Отсутствие четкой обратной связи при таких тяжелых операциях приводит к ощущению зависания приложения.
В Local-First Desktop AI приложении пользовательское восприятие качества зависит от микросервисной архитектуры localhost. Отказы Ollama или CORTEX должны быть обработаны и представлены как критические ошибки локального сервиса, а не как общие ошибки сети. Четкая визуализация статуса LLM (например, через иконку в системном трее, управляемую ядром Rust) критически важна для укрепления доверия пользователя к автономности и надежности приложения.17


4. Фаза 2: Инжиниринг Фронтенда и Архитектура Стриминга




4.1. Выбор Frontend Фреймворка


Рекомендация: Принятие Svelte (или SvelteKit) в качестве основного фреймворка для пользовательского интерфейса.
Svelte, в отличие от React, генерирует код с минимальным объемом JavaScript и практически не имеет оверхеда времени выполнения, поскольку компилирует компоненты в нативный DOM-код.18 Этот минимализм идеально соответствует легковесной оболочке Tauri, обеспечивая наилучшую общую производительность. Встроенная реактивность Svelte (stores) значительно упрощает управление асинхронным состоянием, поступающим из Rust Core, устраняя необходимость в сложных, тяжеловесных внешних библиотеках управления состоянием, которые часто требуются в React (например, Redux/MobX).19


4.2. Архитектура Стриминга LLM-Ответов


Обеспечение стриминга токенов с минимальной задержкой — ключевая техническая задача.
* Протокол: Потоковая передача LLM-токенов — это однонаправленный, непрерывный поток данных, который идеально соответствует паттерну Server-Sent Events (SSE).20
* Механизм IPC: Tauri Events. Вместо использования стандартного HTTP-прокси или WebSocket для связи между Frontend и Rust Core, критически важно использовать нативный механизм Tauri — Events (События).21 Tauri Events — это однонаправленный механизм IPC ("огонь и забыть"), который позволяет Rust Core немедленно передавать каждый токен во фронтенд с минимальной задержкой.22 Фронтенд просто подписывается на событие, например, llm-token-stream.
Этот подход является стратегически важным, поскольку задача стриминга LLM — это передача большого количества небольших пакетов данных (токенов) с экстремально низкой задержкой. Использование IPC Events Tauri обеспечивает прямой, неблокирующий канал связи между Rust Core и Webview. Это позволяет обойти узкие места, связанные с HTTP-сериализацией и десериализацией, которые неизбежно присутствуют в Python-центричных фреймворках, обеспечивая необходимую низкую задержку.4


4.3. Управление Глобальным и Локальным Состоянием


Для управления сложным состоянием приложения (история чатов, настройки, состояние RAG и Ollama) можно применять специализированные Local-First библиотеки. Хотя Svelte stores достаточно для управления реактивностью, для персистенции и потенциальной будущей синхронизации могут быть использованы такие инструменты, как Legend-State, который объединяет управление состоянием, персистенцию и асинхронный синхронизационный слой.24


5. Фаза 3: Локальный Бэкенд и Мост (Local Backend / Bridge Architecture)




5.1. Критический Выбор Механизма Связи: FFI vs. HTTP Proxy


Для связи между веб-интерфейсом и локальными LLM/RAG сервисами (Ollama, CORTEX) необходимо выбрать оптимальный мост. Прямое использование Foreign Function Interface (FFI) для связи между Rust и существующей Python-логикой (CORTEX) не рекомендуется. FFI вносит чрезмерную сложность и, как показывает опыт, может приводить к проблемам производительности и блокировке UI при работе с большими наборами данных и интенсивным IO.26
Оптимальная стратегия — гибридный подход, сочетающий стандартизированные HTTP-запросы и высокоскоростной IPC для потоковой передачи.


Механизм
	Назначение
	Преимущества
	Недостатки
	Local HTTP/SSE Proxy
	Запросы/Ответы, Управление Ollama/CORTEX API
	Простота, использование существующих, стабильных API Ollama.27
	Незначительный сетевой оверхед на localhost.
	Tauri Events (Async IPC)
	Асинхронный Стриминг, Статусы
	Неблокирующий, минимальная задержка, идеально для потока токенов.21
	Однонаправленный (не подходит для запросов с ответом).
	FFI (Foreign Function Interface)
	Прямой вызов функций
	Минимальная теоретическая задержка.
	Сложность, риск блокировки UI, зависимость от внешних FFI-библиотек.26
	

5.2. Рекомендация для WORLD_OLLAMA: Гибридный HTTP/IPC Мост


Rust Core должен функционировать как высокопроизводительный Оркестратор Процессов (Process Manager).
1. Управление Процессами: Rust Core запускает Ollama и CORTEX GraphRAG как отдельные локальные HTTP-сервисы и управляет их жизненным циклом.
2. Синхронные Операции: Frontend вызывает Rust Core через Tauri Commands (invoke) 28 для запросов метаданных, переключения моделей или запуска RAG-задач. Rust Core затем выполняет стандартный HTTP-запрос к локальным сервисам.
3. Асинхронный Стриминг: Rust Core принимает поток токенов от Ollama (через HTTP-стриминг) и немедленно перенаправляет его во Frontend с помощью Tauri Events.21
Такое разделение труда стратегически выгодно: Rust (Tauri Core) принимает на себя роль высокопроизводительного оркестратора (управление процессами, асинхронность, IPC-мост), в то время как Python (CORTEX GraphRAG) остается на уровне стабильных, специализированных сервисных API. Это позволяет быстро мигрировать, устраняя проблемы упаковки и стриминга, без необходимости полной переделки существующей бэкенд-логики GraphRAG.


5.3. Обработка Тяжелых RAG-Операций и Асинхронность Rust Core


RAG-индексация и обработка больших файлов являются блокирующими операциями ввода-вывода (IO).14 Если эти задачи выполняются в основном асинхронном потоке Rust Core, это приведет к зависанию пользовательского интерфейса.26
Решение: Все длительные, блокирующие задачи (например, парсинг больших документов, создание эмбеддингов, работа с локальными файлами) должны быть обернуты с использованием функции tauri::async_runtime::spawn_blocking.29 Эта функция позволяет выполнять блокирующую задачу в отдельном пуле потоков, выделенном для IO-операций, гарантируя, что основной асинхронный цикл и UI остаются свободными и отзывчивыми.28 Результаты или статус выполнения этих фоновых операций затем должны передаваться обратно во Frontend через Tauri Events.


6. Фаза 4: Упаковка, Развертывание и Оптимизация (Packaging & Deployment)


Финальная фаза фокусируется на достижении минимального размера дистрибутива и обеспечении безопасности, используя встроенные возможности Tauri/Rust.


6.1. Чек-лист по Оптимизации Размера Дистрибутива


Для достижения минимального размера дистрибутива (близкого к $\sim$2.5 МБ) необходимо настроить профиль сборки Rust. В файл src-tauri/Cargo.toml должны быть добавлены следующие оптимизации:
* lto = true: Включение оптимизаций времени компоновки (Link-Time Optimizations).30
* opt-level = "s": Приоритет оптимизации размера бинарного файла (вместо скорости, для которой используется уровень 3).30
* panic = "abort": Отключение механизма раскрутки паники (panic unwinding) для удаления обработчиков и уменьшения итогового размера.30
* strip = true: Удаление символов отладки из бинарного файла.30
На стороне фронтенда следует включить tree shaking и minification для удаления неиспользуемого JavaScript, а также оптимизировать все медиа-ресурсы, используя современные форматы изображений.31


6.2. Конфигурация Tauri Allowlist для Безопасности


Безопасность в Tauri основана на принципе наименьших привилегий. Конфигурация должна начинаться с жесткого ограничения всех API: "all": false.31 Только необходимые функции должны быть включены вручную.
* Необходимый Allowlist для WORLD_OLLAMA:
   * fs (File System): Для локального управления RAG-документами и кэшем.
   * shell: { "execute": true }: Критически необходим для запуска Ollama и CORTEX как локальных процессов. Должен быть строго ограничен только этими двумя исполняемыми файлами.
   * dialog: Для нативного выбора файлов пользователем.
Усиление безопасности через ограничение API имеет дополнительное преимущество: чем меньше функций API включается в allowlist, тем меньше кода компилируется в итоговый бинарный файл Rust, что напрямую способствует достижению минимального размера дистрибутива.31 Таким образом, повышение безопасности одновременно выступает как мощный инструмент оптимизации размера.


6.3. Процесс Сборки Кросс-Платформенных Исполняемых Файлов


Tauri автоматизирует процесс сборки нативных кросс-платформенных пакетов, включая .exe для Windows, .dmg для macOS и AppImage для Linux. Применение указанных выше оптимизаций гарантирует, что эти дистрибутивы будут минималистичными и высокобезопасными.


7. Анализ Открытых Архитектур (Case Study: Open WebUI)




7.1. Обзор Архитектуры Open WebUI и ее Компонентов


Open WebUI является широко принятым, самохостинговым Open Source решением, разработанным специально для работы с Ollama и OpenAI-совместимыми API.32
Архитектурно Open WebUI демонстрирует сильное разделение между интерфейсом (Frontend) и LLM Runtime (Ollama). UI представляет собой веб-интерфейс, который взаимодействует с Ollama через его стандартный API-эндпоинт.27 Он включает богатый набор функций, таких как встроенный движок для RAG, управление моделями, и поддержку Progressive Web App (PWA).33


7.2. Уроки по Реализации Стриминга и Управления Моделями


Успех Open WebUI подтверждает необходимость архитектурного декоуплинга: UI должен быть быстрым и функционально богатым, не отвечая за тяжеловесное управление LLM-движком.27 Open WebUI требует поддержки WebSocket для потоковой передачи ответов 33, что подтверждает необходимость использования протокола непрерывного потока данных (SSE/WebSocket/Tauri Events) для LLM-стриминга.
Основной урок, который можно извлечь из Open WebUI, — это его функциональный набор. Open WebUI служит отличным функциональным прототипом, показывая, какие функции (RAG, управление моделями, многопользовательский режим) критически важны для LLM-интерфейса.


7.3. Применимость PWA-подхода Open WebUI


Open WebUI предлагает Progressive Web App (PWA) для мобильного и локального использования.33 Однако PWA-подход, даже в самом лучшем исполнении, не может заменить нативную оболочку Tauri. PWA не решает ключевые проблемы WORLD_OLLAMA:
1. Отсутствие прямого, нативного доступа к системе для запуска Ollama и управления процессами.
2. Необходимость настройки внешних компонентов (Docker, reverse proxies) для установки и обеспечения безопасного контекста (HTTPS).34
PWA демонстрирует максимум возможностей, которые можно получить от веб-стека. Однако его ограничения, особенно в части упаковки и нативного управления ресурсами, являются прямым аргументом в пользу миграции на Tauri. Tauri берет функциональную мощь PWA и трансформирует ее в истинный, минималистичный, интегрированный десктопный продукт, полностью решая проблемы ресурсов и упаковки.


8. Заключение и Рекомендуемые Следующие Шаги


Архитектура, основанная на Tauri (Rust Core) и Svelte Frontend, представляет собой оптимальную и единственно жизнеспособную стратегию для достижения требований к высокой производительности, минимальному размеру дистрибутива и нативной упаковке проекта WORLD_OLLAMA. Применение гибридного HTTP/IPC моста с использованием Tauri Events для стриминга устранит фундаментальные проблемы задержки, присущие текущему Python-центричному стеку.


Рекомендуемая Последовательность Работ (Дорожная Карта)


Четырехфазная дорожная карта предназначена для минимизации риска и обеспечения быстрой реализации критических архитектурных улучшений.
Фаза
	Этап и Фокус
	Ключевые Технические Задачи
	Целевой Результат
	Фаза 1
	UX/UI Design (1-2 недели)
	Разработка спецификаций Local-First UX (No Spinners, Индикаторы локального сервиса, UX для RAG-индексации).
	Полный дизайн-спецификация, готовая к разработке.
	Фаза 2
	Core & Bridge Prototype (4-6 недель)
	Настройка Tauri/Svelte/Rust. Разработка Rust Core как Менеджера Процессов (запуск Ollama/CORTEX). Реализация Tauri Event Bridge для LLM-стриминга.
	Функциональный Core, способный стримить ответы Ollama с низкой задержкой.
	Фаза 3
	Feature Parity & Testing
	Миграция существующей логики CORTEX GraphRAG. Внедрение spawn_blocking для всех тяжелых IO-операций RAG.
	Достижение функционального паритета с Neuro-Terminal в новой легковесной архитектуре.
	Фаза 4
	Optimization & Security
	Настройка Cargo.toml с флагами оптимизации (opt-level="s", strip=true). Реализация строгого Tauri Allowlist ("all": false) для обеспечения безопасности и минимализма.
	Кросс-платформенный, минималистичный, безопасный исполняемый файл.
	Источники
1. Streamlit, Gradio, NiceGUI, and Mesop: Building Data Apps Without Web Devs - Medium, дата последнего обращения: ноября 26, 2025, https://medium.com/@manikolbe/streamlit-gradio-nicegui-and-mesop-building-data-apps-without-web-devs-4474106778f5
2. Streamlit vs Gradio: Which One Should You Choose for Your AI App UI? | by Saran Raj k, дата последнего обращения: ноября 26, 2025, https://python.plainenglish.io/streamlit-vs-gradio-which-one-should-you-choose-for-your-ai-app-ui-da95ce228767
3. Tauri VS. Electron - Real world application, дата последнего обращения: ноября 26, 2025, https://www.levminer.com/blog/tauri-vs-electron
4. Tauri vs. Electron Benchmark: ~58% Less Memory, ~96% Smaller Bundle – Our Findings and Why We Chose Tauri : r/programming - Reddit, дата последнего обращения: ноября 26, 2025, https://www.reddit.com/r/programming/comments/1jwjw7b/tauri_vs_electron_benchmark_58_less_memory_96/
5. A benchmark of Tauri vs Electron for desktop apps : r/javascript - Reddit, дата последнего обращения: ноября 26, 2025, https://www.reddit.com/r/javascript/comments/1njbafr/a_benchmark_of_tauri_vs_electron_for_desktop_apps/
6. Local-first software: You own your data, in spite of the cloud - Ink & Switch, дата последнего обращения: ноября 26, 2025, https://www.inkandswitch.com/essay/local-first/
7. Mastering Local-First Apps: The Ultimate Guide to Offline-First Development with Seamless Cloud Sync | by M Mahdi Ramadhan, M. Si | Medium, дата последнего обращения: ноября 26, 2025, https://medium.com/@Mahdi_ramadhan/mastering-local-first-apps-the-ultimate-guide-to-offline-first-development-with-seamless-cloud-be656167f43f
8. Build a local first sync engine with AI - Ersin's Blog, дата последнего обращения: ноября 26, 2025, https://www.ersin.nz/articles/rust-crdt-sync-engine
9. @localfirst/crdx - npm, дата последнего обращения: ноября 26, 2025, https://www.npmjs.com/package/%40localfirst%2Fcrdx
10. yjs/yjs: Shared data types for building collaborative software - GitHub, дата последнего обращения: ноября 26, 2025, https://github.com/yjs/yjs
11. SyncedStore CRDT is an easy-to-use library for building live, collaborative applications that sync automatically. - GitHub, дата последнего обращения: ноября 26, 2025, https://github.com/YousefED/SyncedStore
12. Offline states - Material Design, дата последнего обращения: ноября 26, 2025, https://m2.material.io/design/communication/offline-states.html
13. Retrieval Augmented Generation (RAG) in Azure AI Search - Microsoft Learn, дата последнего обращения: ноября 26, 2025, https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview
14. Building a Simple RAG System Application with Rust - Mastering Backend, дата последнего обращения: ноября 26, 2025, https://masteringbackend.com/posts/building-a-simple-rag-system-application-with-rust
15. Patterns for Building LLM-based Systems & Products - Eugene Yan, дата последнего обращения: ноября 26, 2025, https://eugeneyan.com/writing/llm-patterns/
16. Offline-First Architecture: Designing for Reality, Not Just the Cloud | by Jusuf Topic | Medium, дата последнего обращения: ноября 26, 2025, https://medium.com/@jusuftopic/offline-first-architecture-designing-for-reality-not-just-the-cloud-e5fd18e50a79
17. Designing Offline-First Mobile Apps for Low-Connectivity Markets - Niotechone, дата последнего обращения: ноября 26, 2025, https://niotechone.com/blog/designing-offline-first-mobile-apps-for-low-connectivity-markets/
18. Svelte vs. React: a simple comparison of JavaScript-based tools - SoftTeco, дата последнего обращения: ноября 26, 2025, https://softteco.com/blog/svelte-vs-react
19. Svelte vs React: Which Tool Should You Choose in 2025? - Distant Job, дата последнего обращения: ноября 26, 2025, https://distantjob.com/blog/svelte-vs-react/
20. Realtime data streaming using server-sent events(SSE) with react.js and node.js, дата последнего обращения: ноября 26, 2025, https://dev.to/torver213/realtime-data-streaming-using-server-sent-eventssse-with-reactjs-and-nodejs-2aak
21. Events | Tauri v1, дата последнего обращения: ноября 26, 2025, https://tauri.app/v1/guides/features/events/
22. Inter-Process Communication | Tauri v1, дата последнего обращения: ноября 26, 2025, https://tauri.app/v1/references/architecture/inter-process-communication/
23. Inter-Process Communication - Tauri, дата последнего обращения: ноября 26, 2025, https://v2.tauri.app/concept/inter-process-communication/
24. Local-first architecture with Expo, дата последнего обращения: ноября 26, 2025, https://docs.expo.dev/guides/local-first/
25. Local-First Landscape - localfirst.fm, дата последнего обращения: ноября 26, 2025, https://www.localfirst.fm/landscape
26. Tauri + Rust = Speed, But Here's Where It Breaks Under Pressure | by Srishti Lal | Medium, дата последнего обращения: ноября 26, 2025, https://medium.com/@srish5945/tauri-rust-speed-but-heres-where-it-breaks-under-pressure-fef3e8e2dcb3
27. Insider's Guide To Ollama And OpenWebUI In 2025! - HyScaler, дата последнего обращения: ноября 26, 2025, https://hyscaler.com/insights/ollama-and-openwebui/
28. Calling Rust from the frontend | Tauri v1, дата последнего обращения: ноября 26, 2025, https://tauri.app/v1/guides/features/command/
29. tauri::async_runtime - Rust - Docs.rs, дата последнего обращения: ноября 26, 2025, https://docs.rs/tauri/latest/tauri/async_runtime/index.html
30. App Size - Tauri, дата последнего обращения: ноября 26, 2025, https://v2.tauri.app/concept/size/
31. Reducing App Size | Tauri v1, дата последнего обращения: ноября 26, 2025, https://tauri.app/v1/guides/building/app-size/
32. Open WebUI: Home, дата последнего обращения: ноября 26, 2025, https://docs.openwebui.com/
33. open-webui/open-webui: User-friendly AI Interface ... - GitHub, дата последнего обращения: ноября 26, 2025, https://github.com/open-webui/open-webui
34. Features | Open WebUI, дата последнего обращения: ноября 26, 2025, https://docs.openwebui.com/features/