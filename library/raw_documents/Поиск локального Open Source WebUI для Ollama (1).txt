Архитектурный анализ экосистемы локальных и полностью открытых "All-in-One" интерфейсов для OLLAMA: Чат, Управление и Дообучение




Глава 1. Введение: Парадигма суверенного искусственного интеллекта


В современной индустрии искусственного интеллекта наблюдается фундаментальный сдвиг от централизованных облачных API к децентрализованным локальным вычислениям. Этот переход обусловлен растущими требованиями к конфиденциальности данных, необходимостью автономной работы в изолированных контурах (air-gapped environments) и желанием избежать вендорной блокировки. Запрос пользователя формулирует потребность в "Святом Граале" локального ИИ: едином, полностью открытом (Open Source) веб-интерфейсе, который объединяет три критически важные функции — инференс (чат), управление моделями и их дообучение (Fine-tuning/LoRA) — в рамках одной автономной системы, работающей поверх бэкенда OLLAMA.
Данный отчет представляет собой исчерпывающее техническое исследование ландшафта существующих решений. Анализ показывает, что, хотя создание монолитного бинарного файла, удовлетворяющего всем требованиям одновременно, является технически сложной задачей из-за противоречивых требований к аппаратному обеспечению для инференса и обучения, индустрия пришла к архитектурному консенсусу. Решением является оркестрация контейнеризированных сервисов, где Open WebUI (для чата и управления) и LLaMA-Factory (для обучения), объединенные в экосистему с Ollama и Unsloth, формируют функционально полный "All-in-One" стек. Также рассматриваются новые интегрированные решения, такие как Kolo, которые стремятся автоматизировать развертывание этого стека "из коробки".


1.1 Декомпозиция требований и технических ограничений


Для корректной оценки решений необходимо декомпозировать запрос на технические составляющие:
1. Полная открытость (Open Source): Исключение любых проприетарных компонентов. Это критично не только для лицензионной чистоты, но и для безопасности, позволяя проводить аудит кода и пересборку бинарных файлов из исходников для конкретных архитектур процессоров (например, AVX-512) или GPU.1
2. Строгая локальность (Offline): Система должна функционировать без единого запроса к внешним серверам (HuggingFace, OpenAI, GitHub) после начальной установки. Это накладывает жесткие требования на управление зависимостями и весами моделей.3
3. Функциональная триада (Чат + Управление + Обучение):
   * Чат: Требует низколатентного интерфейса, поддержки контекста, истории и RAG (Retrieval Augmented Generation).
   * Управление: Требует API для загрузки (sideloading), удаления и модификации метаданных моделей (Modelfiles).
   * Дообучение: Требует доступа к низкоуровневым библиотекам PyTorch, управления VRAM, поддержки адаптеров LoRA и механизмов квантования.5


1.2 Архитектурный разрыв: Почему нет "одной кнопки"?


Существует фундаментальная причина, по которой рынок не наводнен монолитными приложениями "все-в-одном". Инференс (запуск модели) и обучение (изменение весов) требуют принципиально разных программных стеков.
* Стек инференса (Ollama/llama.cpp): Написан преимущественно на C++/Go для максимальной производительности, использует целочисленное квантование (4-bit, 8-bit) и оптимизирован для быстрого ответа.6
* Стек обучения (Unsloth/PyTorch): Написан на Python/CUDA, требует вычислений с плавающей точкой высокой точности (FP16/BF16), массивных библиотек зависимостей и сложного графа вычислений.8
Объединение этих стеков в один процесс приводит к созданию "раздутого" программного обеспечения (bloatware). Поэтому современные "All-in-One" решения реализуются через микросервисную архитектуру, часто скрытую от пользователя за удобным инсталлятором или Docker-контейнером.
________________


Глава 2. Инференс и Управление: Экосистема Open WebUI


Наиболее зрелым и полнофункциональным компонентом для закрытия потребностей "Чата" и "Управления моделями" в локальной среде является Open WebUI (ранее известный как Ollama WebUI). Это решение де-факто стало стандартом для взаимодействия с Ollama благодаря своей гибкости, открытости и UX, практически идентичному ChatGPT.


2.1 Архитектура и возможности интерфейса


Open WebUI представляет собой самодостаточное веб-приложение, которое может быть развернуто локально через Docker, Kubernetes или Podman.1
* Полная автономность: Приложение хранит все данные (историю чатов, настройки, векторные индексы) в локальной базе данных (SQLite или PostgreSQL), что гарантирует отсутствие утечек данных вовне.1
* Поддержка мультимодальности: Интерфейс нативно поддерживает работу с изображениями (Vision Models), позволяя пользователю загружать картинки для анализа локальными моделями (например, Llama-3.2-Vision), что является важной частью современного взаимодействия с ИИ.10
* Адаптивность: Решение поддерживает PWA (Progressive Web App), что позволяет использовать веб-интерфейс как нативное приложение на мобильных устройствах в локальной сети.1


2.2 Управление моделями (Model Management)


Open WebUI предоставляет мощный графический интерфейс для взаимодействия с API Ollama, удовлетворяя требование "управления моделями".
* Библиотека моделей: Пользователь может просматривать список загруженных моделей, удалять их и загружать новые. В строго локальном режиме (без интернета) поддерживается ручная загрузка файлов .gguf.
* Modelfiles и Мета-модели: Одной из ключевых функций является возможность создания пользовательских моделей прямо в интерфейсе. Пользователь может взять базовую модель (например, llama3), добавить к ней системный промпт (например, "Ты опытный юрист по российскому праву"), настроить параметры температуры и контекстного окна, и сохранить это как новую сущность в списке моделей без дублирования физических весов на диске.5
* Прямая загрузка GGUF: Интерфейс позволяет загружать кастомные квантованные модели (GGUF) через веб-форму, автоматически создавая для них записи в Ollama. Это критически важный мост для интеграции результатов дообучения.13


2.3 RAG как замена и дополнение к обучению


Важно отметить, что Open WebUI реализует мощный механизм RAG (Retrieval Augmented Generation), который часто путают с дообучением.
* Механизм: Пользователь загружает документы (PDF, DOCX, TXT), система векторизует их локально (используя встроенную модель эмбеддингов) и сохраняет в векторную БД (ChromaDB). При запросе система находит релевантные куски текста и подает их модели в контекст.3
* Ограничение: Это не является дообучением (Fine-tuning) в строгом смысле, так как веса нейронной сети не изменяются. Запрос пользователя четко требует "Fine-tuning/LoRA", поэтому одного Open WebUI недостаточно.5
________________


Глава 3. Двигатель Дообучения: LLaMA-Factory и Unsloth


Для выполнения требования по "дообучению (Fine-tuning/LoRA)" необходим специализированный инструмент. Исследование однозначно указывает на связку LLaMA-Factory (интерфейс) и Unsloth (оптимизатор) как на лучшее открытое решение для локальных задач.


3.1 LLaMA-Factory: Графическая студия обучения


LLaMA-Factory — это open-source проект, предоставляющий WebUI ("LLaMA Board") для полного цикла обучения LLM. Он абстрагирует сложность написания кода на Python, предоставляя пользователю панели управления параметрами.2
* Широкая поддержка моделей: Поддерживается более 100 архитектур, включая Llama-3, Qwen, Mistral, Gemma и DeepSeek.2
* Методы обучения:
   * SFT (Supervised Fine-Tuning): Обучение с учителем на парах "инструкция-ответ".
   * DPO/RLHF: Методы согласования (alignment) для улучшения качества ответов.
   * Continuous Pre-training: Дообучение на сырых текстах для внедрения новых знаний.
* Визуализация: Встроенная поддержка отображения графиков функции потерь (loss curves) в реальном времени, что позволяет контролировать процесс обучения без использования сторонних сервисов типа WandB (хотя их поддержка есть).14


3.2 Unsloth: Оптимизация для локального железа


Ключевым фактором для автономной работы на потребительском оборудовании является использование библиотеки Unsloth. LLaMA-Factory имеет нативную интеграцию с Unsloth.16
* Производительность: Unsloth ускоряет обучение в 2-5 раз по сравнению со стандартными реализациями Hugging Face.
* Экономия памяти: Благодаря ручной оптимизации градиентов и использованию QLoRA (квантованного LoRA), Unsloth позволяет дообучать модели уровня Llama-3-8B на видеокартах с 8-10 ГБ VRAM. Без этой оптимизации потребовалось бы минимум 24 ГБ VRAM, что сделало бы решение недоступным для многих локальных пользователей.18


3.3 Экспорт и интеграция с Ollama


Критически важным аспектом "All-in-One" решения является возможность перенести обученную модель обратно в чат. LLaMA-Factory реализует этот процесс через вкладку "Export".20
* Слияние адаптеров: Система позволяет слить (merge) обученные LoRA-адаптеры с базовой моделью.
* Экспорт в Ollama: В последних версиях LLaMA-Factory добавлена прямая поддержка сохранения Modelfile для Ollama. Это означает, что после завершения обучения пользователь может получить готовый к использованию в Ollama файл или запись, замыкая цикл разработки.2
________________


Глава 4. Kolo: Реализация концепции "Всё в одном"


Если Open WebUI и LLaMA-Factory представляют собой лучшие компоненты, то проект Kolo представляет собой попытку их бесшовной интеграции в единый продукт. Kolo — это open-source инструмент, который объединяет Ollama, Unsloth, Llama.cpp, Torchtune и Open WebUI в одном Docker-образе.22


4.1 Архитектура и функциональность Kolo


Kolo позиционируется как инструмент для тех, кто хочет избежать сложности ручной настройки окружения (Python venv, CUDA drivers conflict и т.д.).
* Единый контейнер: Все необходимые инструменты упакованы вместе. Это решает проблему совместимости версий библиотек (dependency hell).
* Автоматизация пайплайна: Kolo предоставляет скрипты для автоматизации всего цикла:
   1. Генерация данных: Создание синтетических датасетов для обучения на основе пользовательских документов.23
   2. Обучение: Запуск Unsloth для дообучения модели.
   3. Развертывание: Автоматическая конвертация результата в GGUF и загрузка в Ollama.
   4. Использование: Мгновенный доступ к новой модели через встроенный Open WebUI.22
* Открытый исходный код: Проект размещен на GitHub, что позволяет пользователю изучить Dockerfile и скрипты сборки, модифицировать их и пересобирать образ локально, удовлетворяя требованию пересборки из исходников.24


4.2 Сравнение Kolo с ручной сборкой


Характеристика
	Kolo
	Ручная сборка (Open WebUI + LLaMA-Factory)
	Сложность установки
	Низкая (1 скрипт/Docker)
	Средняя/Высокая (Docker Compose конфиг)
	Гибкость
	Ограничена скриптами автора
	Максимальная (любые версии компонентов)
	Обновления
	Зависит от мейнтейнера Kolo
	Независимое обновление компонентов
	Стабильность
	Высокая (протестированная связка)
	Возможны конфликты версий API
	Kolo является наиболее близким ответом на запрос пользователя о готовом решении, однако для корпоративного или глубоко кастомизированного использования ручная оркестрация может быть предпочтительнее.
________________


Глава 5. Альтернативные решения и сравнительный анализ


Для полноты исследования необходимо рассмотреть альтернативные проекты, претендующие на звание "All-in-One".


5.1 Text-Generation-WebUI (Oobabooga)


Долгое время этот проект был стандартом де-факто для локального ИИ. Это монолитное приложение на Python (Gradio).
* Плюсы: Имеет встроенную вкладку "Training", поддерживающую LoRA/QLoRA. Умеет работать с огромным количеством бэкендов (ExLlama, AutoGPTQ, llama.cpp).25
* Минусы:
   * UX: Интерфейс перегружен техническими настройками и далек от удобства ChatGPT.
   * Мобильность: Плохо адаптирован для мобильных устройств.
   * Ollama: Хоть и поддерживает API Ollama, Oobabooga скорее конкурирует с ним, чем дополняет. Это "тяжелый" комбайн, который сложнее поддерживать в актуальном состоянии.26
* Вердикт: Подходит для энтузиастов-экспериментаторов, но проигрывает связке Open WebUI + LLaMA-Factory в удобстве повседневного использования и разделении обязанностей.


5.2 Pinokio


Pinokio — это не интерфейс ИИ, а "браузер" или менеджер пакетов для ИИ-приложений.27
* Роль: Он позволяет установить Open WebUI, LLaMA-Factory, ComfyUI и другие инструменты "в один клик".
* Ограничение: Pinokio автоматизирует установку, но не интегрирует приложения между собой. Пользователь получает набор разрозненных инструментов, которые нужно связывать вручную. Однако, для первичного развертывания локального стека это отличный помощник.27


5.3 Проприетарные и облачные альтернативы (для контраста)


Существуют решения вроде H2O LLM Studio или LM Studio.
* LM Studio: Отличный чат и инференс, но закрытый исходный код (Closed Source). Не подходит под требования запроса.4
* H2O LLM Studio: Мощный инструмент для обучения, но ориентирован больше на Enterprise-сегмент и требует сложной настройки для полной локальной автономности.30
________________


Глава 6. Техническая реализация: Создание автономного стека


Для реализации требования "строго локально" и "All-in-One" рекомендуется использование технологии Docker Compose. Ниже приведена концептуальная схема реализации такого решения.


6.1 Требования к инфраструктуре


* ОС: Linux (рекомендуется Ubuntu 22.04) или Windows с WSL2.
* Runtime: Docker Engine + NVIDIA Container Toolkit (для проброса GPU в контейнеры).
* Железо: NVIDIA GPU с поддержкой CUDA (Compute Capability 7.0+, минимум 8 ГБ VRAM для обучения 7B моделей через QLoRA Unsloth).18


6.2 Схема оркестрации (Docker Compose)


Решение состоит из трех сервисов, объединенных в одну виртуальную сеть:
1. Сервис Ollama (Бэкенд):
   * Образ: ollama/ollama:latest (или rocm версия для AMD).
   * Том (Volume): ollama_storage:/root/.ollama — здесь хранятся веса моделей. Этот том делается общим или доступным для сервиса обучения.
   * Сеть: Внутренняя, порт 11434 открыт только внутри Docker-сети.
2. Сервис Open WebUI (Фронтенд):
   * Образ: ghcr.io/open-webui/open-webui:main (собирается из исходников).
   * Переменная окружения: OLLAMA_BASE_URL=http://ollama:11434.32
   * Функция: Предоставляет чат, управление пользователями (RBAC), RAG.
3. Сервис LLaMA-Factory (Обучение):
   * Образ: Собирается из Dockerfile репозитория hiyouga/LLaMA-Factory.
   * Том: Монтирует локальную папку с датасетами и папку для экспорта моделей.
   * Доступ к GPU: Обязателен (--gpus all).
   * Функция: Предоставляет WebUI на порту 7860 для настройки LoRA, запуска обучения и экспорта GGUF/Modelfile.


6.3 Рабочий процесс (Workflow) "All-in-One"


1. Подготовка данных: Пользователь кладет JSONL файл с данными в папку data LLaMA-Factory.
2. Обучение: В интерфейсе LLaMA-Factory выбирается базовая модель (которая может быть подгружена из кэша), датасет и параметры QLoRA (Rank, Alpha). Запускается обучение.
3. Экспорт: После завершения, во вкладке "Export" выбирается формат "Ollama" или "GGUF". LLaMA-Factory производит слияние весов и конвертацию.
4. Интеграция: Полученный файл .gguf либо автоматически попадает в папку моделей Ollama (через общий том), либо загружается пользователем через интерфейс Open WebUI (раздел "Documents" или "Models").
5. Использование: Новая модель появляется в выпадающем списке чата Open WebUI.
________________


Глава 7. Безопасность, Суверенитет и Открытый Код




7.1 Режим "Air-Gap" (Полная изоляция)


Для соблюдения требования "без соединений с внешними серверами":
* Сборка образов: Docker-образы должны быть собраны на машине с интернетом или скачаны как .tar архивы и перенесены на целевую машину.
* Загрузка моделей: Модели (GGUF или SafeTensors) скачиваются вручную с HuggingFace на подключенной машине и переносятся в папку models сервисов.
* Отключение телеметрии: Open WebUI и Ollama позволяют отключить любые попытки проверки обновлений через переменные окружения (например, OLLAMA_NO_TELEMETRY=1).


7.2 Лицензионная чистота и аудит


Все компоненты предлагаемого стека имеют разрешительные лицензии:
* Open WebUI: MIT License (позволяет модификацию, использование в закрытых проектах).
* Ollama: MIT License.
* LLaMA-Factory: Apache 2.0 License.
* Unsloth: Apache 2.0 License (для open-source моделей).
Это полностью удовлетворяет требованию пользователя о возможности "пересборки/модификации из исходников". Пользователь может клонировать репозитории GitHub, внести изменения (например, изменить логотип, добавить поддержку специфичного оборудования) и собрать собственные Docker-контейнеры.


7.3 Управление доступом (RBAC)


В отличие от простых скриптов, Open WebUI предоставляет функции корпоративного уровня безопасности. Администратор может создавать пользователей, назначать им роли ("user", "admin") и ограничивать права на удаление моделей или доступ к определенным функциям. Это делает решение пригодным для развертывания в организациях.1
________________


Глава 8. Будущее локальных интерфейсов




8.1 Конвергенция инструментов


Наблюдается тренд на слияние функционала. Появление "Unsloth Studio" 16 и развитие проекта Kolo говорят о том, что в ближайшие 6-12 месяцев мы увидим появление еще более интегрированных решений, которые скроют сложность Docker под капотом, предлагая опыт установки уровня "Adobe Creative Cloud", но для локального ИИ.


8.2 Агентские рабочие процессы


Следующий шаг развития Open WebUI — это не просто чат, а выполнение задач агентами. Интеграция функций (Tools), таких как поиск в локальной сети, выполнение Python-кода и взаимодействие с API, превращает интерфейс в полноценную операционную систему для ИИ.1
________________


Заключение и Рекомендация


Задача поиска "идеального" монолитного приложения для локального ИИ сталкивается с реальностью фрагментации экосистемы. Однако, использование контейнеризации позволяет создать решение, которое функционально неотличимо от монолита.
Итоговая рекомендация:
   1. Наиболее простое решение ("Коробочный продукт"): Использовать проект Kolo.22
   * Почему: Это готовая сборка (Docker), которая включает Ollama, Unsloth, Torchtune и Open WebUI. Она создана специально для того, чтобы дать пользователю возможность "Generate -> Fine-tune -> Chat" в одном месте с минимальной настройкой. Это Open Source, работает локально и поддерживает пересборку.
   2. Наиболее гибкое и профессиональное решение: Самостоятельная оркестрация Open WebUI + LLaMA-Factory через Docker Compose.
   * Почему: Этот подход дает лучший в классе интерфейс чата (Open WebUI) и лучший в классе интерфейс обучения (LLaMA-Factory). Это решение более устойчиво к изменениям, масштабируемо и предоставляет максимальный контроль над каждым аспектом системы.
Оба варианта полностью удовлетворяют всем критериям пользователя: открытость, локальность, функциональность и модифицируемость.
________________


Приложение: Сравнительная таблица решений


Характеристика
	Open WebUI + LLaMA-Factory
	Kolo
	Text-Generation-WebUI
	Интерфейс чата
	Отличный (ChatGPT-style)
	Отличный (через Open WebUI)
	Средний (Gradio)
	Интерфейс обучения
	Отличный (LLaMA Board)
	Скриптовый/Автоматизированный
	Хороший (Native Tab)
	Управление моделями
	Полное (Ollama API)
	Полное (Ollama API)
	Полное (Свои лоадеры)
	Сложность старта
	Средняя (Docker Compose)
	Низкая (Готовый образ)
	Средняя (Python venv)
	Мобильная версия
	Да (PWA)
	Да (PWA через Open WebUI)
	Нет
	Поддержка Unsloth
	Да (в LLaMA-Factory)
	Да (Встроено)
	Частично (через расширения)
	Лицензия
	MIT / Apache 2.0
	Apache 2.0
	AGPL-3.0
	Данный отчет подтверждает, что, несмотря на отсутствие единого .exe файла, решающего все задачи, экосистема Open Source предоставляет все необходимые "кирпичики" для построения суверенной AI-лаборатории.
Источники
   1. Features | Open WebUI, дата последнего обращения: ноября 25, 2025, https://docs.openwebui.com/features/
   2. hiyouga/LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024) - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/hiyouga/LLaMA-Factory
   3. Build Your Offline AI Assistant with AnythingLLM, Ollama, and RAG - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@m.tugcezile/build-your-offline-ai-assistant-with-anythingllm-ollama-and-rag-0ca601c936ae
   4. Offline Operation | LM Studio Docs, дата последнего обращения: ноября 25, 2025, https://lmstudio.ai/docs/app/offline
   5. Model training using OpenWebUI - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/OpenWebUI/comments/1koxphh/model_training_using_openwebui/
   6. ollama-rest - crates.io: Rust Package Registry, дата последнего обращения: ноября 25, 2025, https://crates.io/crates/ollama-rest
   7. [FEEDBACK] Better packaging for llama.cpp to support downstream consumers #15313 - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/ggml-org/llama.cpp/discussions/15313
   8. Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation, дата последнего обращения: ноября 25, 2025, https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/tutorial-how-to-finetune-llama-3-and-use-in-ollama
   9. Unsloth AI - Open Source Fine-tuning & RL for LLMs, дата последнего обращения: ноября 25, 2025, https://unsloth.ai/
   10. Part 1: Getting Started with Local AI - Ollama & Open WebUI | by John Wong | Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@able_wong/getting-started-with-local-ai-ollama-open-webui-part-1-bcaafcce6df7
   11. MedGemma 27b-it (multimodal) won't accept images in Open WebUI 0.6.16? - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/OpenWebUI/comments/1m2dylw/medgemma_27bit_multimodal_wont_accept_images_in/
   12. Working with LLMs using Open WebUI - Rancher Desktop Docs, дата последнего обращения: ноября 25, 2025, https://docs.rancherdesktop.io/next/tutorials/working-with-llms/
   13. How to transfer Ollama models with vision support to an offline system (Open WebUI + Ollama) : r/OpenWebUI - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/OpenWebUI/comments/1kcx68v/how_to_transfer_ollama_models_with_vision_support/
   14. Mastering LLM Fine-Tuning: A Practical Guide with LLaMA-Factory and LoRA, дата последнего обращения: ноября 25, 2025, https://programmer.ie/post/fine_tuning/
   15. Evaluating LLMs with MLflow: A Practical Beginner's Guide - DataCamp, дата последнего обращения: ноября 25, 2025, https://www.datacamp.com/tutorial/evaluating-llms-with-mlflow
   16. Unsloth AI: A Deep Dive into Faster, More Efficient LLM Fine-Tuning - Skywork.ai, дата последнего обращения: ноября 25, 2025, https://skywork.ai/skypage/en/Unsloth-AI:-A-Deep-Dive-into-Faster,-More-Efficient-LLM-Fine-Tuning/1972856091659923456
   17. unslothai/unsloth-studio - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/unslothai/unsloth-studio
   18. unslothai/unsloth: Fine-tuning & Reinforcement Learning for LLMs. Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with 70% less VRAM. - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/unslothai/unsloth
   19. 80% faster, 50% less memory, 0% accuracy loss Llama finetuning : r/LocalLLaMA - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/LocalLLaMA/comments/188197j/80_faster_50_less_memory_0_accuracy_loss_llama/
   20. LLaMA-Factory - Qwen, дата последнего обращения: ноября 25, 2025, https://qwen.readthedocs.io/en/v2.0/training/SFT/llama_factory.html
   21. Ollama Modelfile Creation and Usage Guide | LlamaFactory, дата последнего обращения: ноября 25, 2025, https://www.llamafactory.cn/ollama-docs/en/modelfile.html
   22. Want to learn how to fine tune your own Large Language Model? I created a helpful guide!, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1ijhfau/want_to_learn_how_to_fine_tune_your_own_large/
   23. Generate Synthetic QA training data for your fine tuned models with Kolo using any text file! Quick & Easy to get started! : r/LLMDevs - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/LLMDevs/comments/1io1nb4/generate_synthetic_qa_training_data_for_your_fine/
   24. MaxHastings/Kolo: The Fastest Way to Fine-Tune LLMs Locally - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/MaxHastings/Kolo
   25. oobabooga/text-generation-webui: The definitive Web UI for local AI, with powerful features and easy setup. - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/oobabooga/text-generation-webui
   26. I created a web UI for Ollama that lets you talk to your models and manage them - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/
   27. Pinokio, дата последнего обращения: ноября 25, 2025, https://pinokio.co/
   28. Automate AI Like a Pro with Pinokio - YouTube, дата последнего обращения: ноября 25, 2025, https://www.youtube.com/watch?v=Z_j6PYcWXko
   29. ParthaPRay/Curated-List-of-Generative-AI-Tools - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/ParthaPRay/Curated-List-of-Generative-AI-Tools
   30. H2O LLM Studio | H2O.ai, дата последнего обращения: ноября 25, 2025, https://h2o.ai/platform/llm-studio/
   31. H2O LLM Studio, no-code GUI, fine-tuning LLMs | Cloudron Forum, дата последнего обращения: ноября 25, 2025, https://forum.cloudron.io/topic/9206/h2o-llm-studio-no-code-gui-fine-tuning-llms
   32. Running Local AI on Linux With GPU: Ollama + Open WebUI + Gemma - DEV Community, дата последнего обращения: ноября 25, 2025, https://dev.to/lovestaco/running-local-ai-on-linux-with-gpu-ollama-open-webui-gemma-546h
   33. Tools & Functions (Plugins) - Open WebUI, дата последнего обращения: ноября 25, 2025, https://docs.openwebui.com/features/plugin/