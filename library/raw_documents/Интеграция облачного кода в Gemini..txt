Экспертный Отчет: Автоматизация Развертывания и Регистрации Custom Tools для LLM-Агентов Gemini: Сравнительный Анализ MLOps Workflow (CLI) против Ручного Консольного Управления




I. Архитектурный Контекст и Понимание Function Calling (Tool Use)




1.1. Роль External Tools в Расширении Возможностей Gemini (RAG и Action Taking)


Современные Large Language Models (LLM), такие как Gemini, способны значительно расширять свои базовые возможности генерации текста за счет механизма, известного как Function Calling или Tool Use. Этот механизм позволяет LLM взаимодействовать с внешними системами и данными, преодолевая ограничения статического набора знаний. Когда модель получает запрос, требующий актуальной или динамической информации (например, текущей погоды или данных из корпоративной базы), она не выполняет код напрямую, а генерирует структурированный вывод в формате JSON, который представляет собой интенцию вызова внешней функции и ее необходимые аргументы.1
Функциональная диверсификация, обеспечиваемая Tool Use, включает два основных сценария. Во-первых, это Извлечение Данных (Retrieval-Augmented Generation, RAG), при котором инструменты используются для доступа к актуальной информации, конвертации валют, или получении специфических данных из API.1 Во-вторых, это Выполнение Действий (Action Taking), позволяющее агенту оркестровать внешние операции, такие как отправка форм, обновление состояния приложения, или выполнение сложных многоэтапных агентских рабочих процессов.1
Для того чтобы LLM могла надежно "решить" вызвать внешний инструмент, этот инструмент должен быть описан с использованием строгого и стандартизированного технического контракта — спецификации OpenAPI 3.0.2 Модель Gemini полагается исключительно на это описание (и системные инструкции) для принятия решения о вызове.1 Если контракт, то есть OpenAPI схема, не отражает актуальный и развернутый API или содержит неточности, LLM-агент может принять неверное решение о вызове (или сгенерировать неверные аргументы), что приведет к сбоям в работе системы. Следовательно, в MLOps пайплайне генерация и валидация схемы должны быть жестко привязаны непосредственно к коду развертываемой функции, чтобы избежать критического "дрейфа контракта" (schema drift). Ручное управление схемой (как в Пути B) представляет собой высокий риск для надежности всей AI-системы.


1.2. Обзор Технологического Стека Google Cloud для Custom Tooling


Развертывание кастомной функции из облачного репозитория и ее регистрация в платформе Gemini требует интеграции нескольких ключевых сервисов Google Cloud Platform (GCP).
1. Serving Layer (Cloud Run): Для размещения кастомной функции предпочтительно использовать Cloud Run. Этот бессерверный контейнерный сервис обеспечивает автоматическую масштабируемость и простоту развертывания. Cloud Run поддерживает прямой деплоймент из исходного кода, используя флаг --source.3
2. Agent Platform (Vertex AI Agent Builder): Эта платформа служит центральным хабом для создания, управления и масштабирования LLM-агентов.4 В контексте постоянного использования инструментов, Agent Builder является местом, где регистрируются кастомные инструменты типа 'OPENAPI'.5 В отличие от прямого Function Calling через Gemini API, где спецификация передается при каждом вызове, регистрация Tool в Agent Builder делает его постоянной, управляемой частью системы агента.4
3. CI/CD Foundation (Buildpacks/Artifact Registry): При использовании команды gcloud run deploy --source, автоматизация Google Cloud, использующая Buildpacks, автоматически преобразует исходный код (например, Python) из Git в готовый контейнерный образ без необходимости написания и поддержки традиционного Dockerfile.3 Полученный образ автоматически сохраняется в Artifact Registry в проекте. Это значительно упрощает MLOps workflow.


1.3. Принцип Работы Function Calling: Жизненный Цикл Запроса и Регистрации Tool


Жизненный цикл использования инструмента агентом Vertex AI состоит из следующих последовательных этапов:
1. Определение и Регистрация: Разработчик определяет функцию (код) и ее декларацию (OpenAPI schema). Спецификация регистрируется в Vertex AI Agent Builder.1
2. Запрос: Пользователь отправляет запрос агенту, например, "Что за курс обмена доллара к шведской кроне?".6
3. Генерация Вызова: LLM, используя внутренние рассуждения, определяет, что для ответа требуется внешний инструмент. Модель генерирует структурированный JSON-вывод, specifying tool_call с именем функции и необходимыми параметрами.7
4. Выполнение: Приложение (runtime), в котором работает агент, перехватывает JSON-вызов и выполняет соответствующий HTTP-вызов к развернутому Cloud Run Endpoint.
5. Обратная Связь: Результат выполнения API (например, JSON с курсом обмена) возвращается модели как дополнительный контекст.
6. Финальный Ответ: Модель использует этот контекст для генерации окончательного, человекочитаемого ответа пользователю.1
Table 1.1: Сравнение Подходов к Интеграции Custom Tooling


Подход
	Цель
	Требование к Спецификации
	Управление
	Прямой Gemini API (Function Calling)
	PoC, Сессионная работа
	Передается в каждом generateContent вызове
	Скрипт-ориентированное
	Vertex AI Agent Builder (OpenAPI Tool)
	Производство, постоянные агенты
	Регистрация в консоли/Tool API
	Платформенное (Agent Builder) 4
	

II. Требования к Инфраструктуре и Исходному Коду




2.1. Подготовка Облачного Проекта и Требования IAM


Для успешного развертывания функции из облачного репозитория и ее интеграции в Vertex AI Agent Builder необходима тщательная предварительная настройка инфраструктуры и прав доступа (IAM).
Базовая конфигурация включает выбор или создание проекта, активацию биллинга и включение требуемых API, таких как Google Generative Language API, Cloud Run API, Cloud Build API, и Artifact Registry API.9
Для Автоматизированного Развертывания (Путь A), Service Account, выполняющий CI/CD пайплайн (например, Cloud Build Service Account), должен обладать следующими критическими ролями для обеспечения бесперебойного процесса:
* roles/run.admin (Cloud Run Admin): Необходима для создания и обновления сервиса Cloud Run.9
* roles/run.developer (Cloud Run Source Developer): Требуется для запуска сборки контейнера из исходного кода.9
* roles/artifactregistry.writer: Это критически важная, но часто упускаемая роль. Когда используется gcloud run deploy --source, процесс Buildpacks автоматически создает контейнерный образ и сохраняет его в Artifact Registry с именем cloud-run-source-deploy.3 Если CI/CD Service Account не имеет прав на запись в этот репозиторий, автоматизированное развертывание завершится ошибкой, даже если права на Cloud Run установлены корректно.
Архитектурная зрелость требует явного определения этих "теневых зависимостей" IAM. MLOps-инженеры должны включить все эти роли в декларативную конфигурацию инфраструктуры (IaC), чтобы гарантировать воспроизводимость и безопасность развертывания.
Кроме того, для обеспечения безопасного вызова (Security Principle), Service Account, под которым работает LLM-агент Vertex AI, должен иметь роль roles/run.invoker. Это позволяет агенту вызывать приватный, аутентифицированный Cloud Run сервис.
Table 2.2: Ключевые IAM Роли для Автоматизированного Развертывания


Действие
	Необходимая IAM Роль
	Обоснование
	Развертывание Cloud Run Service
	roles/run.admin
	Управление жизненным циклом сервиса Cloud Run.9
	Запуск Сборки из Исходника
	roles/run.developer
	Требуется для --source развертывания.9
	Запись Образа Контейнера
	roles/artifactregistry.writer
	Требуется для Buildpacks, использующих Artifact Registry.3
	Программная Регистрация Tool
	roles/aiplatform.editor (или специализированная)
	API доступ к управлению инструментами Vertex AI.5
	Вызов Агентом Приватного Сервиса
	roles/run.invoker
	Аутентифицированный вызов Cloud Run Endpoint LLM-агентом.
	

2.2. Структура Репозитория и Требования к Исходному Коду Python


Исходный код должен быть структурирован таким образом, чтобы Buildpacks мог его автоматически обнаружить и контейнеризировать. Типичная структура репозитория для функции, развертываемой через Cloud Run, включает:






/repo_root
 /service_name
   main.py (HTTP handler for Tool)
   requirements.txt (перечисление зависимостей, например, functions-framework, flask) [11]
   tool_spec_template.yaml (базовая спецификация OpenAPI)
   deployment_script.sh (логика CI/CD)

Функция должна представлять собой легкий HTTP-обработчик, который быстро обрабатывает входящий запрос от агента и возвращает структурированные данные (например, JSON).


2.3. Генерация Схемы OpenAPI 3.0: Ядро Контракта


Спецификация OpenAPI является критическим контрактом между кодом разработчика и моделью LLM. Ключевые поля, необходимые для интеграции с Cloud Run:
1. servers/url: Этот URL должен быть динамически извлечен после развертывания Cloud Run. Поскольку Cloud Run по умолчанию генерирует уникальные URL, ручное управление этим полем невозможно в масштабе.
2. paths: Конечные точки, которые LLM может вызывать.
3. securityDefinitions: Секция для определения механизма аутентификации. Поскольку сервис Cloud Run должен быть приватным, здесь указывается требование аутентификации, как правило, через Google ID Token.
Для обеспечения максимальной эффективности Function Calling, рекомендуется следовать лучшим практикам 1: использовать сильную типизацию параметров (Strong Typed Parameters), а также писать четкие, подробные описания функций и их параметров, поскольку эти описания напрямую влияют на качество принятия решений моделью.


III. Путь Развертывания A: Автоматизированный MLOps Workflow (CLI/Script)


Автоматизированный MLOps рабочий процесс является единственно надежным и масштабируемым решением для интеграции функций облачного репозитория в производственную среду LLM-агентов. Этот подход соответствует принципам Infrastructure as Code (IaC) и Continuous Integration/Continuous Delivery (CI/CD).


3.1. Этап 1: Непрерывное Развертывание из Git (CI/CD Pipeline)


Процесс начинается с настройки триггера в Cloud Build, который автоматически запускает пайплайн при коммите в основной репозиторий.
Ключевой шаг — это атомарная команда развертывания, выполняемая CI/CD Service Account:


Bash




# 1. Развертывание сервиса и автоматическая сборка
gcloud run deploy --source. --region --no-allow-unauthenticated 

# 2. Извлечение URL развернутого сервиса
SERVICE_URL=$(gcloud run services describe --region --format='value(status.url)')

Использование флага --source 3 позволяет системе автоматически определить язык (например, Python) и зависимости (requirements.txt), построить контейнерный образ с помощью Buildpacks и развернуть его в Cloud Run, что значительно ускоряет цикл CI/CD, устраняя необходимость в ручном управлении Docker. Использование --no-allow-unauthenticated принудительно устанавливает сервис как приватный, что является фундаментальным требованием безопасности.
Необходимость программного извлечения SERVICE_URL после развертывания критически важна. Этот динамически сгенерированный URL гарантирует, что следующий шаг (обновление схемы) всегда будет ссылаться на актуальный и правильно работающий сервис.


3.2. Этап 2: Программная Валидация и Обновление Схемы


После успешного развертывания сервиса и извлечения его URL, пайплайн выполняет скрипт (часто Python), который обеспечивает синхронизацию между развернутым кодом и его контрактом:
1. Загрузка шаблона: Считывается базовый tool_spec_template.yaml.
2. Динамическая Инъекция: Извлеченный SERVICE_URL вставляется в поле servers/url спецификации OpenAPI.
3. Валидация: Финальная схема валидируется на соответствие стандарту OpenAPI 3.0.
Этот процесс гарантирует, что LLM-агент всегда получает корректный и актуальный сетевой адрес для вызова функции, исключая ошибки, связанные с ручным копированием и вставкой.


3.3. Этап 3: Регистрация Tool через Vertex AI Tool API (Agent Command)


Автоматизация регистрации Tool в Agent Builder является кульминацией MLOps пайплайна. Вместо ручного копирования в консоль, CI/CD использует Vertex AI Agent Builder SDK или REST API для создания или обновления зарегистрированного Tool.
Скрипт использует API для программной регистрации:


Python




# Пример использования SDK для обновления Tool
# (Требует наличия соответствующих ролей IAM для доступа к AI Platform API)
from vertexai.agent_builder.tools import Tool
#...
tool = Tool.register(
   display_name="CloudRepoFunction",
   tool_type="OPENAPI",
   openapi_spec=load_yaml('tool_spec.yaml'),
   project_id=PROJECT_ID
)
tool.update() 

Программный подход обеспечивает мгновенное обновление контракта, версионирование и полный аудит изменений.


3.4. Этап 4: Настройка Безопасности и Тестирование


Последний критический шаг автоматизированного пайплайна — настройка безопасности. Если сервис Cloud Run приватный (что обязательно для производства), необходимо предоставить LLM-агенту доступ к этому сервису.
1. Назначение Invoker Role: Пайплайн должен настроить IAM Policy на развернутом Cloud Run Service, предоставив Service Account, связанному с Vertex AI Agent Engine, роль roles/run.invoker. Это гарантирует, что только авторизованный агент может вызывать инструмент, реализуя принцип минимальных привилегий.
2. Интеграционное Тестирование: Пайплайн завершается автоматизированным тестом, который вызывает конечную точку агента, чтобы убедиться, что агент успешно генерирует tool_calls и корректно обрабатывает полученный результат.6


IV. Путь Развертывания B: Ручное Интегрирование через Консоль Vertex AI (Console Workflow)


Ручное управление через консоль Vertex AI Agent Builder и Gcloud CLI Workflow (Путь B) подходит исключительно для прототипирования (PoC) и не может быть использовано в производственной среде из-за неконтролируемых рисков, связанных с безопасностью, синхронизацией и отсутствием аудита.


4.1. Ручное Развертывание и Извлечение URL


Разработчик вручную выполняет команду gcloud run deploy и ждет вывода URL сервиса. В целях упрощения, на этапе PoC часто выбирается публичный доступ (Allow public access) 12, что является неприемлемым риском для production-систем, особенно если функция обрабатывает конфиденциальные данные. Отсутствие автоматической настройки IAM делает этот процесс небезопасным.


4.2. Ручная Генерация и Валидация OpenAPI


Ключевой недостаток ручного пути — это синхронизация контракта. Разработчик должен вручную скопировать полученный URL Cloud Run и отредактировать локальный файл tool_spec.yaml, вставив URL в секцию servers. Этот процесс подвержен ошибкам копирования/вставки, и часто приводит к тому, что в Agent Builder регистрируется неактуальная спецификация.
Если функция развертывается 10 раз за месяц, MLOps Workflow (Путь A) автоматически выполняет 10 синхронизированных обновлений. В Ручном Пути (B), разработчик должен выполнить около 30 ручных действий (Deploy, Get URL, Update YAML, Paste YAML 5). Это создает огромный технический долг, вызывает расхождение между версиями кода и спецификации, и в конечном итоге, приводит к ненадежному поведению LLM-агента.


4.3. Шаги по Регистрации Tool в Vertex AI Agent Builder Console


После того, как OpenAPI спецификация вручную обновлена, разработчик должен выполнить следующие ручные шаги:
1. Перейти в консоль Google Cloud, раздел Vertex AI Agent Builder, затем в секцию Tools.4
2. Создать новый инструмент, выбрав тип "OPENAPI".5
3. Скопировать и вставить все содержимое YAML в предоставленное текстовое поле.5
4. Вручную сконфигурировать параметры аутентификации, если Cloud Run приватный.
Вся эта последовательность действий лишена аудита и не позволяет отслеживать, какая версия схемы соответствует какой версии кода.


V. Сравнительный Анализ и Выбор Стратегии (CLI vs. Console)


Сравнительный анализ двух рабочих процессов показывает, что для достижения MLOps зрелости, безопасности и масштабируемости, выбор должен однозначно пасть на автоматизированный подход.
Table 5.1: Сравнение Рабочих Процессов Развертывания и Регистрации Custom Tool


Критерий
	Путь A: Автоматизированный MLOps (CLI/Script)
	Путь B: Ручное Управление (Console UI)
	Уровень Зрелости
	Production-Ready, IaC, Воспроизводимость
	Прототипирование, Тестирование 13
	Развертывание (Git)
	Автоматизированный CI/CD, gcloud deploy --source 3
	Ручное CLI/UI
	Синхронизация Схемы
	Автоматическая (Dynamic URL Injection)
	Ручная, высокий риск дрейфа контракта
	Безопасность IAM
	Встроена в пайплайн (Автоматическое назначение roles/run.invoker)
	Зависит от ручной, подверженной ошибкам конфигурации 10
	Время Обновления Tool
	Минуты (автоматический цикл)
	Часы (включая ручную валидацию и ошибки)
	Аудит и Версионирование
	Полный аудит через Cloud Build/Git/Artifact Registry
	Низкий аудит, отслеживание затруднено
	

5.1. Рекомендации по Безопасности и Аутентификации API Calls


Аутентификация API-вызовов является критической точкой интеграции. Принцип Нулевого Доверия (Zero Trust) требует, чтобы Cloud Run Service был развернут как приватный (--no-allow-unauthenticated). Это предотвращает несанкционированные вызовы функции.
В приватной конфигурации аутентификация вызова Tool должна осуществляться через Google ID Tokens. Это означает, что Service Account, связанный с Vertex AI Agent Engine, должен иметь роль roles/run.invoker только для этого конкретного Cloud Run сервиса. Только автоматизированный пайплайн (Путь A) позволяет программно реализовать эту тонкую настройку IAM, гарантируя, что безопасность является неотъемлемой и повторяемой частью каждого развертывания. При ручном подходе (Путь B) эта настройка является сложной и часто игнорируется, что может привести к критическим уязвимостям.


VI. Заключение и Стратегические Рекомендации


Для исправления и внедрения функции, позволяющей агенту Gemini надежно считывать код из облачного репозитория, критически необходимо использовать Путь A: Автоматизированный MLOps Workflow. Ручное управление (Путь B) не соответствует требованиям производственной надежности, безопасности и масштабируемости.


6.1. Резюме: MLOps для LLM Tooling


Концепция "команды агенту" в контексте MLOps не сводится к одной CLI-команде, а представляет собой интегрированный, безопасный, и версионированный пайплайн, который автоматически выполняет следующие атомарные шаги:
1. Deployment: Развертывание функции из Git, используя gcloud run deploy --source. (Build & Serve).
2. Extraction: Программное извлечение динамического и актуального SERVICE_URL.
3. Schema Update: Обновление спецификации OpenAPI путем вставки динамического URL, обеспечивая синхронизацию контракта.
4. Registration: Использование Vertex AI Tool API или SDK для автоматического обновления Tool в Agent Builder (Agent Command).
5. Security: Автоматическое назначение roles/run.invoker Service Account агента на приватный Cloud Run Service.


6.2. План дальнейшего развития: Интеграция и Масштабирование


Для повышения зрелости системы и подготовки к масштабированию, когда количество инструментов начнет расти, рекомендуются следующие архитектурные шаги:
1. Декларативное Управление Инфраструктурой (IaC): Переход от скриптов Bash/Python к декларативному управлению инфраструктурой (например, Terraform или Pulumi) для управления IAM, Cloud Run Service и даже настройками Agent Builder. Это обеспечит полный аудит и воспроизводимость инфраструктуры.
2. Централизация API Management: В условиях, когда агенты будут использовать множество инструментов, рекомендуется использовать Apigee API Hub, который Vertex AI Agent Builder может интегрировать.4 Apigee служит централизованным каталогом для управления, обеспечения безопасности и версионирования всех LLM Tools, предоставляя единую точку контроля над всей инструментальной поверхностью агента.
3. Использование Agent Development Kit (ADK): Мониторинг и внедрение Vertex AI Agent Development Kit (ADK) 4 позволит стандартизировать процесс определения и регистрации Tool. ADK предоставляет фреймворк, который упрощает создание сложных мультиагентных систем, предлагая специализированные библиотеки для управления функциональностью инструмента.
