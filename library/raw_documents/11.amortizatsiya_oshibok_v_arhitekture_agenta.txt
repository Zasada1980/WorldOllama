Архитектура Отказоустойчивого Агента: Интеграция Принципа TRIZ №11 (Заблаговременная Амортизация)




Аннотация


В современном ландшафте искусственного интеллекта надежность автономных агентов становится критическим барьером для их внедрения в промышленные и социально значимые процессы. В отличие от детерминированного программного обеспечения, большие языковые модели (LLM) оперируют в вероятностном пространстве, где ошибки — галлюцинации, логические сбои, токсичные выбросы — являются не исключением, а статистической неизбежностью. Данный исследовательский отчет предлагает фундаментальный пересмотр архитектуры ИИ-агентов через призму Теории решения изобретательских задач (ТРИЗ), в частности, Принципа №11 «Заблаговременная Амортизация» (Beforehand Cushioning).
В работе проводится глубокий анализ методов квантификации неуверенности (Uncertainty Quantification), включая новейшие подходы Inverse-Entropy Weighted Voting и Semantic Entropy, которые служат триггерами для протоколов «Мягкой Посадки». Исследуется метафора аэрокосмических систем аварийного спасения (Launch Abort Systems) для создания механизмов прерывания генерации (Circuit Breakers) и стримингового мониторинга безопасности. Рассматриваются стратегии компенсации противоречивости данных в RAG-системах (Retrieval-Augmented Generation) через мультиагентные дебаты и синтез конфликтующих источников. Цель отчета — предоставить исчерпывающее руководство по созданию агентов, способных сохранять операционную устойчивость и доверие пользователя в условиях неопределенности.
________________


1. Введение: Эпистемологический Кризис и Принцип Амортизации




1.1. От бинарной надежности к вероятностной устойчивости


Традиционная инженерия программного обеспечения (Software Engineering) базируется на детерминизме. Если сервер базы данных недоступен, система возвращает ошибку 500. Если переменная не определена, выбрасывается исключение. Эти состояния бинарны: система либо работает, либо сломана. В эпоху генеративного ИИ (GenAI) мы сталкиваемся с эпистемологическим кризисом: агент может работать технически безупречно — возвращать синтаксически корректный JSON, отвечать с минимальной задержкой — но при этом генерировать контент, который фактологически ложен, этически неприемлем или логически противоречив.1
Эта «тихая поломка» (silent failure) требует смены парадигмы обеспечения качества. Мы не можем гарантировать отсутствие ошибок в стохастической системе, но мы можем спроектировать архитектуру так, чтобы последствия этих ошибок были смягчены в момент их возникновения. Именно здесь на сцену выходит Принцип ТРИЗ №11 «Заблаговременная Амортизация» (Beforehand Cushioning).


1.2. Принцип TRIZ №11: Декомпозиция для когнитивных архитектур


Классическая формулировка принципа №11 в ТРИЗ гласит: «Подготовить аварийные средства заранее, чтобы компенсировать относительно низкую надежность объекта».2 В физическом мире примерами служат подушки безопасности в автомобилях, запасные парашюты или магнитные полосы на фотопленке.2
В контексте когнитивных архитектур ИИ «объектом с низкой надежностью» является сам процесс авторегрессионной генерации токенов. Применение принципа №11 требует внедрения механизмов, которые находятся в режиме ожидания (standby) и активируются только при детектировании риска.
Таблица 1.1. Проекция Принципа №11 на архитектуру ИИ-агента


Аспект ТРИЗ
	Физический аналог
	Аналог в ИИ-Агенте
	Функция
	Детекция угрозы
	Акселерометр подушки безопасности
	Квантификация неуверенности (Logprobs/Entropy) 4
	Обнаружение отклонения от нормальной траектории генерации.
	Амортизация удара
	Раскрытие купола парашюта
	Переключение на хеджирование (Linguistic Hedging) 5
	Снижение категоричности ответа для минимизации ответственности.
	Спасение
	Система катапультирования пилота
	Прерывание генерации (Circuit Breaker) 6
	Принудительная остановка при детекции токсичности или галлюцинации.
	Компенсация
	Резервная система управления
	Fallback-стратегии (Web Search, Ask User) 7
	Переход к альтернативному способу решения задачи при отказе основного.
	

1.3. Психологический аспект: Алгоритмическое отвращение и доверие


Внедрение амортизации — это не только техническая, но и психо-эргономическая необходимость. Исследования показывают существование феномена «алгоритмического отвращения» (algorithmic aversion): пользователи теряют доверие к алгоритмам гораздо быстрее, чем к людям, после единичной ошибки.8 Однако, если система демонстрирует способность к самодиагностике и «честному» признанию ограничений (позитивный отказ), доверие может парадоксальным образом возрасти.9 Амортизация трансформирует опыт неудачи из «поломки» в «проявление компетентности через осторожность».
________________


2. Часть I. Протоколы «Мягкой Посадки» (Soft Landing Protocols)


Реализация принципа амортизации начинается с создания системы раннего предупреждения. Агент не может «подстелить соломку», если не знает, что падает. Протоколы «Мягкой Посадки» (Soft Landing Protocols) — это комплекс алгоритмов, позволяющих агенту плавно изменить стратегию взаимодействия при снижении уверенности, вместо того чтобы упорствовать в генерации потенциально ложного факта.


2.1. Квантификация Неуверенности: Математика сомнений


Для реализации триггера безопасности необходимо выйти за пределы бинарной логики и использовать вероятностные метрики. Исследования 2024-2025 годов предлагают ряд продвинутых методов оценки уверенности (Confidence Score), которые значительно превосходят простые вероятности токенов.


2.1.1. Энтропия и Logprobs: Сигналы первого уровня


На базовом уровне каждая LLM возвращает логарифмические вероятности (logprobs) для каждого сгенерированного токена. Высокая энтропия Шеннона в распределении вероятностей следующего токена указывает на то, что модель «колеблется» между несколькими вариантами.4
Однако простое использование logprobs может вводить в заблуждение, так как модели часто бывают «уверенно неправы» (overconfident). Для решения этой проблемы был разработан метод Inverse-Entropy Weighted (IEW) Voting.4 В этом подходе:
1. Генерируется несколько цепочек рассуждений (Reasoning Chains).
2. Для каждой цепочки рассчитывается энтропия на уровне токенов.
3. При агрегации ответов голоса взвешиваются обратно пропорционально их энтропии (чем выше неопределенность, тем ниже вес голоса).
Этот метод показал эффективность в 96.7% последовательных конфигураций, превосходя традиционные базовые линии.4 Это позволяет использовать IEW как надежный триггер: если взвешенная уверенность падает ниже порога $T$, активируется протокол амортизации.


2.1.2. Семантическая Энтропия и Самосогласованность


Более сложный, но эффективный метод — измерение семантической неопределенности. Метод Kernel Language Entropy (KLE) 10 и концепция Semantic Entropy 1 решают проблему, когда модель генерирует лексически разные, но семантически идентичные ответы (например, «Берлин» и «Столица Германии»).
Алгоритм работает следующим образом:
1. Генерируется $N$ выборок ответов (Sampled Responses).
2. Ответы кластеризуются по семантическому смыслу (используя модель-импликатор или эмбеддинги).
3. Если все ответы попадают в один семантический кластер — уверенность высокая.
4. Если ответы распределены по разным, противоречащим друг другу кластерам (например, разные даты события) — энтропия высока, уверенность низкая.
Исследования показывают, что методы, основанные на энтропии и согласованности (consistency-based methods), эффективно оценивают неопределенность модели даже в присутствии неопределенности данных (Data Uncertainty), когда вопрос допускает несколько правильных ответов.1


2.1.3. Конформное прогнозирование (Conformal Prediction)


Для задач, где цена ошибки критически высока, применяется Конформное Прогнозирование.12 Этот статистический метод позволяет построить множество прогнозов $C(X)$, которое гарантированно содержит истинный ответ $Y$ с заданной вероятностью $1-\alpha$ (например, 95%).
* Механизм: Калибровка порогового значения сходства ответов на основе отложенной выборки.
* Амортизация: Если размер множества прогнозов $C(X)$ становится слишком большим (система не может сузить круг вариантов) или пустым (система отвергает все варианты), это служит сигналом для отказа от ответа («I don't know») вместо галлюцинации.13


2.2. Алгоритмы развертывания «Парашюта»: От генерации к навигации


Как только датчики фиксируют падение Confidence Score ниже критического уровня, архитектура должна сменить режим работы. Это и есть суть «Заблаговременной Амортизации»: не пытаться пробить стену лбом, а обойти её.


2.2.1. Переход к стратегии поиска (Adaptive RAG)


В системах Adaptive RAG используется классификатор сложности запроса (Query Complexity Classifier) или анализатор уверенности ретривера.14
Алгоритм реализации:
1. Оценка: Если уверенность в ответе из внутренней памяти (Parametric Knowledge) низкая:
2. Действие 1 (Fallback to Retrieval): Активируется поиск по векторной базе знаний.
3. Оценка: Если метрика сходства (similarity score) найденных документов низкая (например, < 0.5) или велик разрыв между топ-1 и топ-2 документами (low margin) 14:
4. Действие 2 (Web Search Extension): Система автоматически расширяет поиск на внешний интернет (Web Search), используя инструменты типа Tavily или Google Search.16
5. Действие 3 (Ask User): Если и веб-поиск не дает однозначного результата, агент переходит в режим диалога: «Я нашел противоречивые данные. Уточните, вас интересует контекст А или контекст Б?».
Этот каскадный подход (No Retrieval -> Single-step Retrieval -> Multi-step Retrieval -> Web Search -> Ask User) обеспечивает плавную деградацию функциональности без потери лица.18


2.2.2. Переход к задаванию вопросов (Clarification Protocols)


Вместо того чтобы гадать, агент использует шаблон Refusal Breaker.19
* Сценарий: Запрос пользователя амбигуэнтен.
* Стандартная реакция: Галлюцинация наиболее вероятного варианта.
* Амортизированная реакция: Генерация вопроса. «Чтобы дать точный ответ, мне нужно уточнить: вы имеете в виду Х или Y?». Это переводит ответственность за устранение неопределенности обратно на пользователя, сохраняя при этом полезность (helpfulness) агента.


2.3. Лингвистическое Хеджирование: Подушка безопасности из слов


Хеджирование (Hedging) — это лингвистическая стратегия снижения эпистемической ответственности. Это добавление модальных глаголов, вводных слов и конструкций, которые делают утверждение менее категоричным.5


2.3.1. Таксономия Хеджирования


Основываясь на работах Хайленда (Hyland) и анализе корпусов, мы выделяем три класса маркеров, которые должны внедряться в ответы агента динамически в зависимости от уровня уверенности:
Таблица 2.1. Классификация средств лингвистической амортизации


Категория (Hyland)
	Функция
	Примеры (En/Ru)
	Сценарий применения (Уровень уверенности)
	Downtoners (Ослабители)
	Снижение силы утверждения
	Probably, likely, possibly / Вероятно, возможно, скорее всего
	Medium (0.5 - 0.7). Модель видит паттерн, но данные не полны.
	Rounders (Округлители)
	Указание на приблизительность
	About, approximately, roughly / Около, порядка, приблизительно
	Работа с числовыми данными, когда источники расходятся в деталях.21
	Plausibility Shields (Щиты правдоподобия)
	Ссылка на логический вывод, а не факт
	Seems, appears, suggests / Судя по всему, данные предполагают, создается впечатление
	Low-Medium (0.3 - 0.5). Инференция на основе косвенных данных.
	Attribution Shields (Атрибутивные щиты)
	Перенос ответственности на источник
	According to X, Y claims / По данным источника Х, как утверждает Y
	Low. Конфликт источников или непроверенная информация.23
	

2.3.2. Автоматическое внедрение (Prompt Engineering & Post-processing)


Реализация хеджирования может быть выполнена двумя путями:
1. System Prompting: Инструкция модели в системном промпте.
   * Шаблон: «Если твоя уверенность в ответе не абсолютна, используй 'хеджирование' (слова: вероятно, возможно). Никогда не выдумывай факты. Если не знаешь — скажи, что данные предполагают X, но это требует проверки».24
2. Reranking/Rewriting: Использование второй модели (Verifier), которая оценивает категоричность ответа первой модели. Если категоричность высока, а фактологическая проверка (Fact-checking) дает низкий балл, вторая модель переписывает ответ, добавляя хеджирующие конструкции.22
Эксперименты показывают, что пользователи воспринимают хеджированные ответы как более «честные» и «человечные», что повышает долгосрочное доверие к системе, даже если конкретный ответ был неполным.26
________________


3. Часть II. Аэрокосмическая Метафора: Системы аварийного спасения и Резервирование


Принцип «Заблаговременной Амортизации» находит свое наиболее драматичное и эффективное воплощение в аэрокосмической отрасли. Анализ систем аварийного спасения (САС) ракет-носителей дает нам мощную метафору для проектирования архитектуры безопасности ИИ.


3.1. Метафора САС: Архитектуры «Puller» и «Pusher»


В ракетостроении выделяют два типа систем спасения экипажа, каждый из которых имеет прямой аналог в программной архитектуре.27
1. Puller (Тянущая схема):
* В космосе: Твердотопливная башня САС (как на кораблях «Союз» или Orion), расположенная над капсулой. При аварии она активируется мгновенно и «выдергивает» капсулу из опасной зоны. Это активная, агрессивная система, работающая на опережение.
* В ИИ (Прерывание): Это системы мониторинга в реальном времени (Streaming Monitors), которые анализируют поток токенов и обрывают соединение (abort) при обнаружении признаков «отравления» контекста или галлюцинации. Они не ждут конца ответа.
2. Pusher (Толкающая схема):
* В космосе: Двигатели интегрированы в сам корабль (как на Crew Dragon) и «отталкивают» его от ракеты.
* В ИИ (Валидация): Это пост-процессинг (Output Guardrails). Ответ генерируется целиком, затем проверяется валидатором, и только потом отдается (или блокируется) пользователю.
Сравнительный анализ и выбор для амортизации:
Традиционные системы безопасности ИИ часто работают по схеме Pusher (пост-валидация). Однако принцип «Заблаговременной Амортизации» требует перехода к схеме Puller. Почему? Потому что в диалоговых системах время реакции критично. Если модель начала генерировать токсичный контент или инструкцию по изготовлению бомбы, ожидание полной генерации (Pusher) неприемлемо. Необходима система, которая «выдернет» агента из диалога (Puller) до завершения вредоносной мысли.


3.2. «Красная кнопка»: Прерывание генерации (Circuit Breakers)


Реализация схемы «Puller» в программном коде требует паттерна Circuit Breaker (Автоматический выключатель) в сочетании с потоковым анализом.6


3.2.1. Streaming Content Monitoring (SCM)


Новейшие исследования 2025 года предлагают использовать Streaming Content Monitor (SCM).29 Это легковесная модель (например, дистиллированный BERT или классификатор), которая работает параллельно с основной LLM.
* Механизм Partial Detection: SCM анализирует не весь ответ, а буфер из первых $N$ токенов (например, первые 18% ответа).
* Двойное обучение: SCM обучается как на метках уровня всего ответа (response-level), так и на метках уровня токенов (token-level).
* Логика прерывания: Если накопленная вероятность вредоносного контента в буфере превышает порог (Threshold), SCM посылает сигнал STOP основному оркестратору.
Таблица 3.1. Сравнение Полной Детекции (Full) и Потоковой Детекции (Partial/Streaming)
Характеристика
	Full Detection (Pusher)
	Streaming/Partial Detection (Puller)
	Время реакции
	Высокое (после генерации всего текста)
	Низкое (в процессе генерации)
	Влияние на UX
	Задержка перед ответом (latency)
	Ответ идет сразу, но может оборваться
	Безопасность
	Риск утечки при стриминге (если не буферизовать)
	Высокая (прерывание атаки)
	Ресурс
	Требует полной генерации токенов (дорого)
	Экономит токены (ранняя остановка)
	Применимость
	Финальные отчеты, письма
	Чат-боты, голосовые помощники
	

3.2.2. Реализация в NeMo Guardrails и Guardrails AI


Фреймворки типа NVIDIA NeMo Guardrails реализуют этот подход через асинхронную валидацию чанков.31
* Streaming Mode: В режиме стриминга NeMo буферизует токены в небольшие чанки. Валидаторы (Rails) проверяют чанк до того, как он будет отправлен пользователю. Если чанк нарушает политику (например, содержит PII или галлюцинацию), он блокируется, и генерируется заранее подготовленное сообщение об отказе (Canned Response).
* Circuit Breaker Pattern: В библиотеке Portkey и других инструментах реализован паттерн Circuit Breaker, который отключает доступ к конкретной модели или провайдеру, если частота ошибок или тайм-аутов превышает норму, переключаясь на резервный (Fallback) канал.32


3.3. Принцип амортизации удара: UX отказов (Graceful Degradation)


Когда «Красная кнопка» нажата, система переходит в аварийный режим. В аэрокосмической отрасли перегрузки при срабатывании САС могут достигать 15-20g. Задача инженеров — сделать так, чтобы экипаж выжил. В UX задача — сохранить лояльность пользователя.
Для этого используется принцип Graceful Degradation (Постепенная/Изящная деградация).33
Стратегии смягчения реакции (Positive Refusal Strategies):
1. Positive Refusal (Позитивный отказ):
   * Суть: Отказ должен содержать не только «НЕТ», но и «ЧТО ВМЕСТО».
   * Паттерн: «Я не могу выполнить действие X (причина), но я могу предложить Y или Z».
   * Пример: «Я не могу предоставить медицинскую консультацию, так как я ИИ, но я могу найти список ближайших клиник или статьи на PubMed по вашим симптомам».9
2. Прозрачность ограничений (Transparency):
   * Пользователи более склонны прощать ошибки, если понимают их причину. Фраза «Из-за конфликта в источниках данных я не могу дать точный ответ» воспринимается лучше, чем молчаливое игнорирование или ложный ответ.36
3. Восстановление контроля (User Control):
   * Предоставление пользователю возможности «переопределить» решение или изменить стратегию. Например, кнопка «Все равно искать» (с предупреждением о рисках) или «Уточнить запрос».37
________________


4. Часть III. Компенсация ненадежности источников: Подушка безопасности знаний


Третий уровень реализации принципа ТРИЗ №11 касается работы с данными. Если база знаний (Knowledge Base) содержит противоречия, устаревшие данные или шум, агент должен «подстелить соломку», чтобы не выдать пользователю некорректный синтез.


4.1. Управление конфликтами в RAG (Conflict Handling)


В RAG-системах источники часто противоречат друг другу. Источник А может говорить, что «Продукт Х поддерживает функцию Y», а Источник Б (более новый) — что «Функция Y удалена в версии 2.0». Слепой выбор (random selection) или усреднение здесь недопустимы.


4.1.1. Типология конфликтов


Для эффективной амортизации необходимо классифицировать тип конфликта 39:
Таблица 4.1. Типология конфликтов знаний и стратегии амортизации


Тип конфликта
	Описание
	Стратегия амортизации
	Freshness Conflict (Конфликт свежести)
	Данные отличаются временной меткой (2020 vs 2024).
	Приоритет новизны: Использовать более новый источник, но явно указать, что старый источник содержит иную информацию (для контекста).
	Opinion Conflict (Конфликт мнений)
	Разные экспертные оценки или субъективные суждения.
	Multi-Viewpoint Summarization: Представить обе точки зрения как равноправные гипотезы.40
	Factual Conflict (Фактический конфликт)
	Прямое противоречие в фактах (числа, имена).
	Cross-Check & Hedge: Искать третий источник (Web Search) для верификации. Если невозможно — выдать оба значения с высокой степенью хеджирования.
	Misinformation (Дезинформация)
	Один из источников содержит ложь или галлюцинацию.
	Reputation Filtering: Использовать метаданные о надежности источника. Блокировать ненадежный источник.
	

4.2. Мультиагентные Дебаты (Multi-Agent Debate)


Одним из самых мощных методов амортизации конфликтов является подход MAD (Multi-Agent Debate) или Madam-RAG.42
Механизм действия:
Вместо того чтобы один агент пытался разрешить конфликт внутри себя (что часто приводит к галлюцинациям), система разворачивает виртуальные «дебаты»:
1. Агент А получает задачу защищать точку зрения Источника 1.
2. Агент Б защищает точку зрения Источника 2.
3. Агент-Судья (Aggregator) анализирует аргументы обеих сторон.
Амортизационный эффект:
В процессе дебатов агенты выявляют слабые места в аргументации друг друга. Агент-Судья, видя это, не выбирает победителя наугад, а формирует синтезированный ответ, который отражает сложность ситуации. Например: «Хотя Источник 1 утверждает X, анализ показывает, что это данные 2021 года. Источник 2 опровергает это, ссылаясь на новые тесты. Рекомендуется опираться на Источник 2, но учитывать риск...».
Этот подход позволяет превратить противоречие из «ошибки системы» в «экспертную аналитику», значительно повышая ценность ответа для пользователя. Исследования показывают, что Madam-RAG улучшает точность на сложных датасетах (AmbigDocs) на 11.40%.43


4.3. Синтез противоречий: Шаблоны Промптов


Если использование мультиагентной системы слишком ресурсоемко, можно использовать специализированные шаблоны промптов (Synthesis Prompts), которые инструктируют модель обрабатывать конфликты явно.45
Пример структуры промпта для амортизации конфликтов:
"Ты — аналитический эксперт. Тебе предоставлены фрагменты текста, которые могут содержать противоречивую информацию.
Инструкция по амортизации:
1. Сначала выяви все противоречия между источниками (даты, факты, мнения).
2. НЕ ПЫТАЙСЯ сгладить противоречия или выбрать одну сторону без явных доказательств (например, более поздней даты).
3. В финальном ответе явно перечисли расхождения. Используй фразы: «Источники расходятся во мнениях...», «Документ А утверждает Х, в то время как Документ Б указывает Y».
4. Оцени надежность источников, если доступны метаданные.
Контекст: {context}
Вопрос: {query}"
Такой промпт заставляет модель переходить в режим «мета-анализа», создавая защитный слой между пользователем и сырыми, противоречивыми данными.
________________


5. Часть IV. Системная Архитектура и Реализация


Интеграция описанных принципов требует перехода от линейных цепочек (Chains) к циклическим графам (Graphs). Использование фреймворков типа LangGraph позволяет реализовать сложные сценарии ветвления и возврата, необходимые для амортизации.48


5.1. Интегрированная Схема Отказоустойчивого Агента


Предлагаемая архитектура состоит из 5 ключевых узлов, каждый из которых реализует определенный аспект ТРИЗ №11.
Узлы графа:
1. Input Guard (Входной шлюз):
   * Функция: Проверка на Prompt Injection и безопасность.
   * Метод: NeMo Guardrails / Llama Guard.
   * Амортизация: Если атака — мягкий отказ (Canned Response).
2. Adaptive Retrieval (Адаптивный поиск):
   * Функция: Определение стратегии поиска.
   * Метод: Query Complexity Classifier.
   * Амортизация: Если запрос сложный — разбиение на подзадачи (Decomposition) или Web Search.15
3. Conflict & Confidence Analyzer (Анализатор):
   * Функция: Оценка retrieved documents и генерация черновика ответа.
   * Метод: Self-Consistency, Logprobs Entropy, Conflict Detection.
   * Триггер: Если Confidence < Threshold или Conflict == True -> переход в ветку Cushioning.
4. Cushioning / Fallback Node (Узел Амортизации):
   * Функция: Обработка неудачи.
   * Стратегии:
      * Ask User: «Уточните контекст».
      * Debate: Запуск Madam-RAG для разрешения конфликта.
      * Hedged Generation: Переписывание ответа с модальными глаголами.
5. Streaming Circuit Breaker (Выходной предохранитель):
   * Функция: Мониторинг генерации.
   * Метод: Partial Detection Model.
   * Действие: Прерывание потока при детекции галлюцинации/токсичности.


5.2. Псевдокод логики маршрутизации (Python/LangGraph)




Python




# Концептуальная схема реализации узла маршрутизации
def route_after_retrieval(state):
   """
   Принимает состояние агента после поиска документов.
   Решает: генерировать ответ, уточнять у пользователя или запускать дебаты.
   """
   docs = state["documents"]
   query_complexity = state["complexity"]
   
   # 1. Проверка на конфликты (Knowledge Conflict)
   conflict_score = detect_conflicts(docs)
   if conflict_score > HIGH_CONFLICT_THRESHOLD:
       return "multi_agent_debate_node" # Запуск дебатов для разрешения
       
   # 2. Проверка достаточности информации (Confidence)
   relevance_scores = evaluate_relevance(docs, state["query"])
   if max(relevance_scores) < LOW_CONFIDENCE_THRESHOLD:
       if state["attempt_count"] < MAX_RETRIES:
            return "web_search_fallback" # Расширение поиска
       else:
            return "ask_user_clarification" # Сдаемся и спрашиваем пользователя (Мягкая посадка)
            
   # 3. Штатный режим
   return "generate_answer_node"

# Узел генерации с хеджированием
def generate_answer_node(state):
   # Если уверенность средняя, инструктируем использовать хеджирование
   if state["confidence"] == "MEDIUM":
       system_prompt = HEDGING_PROMPT_TEMPLATE
   else:
       system_prompt = STANDARD_PROMPT
       
   response = llm.invoke(system_prompt, state["query"])
   return {"response": response}



5.3. Компенсаторные механизмы интерфейса (UI Patterns)


Архитектура бэкенда должна быть поддержана соответствующими UI-паттернами 33:
1. Confidence Indicators (Индикаторы уверенности): Визуальные маркеры (цветные полоски, иконки), показывающие, насколько ИИ уверен в ответе. Это управляет ожиданиями пользователя.
2. Citations & Sources (Цитаты): Кликабельные ссылки на источники. В случае конфликта — выделение противоречащих фрагментов.
3. Flight Strip View: Отображение «хода мыслей» агента (например, «Ищу в базе... Не нашел... Ищу в интернете... Нашел противоречие... Анализирую»). Это делает процесс «амортизации» прозрачным и понятным, снижая фрустрацию от задержки.
________________


6. Заключение: Будущее «Честного ИИ»


Интеграция принципа ТРИЗ №11 «Заблаговременная Амортизация» в архитектуру ИИ-агентов знаменует собой переход от наивного оптимизма («ИИ знает всё») к инженерному реализму («ИИ — это вероятностная система, подверженная сбоям»).
Проведенное исследование показывает, что создание отказоустойчивого агента требует многоуровневой системы защиты:
1. Сенсоры (Logprobs, Entropy): Для обнаружения момента потери устойчивости.
2. Парашюты (Adaptive RAG, Hedging): Для мягкого приземления в зоне неопределенности.
3. Системы спасения (Circuit Breakers): Для экстренного прерывания опасных сценариев.
4. Подушки безопасности (Conflict Synthesis): Для защиты от жестких противоречий в данных.
Такой подход позволяет создать систему, которая может «не знать» ответа, но никогда не «врет» и не вводит пользователя в заблуждение. В долгосрочной перспективе именно эта «эпистемическая честность», обеспеченная архитектурной амортизацией, станет главным фактором доверия к автономным агентам в критически важных сферах — от медицины до юриспруденции. Мы переходим от эры Artificial Intelligence к эре Reliable Intelligence, где способность системы грамотно обрабатывать свои ошибки ценится выше, чем способность иногда выдавать гениальные ответы.
Источники
1. MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty - ACL Anthology, дата последнего обращения: ноября 25, 2025, https://aclanthology.org/2025.findings-naacl.325.pdf
2. 40 TRIZ Principles, дата последнего обращения: ноября 25, 2025, https://www.triz40.com/aff_Principles_TRIZ.php
3. TRIZ and Software - 40 Principle Analogies, Part 1, дата последнего обращения: ноября 25, 2025, https://novalis.org/triz-talk/softwarearticle.html
4. The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2511.02309v1
5. Epistemic Modality Markers as Hedges in Research Articles. A Cross-Disciplinary Study - CORE, дата последнего обращения: ноября 25, 2025, https://core.ac.uk/download/pdf/16363623.pdf
6. Circuit Breaker Pattern - Azure Architecture Center | Microsoft Learn, дата последнего обращения: ноября 25, 2025, https://learn.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker
7. DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2510.19669v1
8. AI Stigma: Why Some Users Resist AI's Help - UX Tigers, дата последнего обращения: ноября 25, 2025, https://www.uxtigers.com/post/ai-stigma
9. 6 UX Design Tips to Make AI Trustworthy and Easier to Use - Salesforce, дата последнего обращения: ноября 25, 2025, https://www.salesforce.com/blog/6-ux-design-tips-trust-ai/
10. Logprobs Know Uncertainty: Fighting LLM Hallucinations - ResearchGate, дата последнего обращения: ноября 25, 2025, https://www.researchgate.net/publication/394078106_Logprobs_Know_Uncertainty_Fighting_LLM_Hallucinations
11. The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute - OpenReview, дата последнего обращения: ноября 25, 2025, https://openreview.net/pdf?id=uaUYx9DMDl
12. Mitigating LLM Hallucinations via Conformal Abstention - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2405.01563v1
13. [2405.01563] Mitigating LLM Hallucinations via Conformal Abstention - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/abs/2405.01563
14. RAG Series – Adaptive RAG, understanding Confidence, Precision & nDCG - dbi services, дата последнего обращения: ноября 25, 2025, https://www.dbi-services.com/blog/rag-series-adaptive-rag-understanding-confidence-precision-ndcg/
15. Adaptive RAG: The Ultimate Guide to Dynamic Retrieval-Augmented Generation, дата последнего обращения: ноября 25, 2025, https://www.machinelearningplus.com/gen-ai/adaptive-rag-ultimate-guide-to-dynamic-retrieval-augmented-generation/
16. How to add LLM Fallback to your LangChain Application - DigitalOcean, дата последнего обращения: ноября 25, 2025, https://www.digitalocean.com/community/tutorials/langchain-llm-fallback-gradient-ai
17. A tutorial on building local agent using LangGraph, LLaMA3 and Elasticsearch vector store from scratch, дата последнего обращения: ноября 25, 2025, https://www.elastic.co/search-labs/blog/local-rag-agent-elasticsearch-langgraph-llama3
18. Adaptive RAG: Elevating Query Handling with Smarter Retrieval and Generation - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@sumakbn/adaptive-rag-elevating-query-handling-with-smarter-retrieval-and-generation-f5853f5e6ff1
19. Prompt Engineering via Prompt Patterns — Refusal Breaker Pattern | by Ali Aslam | Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@a1guy/prompt-engineering-via-prompt-patterns-refusal-breaker-pattern-0abcc18f2898
20. Prompt Engineering - Refusal Breaker Pattern | GenAI | ChatGPT - Debabrata Pruseth, дата последнего обращения: ноября 25, 2025, https://debabratapruseth.com/prompt-engineering-refusal-breaker-pattern/
21. “We find that…” changing patterns of epistemic positioning in research writing - PMC - NIH, дата последнего обращения: ноября 25, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC12513368/
22. How About Kind of Generating Hedges using End-to-End Neural Models? - ACL Anthology, дата последнего обращения: ноября 25, 2025, https://aclanthology.org/2023.acl-long.50.pdf
23. Hedging Techniques in Academic Writing with Examples, дата последнего обращения: ноября 25, 2025, https://www.ref-n-write.com/blog/hedging-techniques-in-academic-writing-with-examples/
24. How do I design a good system message? - Cogniti, дата последнего обращения: ноября 25, 2025, https://cogniti.ai/docs/how-do-i-design-a-good-system-message/
25. Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement - Johns Hopkins Computer Science, дата последнего обращения: ноября 25, 2025, https://www.cs.jhu.edu/~aadelucia/assets/research/confidence_estimation_TrustNLP2023.pdf
26. I'm getting good results with "overexplain" and "hedging language" - Prompting, дата последнего обращения: ноября 25, 2025, https://community.openai.com/t/im-getting-good-results-with-overexplain-and-hedging-language/564163
27. Conceptual Study of Launch Abort System and Capabilities for the Nyx Earth Crew Spacecraft - DiVA portal, дата последнего обращения: ноября 25, 2025, https://www.diva-portal.org/smash/get/diva2:1900954/FULLTEXT01.pdf
28. How Orion's Launch Abort System Protects Astronauts | Lockheed Martin, дата последнего обращения: ноября 25, 2025, https://www.lockheedmartin.com/en-us/news/features/2025/how-orion-s-launch-abort-system-protects-astronauts.html
29. From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2506.09996v1
30. (PDF) From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring - ResearchGate, дата последнего обращения: ноября 25, 2025, https://www.researchgate.net/publication/392597727_From_Judgment_to_Interference_Early_Stopping_LLM_Harmful_Outputs_via_Streaming_Content_Monitoring
31. Stream Smarter and Safer: Learn how NVIDIA NeMo Guardrails Enhance LLM Output Streaming | NVIDIA Technical Blog, дата последнего обращения: ноября 25, 2025, https://developer.nvidia.com/blog/stream-smarter-and-safer-learn-how-nvidia-nemo-guardrails-enhance-llm-output-streaming/
32. Retries, fallbacks, and circuit breakers in LLM apps: what to use when - Portkey, дата последнего обращения: ноября 25, 2025, https://portkey.ai/blog/retries-fallbacks-and-circuit-breakers-in-llm-apps/
33. Slow AI: Designing User Control for Long Tasks - UX Tigers, дата последнего обращения: ноября 25, 2025, https://www.uxtigers.com/post/slow-ai
34. 7 Essential UX Design Principles for AI Applications - AKY X, дата последнего обращения: ноября 25, 2025, https://akyx.digital/blog/7-essential-ux-design-principles-for-ai-applications
35. AI-driven UX patterns vs. traditional UX | Standard Beagle Studio, дата последнего обращения: ноября 25, 2025, https://standardbeagle.com/ai-driven-ux-patterns-vs-traditional-ux/
36. How AI Models Rank Conflicting Information: What Wins in a Tie?, дата последнего обращения: ноября 25, 2025, https://www.covert.com.au/how-ai-models-rank-conflicting-information-what-wins-in-a-tie/
37. 20+ GenAI UX patterns, examples and implementation tactics | by Sharang Sharma, дата последнего обращения: ноября 25, 2025, https://uxdesign.cc/20-genai-ux-patterns-examples-and-implementation-tactics-5b1868b7d4a1
38. Designing UX for AI Errors: How to Handle Failures the Right Way - f1Studioz, дата последнего обращения: ноября 25, 2025, https://f1studioz.com/blog/designing-ux-for-ai-errors-how-to-handle-failures-the-right-way/
39. DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2506.08500v2
40. Master LLM Summarization Strategies and their Implementations - Galileo AI, дата последнего обращения: ноября 25, 2025, https://galileo.ai/blog/llm-summarization-strategies
41. Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2505.21859v1
42. Retrieval-Augmented Generation with Conflicting Evidence - OpenReview, дата последнего обращения: ноября 25, 2025, https://openreview.net/forum?id=z1MHB2m3V9
43. [2504.13079] Retrieval-Augmented Generation with Conflicting Evidence - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/abs/2504.13079
44. Retrieval-Augmented Generation with Conflicting Evidence - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2504.13079v1
45. What strategies can improve the coherence of a RAG answer if the retrieved passages are from different sources or have different writing styles (the “frankenstein” answer problem)? - Milvus, дата последнего обращения: ноября 25, 2025, https://milvus.io/ai-quick-reference/what-strategies-can-improve-the-coherence-of-a-rag-answer-if-the-retrieved-passages-are-from-different-sources-or-have-different-writing-styles-the-frankenstein-answer-problem
46. How can the prompt be designed to handle contradictory information in retrieved documents (for example, guiding the model on how to reconcile conflicts)? - Milvus, дата последнего обращения: ноября 25, 2025, https://milvus.io/ai-quick-reference/how-can-the-prompt-be-designed-to-handle-contradictory-information-in-retrieved-documents-for-example-guiding-the-model-on-how-to-reconcile-conflicts
47. The Art and Science of RAG: Mastering Prompt Templates and Contextual Understanding | by Ajay Verma | Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@ajayverma23/the-art-and-science-of-rag-mastering-prompt-templates-and-contextual-understanding-a47961a57e27
48. Dynamic Routing in LangGraph - Xavier Collantes, дата последнего обращения: ноября 25, 2025, https://xaviercollantes.dev/articles/langgraph-paths
49. Building AI Workflows with LangGraph: Practical Use Cases and Examples - Scalable Path, дата последнего обращения: ноября 25, 2025, https://www.scalablepath.com/machine-learning/langgraph