ПРОТОКОЛ ЭКСТРАКЦИИ: Применение Принципа ТРИЗ №2 «Вынесение» для архитектурной оптимизации автономных ИИ-агентов




Аннотация


В данном отчете представлен исчерпывающий анализ применения второго принципа Теории Решения Изобретательских Задач (ТРИЗ) — принципа «Вынесение» (Taking Out / Extraction) — к проектированию и оптимизации архитектуры современных ИИ-агентов. В условиях экспоненциального роста сложности больших языковых моделей (LLM) и агентных систем, инженерные вызовы смещаются от наращивания мощности к управлению шумом, контекстом и автономностью. Исследование рассматривает три ключевых вектора: техническую реализацию «Чистого Сигнала» через вынесение галлюцинаций (методология Chain of Verification), применение аэрокосмических метафор для управления контекстным окном (механизм «Drop Tanks» и алгоритмы эвикции KV-кэша), а также разрешение фундаментальных противоречий между жесткостью инструкций и гибкостью генерации посредством Конституционного ИИ и семантической маршрутизации. Цель работы — сформулировать инженерный протокол, при котором любые вредные факторы (ошибки, лишний текст, предвзятость, устаревший контекст) изолируются и удаляются из системы до момента взаимодействия с пользователем.
________________


1. Введение: Инженерия вычитания в эпоху генеративного ИИ


Современная парадигма разработки искусственного интеллекта переживает фундаментальный сдвиг. Если последнее десятилетие было посвящено «инженерии добавления» — увеличению количества параметров, объема обучающих данных и длины контекстного окна, — то текущий этап требует перехода к «инженерии вычитания». Монолитные архитектуры, где одна нейросеть пытается одновременно удерживать контекст, генерировать креативный контент, проверять факты и соблюдать этические нормы, сталкиваются с неустранимыми противоречиями. Эти противоречия проявляются в виде галлюцинаций, переполнения контекста («Context Overflow») и нестабильности поведения агентов.1
Для решения этих системных проблем мы обращаемся к классической инженерной методологии — ТРИЗ (Теория Решения Изобретательских Задач), разработанной Генрихом Альтшуллером. Особый интерес представляет Принцип №2: «Вынесение» (Taking Out / Extraction). Согласно каноническому определению, этот принцип предписывает «отделить от объекта мешающую часть или свойство, или выделить единственно нужную часть или свойство».3
В классической инженерии это может означать вынесение шумного компрессора за пределы рабочего помещения или использование оптоволокна для отделения источника света от термочувствительной зоны.4 В контексте программной архитектуры ИИ-агентов принцип «Вынесения» трансформируется в стратегию архитектурной декомпозиции, где вредные свойства (недетерминированность, галлюцинации, устаревший контекст) физически изолируются от полезного сигнала.
В данном отчете мы исследуем, как этот принцип позволяет перейти от стохастической генерации к детерминированным инженерным системам, анализируя методы пост-обработки, управление памятью и уровни автономности.
________________


2. Архитектура «Чистого Сигнала»: Техническая реализация вынесения галлюцинаций


Первой и наиболее критической областью применения принципа «Вынесение» является борьба с галлюцинациями — генерацией правдоподобной, но ложной информации. Галлюцинации являются неотъемлемым свойством вероятностной природы трансформеров, которые оптимизированы для предсказания следующего токена, а не для верификации фактов.1 Принцип ТРИЗ №2 диктует необходимость вынесения процесса проверки истинности из процесса генерации текста.


2.1. Метод Chain of Verification (CoVe): Вынесение верификации в скрытый слой


Методология Chain of Verification (CoVe) представляет собой прямую реализацию принципа вынесения. Она решает фундаментальную проблему: когда модель генерирует ответ и проверяет его в рамках одного прохода (Joint Generation), она склонна подтверждать собственные ошибки из-за эффекта «предвзятости подтверждения».7


2.1.1. Деконструкция процесса CoVe


Согласно исследованиям, эффективная реализация CoVe требует разбиения монолитного процесса генерации на четыре дискретных этапа, где верификация выносится в отдельный, скрытый от пользователя цикл 9:
1. Генерация базового ответа (Baseline Generation): Агент формирует черновой вариант ответа. На этом этапе допускается наличие «шума» (галлюцинаций), так как этот текст не является финальным продуктом для пользователя. Это «сырая руда», требующая обогащения.
2. Планирование верификации (Planning Verifications): Система (или отдельный агент-критик) анализирует черновик и генерирует список проверочных вопросов. Важно, что на этом этапе система не пытается отвечать на вопросы, а только формулирует их (например, «Действительно ли политик X родился в Нью-Йорке?»). Это вынесение вопрошания из ответа.8
3. Исполнение верификации (Verification Execution): Это критический этап экстракции. Ответы на проверочные вопросы генерируются независимо от базового текста. Исследования показывают, что метод Factored Verification (факторная верификация), когда каждый вопрос обрабатывается в изолированном контексте, значительно эффективнее метода Joint (совместного), так как исключает влияние контекста первоначальной ошибки на проверку.8
4. Финальная генерация (Final Refined Response): Полученные проверенные факты используются для переписывания ответа. Любая информация из базового черновика, которая противоречит верифицированным данным, физически удаляется («выносится») из финального вывода.


2.1.2. Сравнительный анализ методов реализации


Существует несколько архитектурных паттернов реализации CoVe, различающихся степенью изоляции (вынесения) проверочного контура 7:
Метод
	Описание
	Степень «Вынесения»
	Риск галлюцинаций
	Joint
	Планирование и проверка происходят в одном промпте с генерацией.
	Низкая (все в одном контексте)
	Высокий (Bias)
	2-Step
	Генерация и проверка разделены на два последовательных вызова LLM.
	Средняя
	Средний
	Factored
	Каждый проверочный вопрос обрабатывается отдельным, независимым вызовом модели.
	Высокая (полная изоляция)
	Низкий
	Factored + Revise
	Добавляется явный этап перекрестной сверки перед финальной сборкой.
	Максимальная
	Минимальный
	Технически, блок «Антигаллюцинация» (уровни 1-3 в модели зрелости агента) функционирует не как часть генератора, а как фильтр-ревизор. Он физически отсекает непроверенные данные. Если верификация не подтверждает факт, он не просто маркируется, а удаляется из финального ответа, реализуя принцип «Only the necessary part» (только нужная часть).3


2.2. Пост-обработка и архитектурные Guardrails: Фильтры на выходе


Помимо алгоритмического вынесения внутри логики агента (CoVe), принцип №2 реализуется через внешние архитектурные компоненты — Guardrails (ограждения). Эти системы действуют как шлюзы, которые инспектируют выходной сигнал модели перед его доставкой пользователю.1


2.2.1. NeMo Guardrails и Llama Guard


Системы типа NVIDIA NeMo Guardrails или Meta Llama Guard представляют собой отдельные программные слои, которые «выносят» функцию безопасности из самого LLM.10
* Входная и Выходная фильтрация (Input vs. Output Filtering): Исследования Llama Guard показывают, что фильтрация может применяться как к промпту пользователя (Input), так и к ответу модели (Output). Внутренние тесты указывают, что входная фильтрация часто дает более высокий уровень отказов (refusal rate), однако выходная фильтрация позволяет модели попытаться сгенерировать безопасный ответ, и только в случае неудачи «вынести» опасную часть.11
* Механизм действия: Если детектор галлюцинаций (например, основанный на RAG-сверке или AlignScore) обнаруживает несоответствие фактам, система может либо полностью заблокировать ответ, либо запустить процедуру Rephrasing (перефразирования), удаляя только галлюцинированный фрагмент.1 Это напоминает процесс очистки нефти, где вредные примеси удаляются на этапе рафинации.


2.2.2. Детекция через внимание (Attention-Based Detection)


Более глубокий уровень экстракции включает анализ внутренних весов внимания модели. Модели детекции могут анализировать распределение вероятностей токенов (logprobs) и энтропию внимания. Если модель «сомневается» (высокая энтропия) но генерирует утвердительный факт, это маркер галлюцинации. Такие сегменты текста могут быть автоматически «вынесены» или помечены как ненадежные еще до того, как они попадут в финальный буфер ответа.6


2.3. Агент-Критик в мультиагентных системах (LangGraph)


В рамках сложных оркестраторов, таких как LangGraph, принцип вынесения реализуется через паттерн Reflexion (рефлексия). Здесь функция критики выносится в отдельного автономного агента.15
* Изоляция ролей: Агент-Генератор (Generator) создает контент, но не имеет права отправить его пользователю. Его выход перенаправляется Агенту-Критику (Critic).
* Цикл ревизии: Критик проверяет текст на наличие логических ошибок, галлюцинаций или лишней информации. Если обнаружены дефекты, он возвращает текст Генератору с инструкциями по исправлению.
* Результат: Пользователь видит только результат, прошедший через этот фильтр. Весь процесс спора и исправления ошибок (который может занимать несколько итераций) скрыт. Это решает проблему «Амнезийного супер-стажера» (Amnesiac Super-Intern), когда мощная модель без контроля генерирует бред.17
________________


3. Аэрокосмическая метафора: «Drop Tanks» и управление контекстом


Вторая грань применения принципа «Вынесение» касается управления ресурсами памяти и вниманием агента. Здесь уместна прямая аналогия с аэрокосмической инженерией, в частности, с концепцией сбрасываемых топливных баков (Drop Tanks).18


3.1. Метафора «Drop Tanks»: Сброс отработанного контекста


В истребительной авиации подвесные топливные баки обеспечивают дальность полета на этапе сближения с целью, но перед вступлением в маневренный бой они сбрасываются, чтобы уменьшить массу и лобовое сопротивление. В архитектуре ИИ-агентов контекст — это топливо, но старый контекст — это балласт.


3.1.1. Проблема переполнения и «Context Poisoning»


LLM имеют ограниченное контекстное окно (аналог объема оперативной памяти). По мере накопления истории диалога, производительность модели падает из-за квадратичной сложности механизма внимания (Attention). Более того, возникает феномен «Context Poisoning» (отравление контекста) и «Lost in the Middle» (потеря в середине), когда критически важные инструкции тонут в массиве старых данных.2 Агент начинает «тормозить» и терять фокус, подобно перегруженному самолету.


3.1.2. Техническая реализация сброса: Ephemeral Memory


Принцип ТРИЗ №2 требует вынесения отработанной информации из активного контекста. В архитектурах типа LangGraph это реализуется через механизмы управления состоянием (State Management).20
* Scoped Memory (Область видимости памяти): Память агента разделяется на долговременную (Persistent, хранит факты о пользователе) и эфемерную (Short-term, хранит текущий диалог).
* Протокол сброса (Reset Pattern): По завершении логического этапа задачи (например, «Сбор информации завершен, переходим к написанию отчета»), агент должен инициировать процедуру reset_memory или delete_state для эфемерной памяти.
   * Алгоритм:
      1. Агент выполняет задачу А.
      2. Агент генерирует сжатое резюме (Summary) результатов задачи А.
      3. Резюме сохраняется в долговременную память или передается в следующий узел графа.
      4. Сырой лог диалога по задаче А (Drop Tank) удаляется из контекста.22
   * Это обеспечивает «легкость» и маневренность агента для следующей задачи, исключая интерференцию старых данных с новыми инструкциями.


3.2. Алгоритмическое вынесение: H2O и Attention Sinks


На уровне инференса модели (ниже уровня промптов) принцип вынесения реализуется через специализированные алгоритмы эвикции KV-кэша (Key-Value Cache Eviction).25


3.2.1. Heavy Hitter Oracle (H2O)


Алгоритм H2O (Heavy Hitter Oracle) основан на наблюдении, что лишь малая часть токенов (около 20%) вносит основной вклад в формирование механизма внимания. Эти токены называются «Heavy Hitters».
* Принцип действия: Алгоритм динамически отслеживает накопленные оценки внимания для каждого токена. Токены, которые редко используются (малый вклад в внимание), физически удаляются («выносятся») из KV-кэша в реальном времени.
* Результат: Это позволяет сократить объем потребляемой памяти до 5-кратного размера без существенной потери качества генерации. Это «умный сброс баков», где сбрасывается только пустая оболочка, а топливо (смысл) остается.25


3.2.2. StreamingLLM и Attention Sinks


При попытке просто удалить старые токены (метод скользящего окна), модели часто теряют когерентность (perplexity explosion). Исследование StreamingLLM выявило причину: первые несколько токенов предложения действуют как «Attention Sinks» (стоки внимания) — они собирают на себя избыточное внимание, даже если не несут смысла.28
* Решение ТРИЗ: Мы «выносим» середину разговора, но оставляем начальные токены (Attention Sinks) и последние токены (Recent Window).
* Метафора: Это похоже на сохранение фундамента здания (Sinks) и строящегося этажа (Recent), при этом снося средние этажи, но сохраняя лифтовую шахту связи. Это позволяет модели работать с бесконечными потоками данных, не переполняя память и не теряя структурной целостности.30


3.3. Вынесение пилота: От второго пилота к невидимому инструменту


Аэрокосмическая метафора продолжается в эволюции роли человека. Переход от пилотируемой авиации к БПЛА (UAV) соответствует переходу от чат-ботов к автономным агентам. Принцип №2 здесь применяется к интерфейсу взаимодействия.32


3.3.1. Уровни автономности и Zero UI


В концепции Zero UI (Нулевой интерфейс) идеальный агент — это невидимый агент.
* Copilot (Уровень 2): Человек и ИИ в одной кабине. ИИ — «второй пилот», требует постоянного диалога. Интерфейс перегружен («чат»).
* Invisible Agent (Уровень 4-5): Человек вынесен из контура управления («Human-out-of-the-loop»). Агент действует в фоновом режиме, реагируя на контекст, а не на команды.34
* Пример — Ghost Text: В GitHub Copilot взаимодействие вынесено из чата непосредственно в редактор кода в виде «призрачного текста» (ghost text). Диалог как таковой отсутствует (вынесен), остается только функция (дополнение кода). Это реализация принципа «выделить единственно нужную часть» (код), убрав «мешающую часть» (необходимость переключаться в чат и формулировать запрос).36


3.3.2. Ambient Intelligence (Окружающий интеллект)


Дальнейшее развитие принципа приводит к Ambient Intelligence. Агент растворяется в среде, становясь функцией самой среды (умный дом, умная IDE). Взаимодействие становится неявным. Вместо «Напиши письмо коллеге», агент, видя черновик и контекст календаря, предлагает готовый текст. Личность агента («Я помощник...») выносится за скобки, оставляя чистую полезность.38
________________


4. Разрешение противоречий через Вынесение: Жесткость vs Гибкость


Центральным противоречием в проектировании ИИ-систем является конфликт между требованиями безопасности (жесткость) и качеством взаимодействия (гибкость, человечность).
* Противоречие: «Агент должен быть строгим (чтобы не нарушать правила) И агент должен быть гибким (чтобы поддерживать естественный диалог)».
* Решение по ТРИЗ: Разделение противоречивых свойств в пространстве или структуре. Мы выносим жесткость в скрытый слой, оставляя гибкость в видимом слое.


4.1. Разделение System Prompt и User Prompt


Современные архитектуры LLM (Claude, GPT-4) нативно поддерживают это разделение через механизм System Prompt (Системный промпт).40
* Скрытый слой (System Prompt): Здесь размещаются «жесткие» инструкции: «Ты — эксперт по Python», «Отвечай только в формате JSON», «Не используй ненормативную лексику». Этот слой действует как BIOS или прошивка. Он невидим для конечного пользователя, но жестко контролирует поведение модели.42
* Видимый слой (User Prompt): Здесь происходит гибкое взаимодействие. Пользователь может общаться естественно, даже небрежно.
* Эффект: Благодаря вынесению правил в системный слой, модель не нуждается в постоянном напоминании о них в ходе диалога. Это решает проблему «замусоривания» контекста правилами и создает иллюзию естественности, при этом сохраняя строгий контроль.43


4.2. Конституционный ИИ (Constitutional AI): Вынесение человека из контура обучения


Компания Anthropic применила принцип вынесения для решения проблемы масштабируемости надзора за безопасностью. Традиционный метод RLHF (Reinforcement Learning from Human Feedback) требует огромного количества человеко-часов для разметки.
* Проблема: Человеческий ресурс ограничен и дорог (мешающий фактор масштабирования).
* Решение (RLAIF): Человек выносится из процесса оценки каждого ответа. Вместо него вводится Конституция — набор явных принципов (вынесенных правил), и модель сама оценивает свои ответы на соответствие этой конституции (AI Feedback).44
* Процесс:
   1. Модель генерирует ответ.
   2. Модель критикует себя (Self-Critique), сверяясь с Конституцией (например, «Выбери ответ, который наиболее полезен и наименее вреден»).
   3. Модель исправляет ответ.
   4. Модель обучается на этих исправленных данных (Fine-tuning).
Это позволяет создать агента, который «интернализировал» жесткие правила безопасности, но применяет их гибко, без постоянного внешнего надзора.46


4.3. Семантическая маршрутизация (Semantic Routing): Вынесение «мозга»


Еще одно противоречие: «Модель должна быть умной (для сложных задач) И модель должна быть быстрой/дешевой (для простых задач)». Использовать GPT-4 для вопроса «сколько будет 2+2» — расточительство.
   * Решение: Применение Semantic Router (Семантического маршрутизатора).48
   * Механизм: Легковесный классификатор (например, BERT) анализирует входящий запрос. Если запрос простой, маршрутизатор выносит тяжелую модель из контура обработки и направляет запрос к малой модели или детерминированной функции (калькулятору).
   * vLLM Semantic Router: Этот инструмент позволяет динамически определять сложность интента и выбирать соответствующий путь исполнения. Тяжелая «артиллерия» (reasoning models) подключается только тогда, когда это действительно необходимо. Таким образом, сложность и стоимость выносятся из простых операций.50
________________


5. Инженерные протоколы и паттерны реализации


На основе проведенного анализа мы формулируем сводный протокол применения принципа ТРИЗ №2 для оптимизации архитектуры ИИ-агента.


5.1. Протокол «Чистый Сигнал» (Anti-Hallucination Protocol)


Этап
	Действие (ТРИЗ: Вынесение)
	Техническая реализация
	Pre-Generation
	Вынесение классификации сложности.
	Semantic Router: Если запрос требует фактов -> Активировать RAG/Tools. Если запрос простой -> Fast LLM.
	Generation
	Разделение генерации и проверки.
	CoVe (Factored): 1. Draft -> 2. Plan Questions -> 3. Verify (Search/Tool) -> 4. Rewrite.
	Post-Generation
	Вынесение фильтрации безопасности.
	NeMo Guardrails / Llama Guard: Проверка Output на токсичность/PII. Блокировка или перефразирование (Masking).
	

5.2. Протокол «Drop Tanks» (Context Management Protocol)


Для долгоживущих автономных агентов (Long-running agents) применяется следующий цикл управления памятью:
   1. Фаза накопления (Accumulation): Агент выполняет подзадачу, накапливая Ephemeral Memory (токены диалога, результаты вызова инструментов).
   2. Фаза кристаллизации (Crystallization): По достижении логической точки (или лимита токенов) запускается процесс суммаризации. Из «сырого» диалога извлекаются (выносятся) ключевые факты и инсайты.
   3. Фаза сброса (Jettison):
   * Сохранить инсайты в Persistent Memory (Vector DB / User Profile).
   * Выполнить DELETE для Ephemeral Memory (Сброс баков).
   * Восстановить System Prompt + Attention Sinks (Фундамент).
   * Внедрить сжатый контекст (Summary) в начало нового окна.
   4. Результат: Агент всегда работает в зоне высокой эффективности внимания (начало контекстного окна), избегая эффекта «Lost in the Middle».2


5.3. Паттерн «Невидимый Пилот» (Invisible Pilot Pattern)


Для проектирования UX агента следует стремиться к уровню автономности 4 (User as Approver) или 5 (User as Observer), где это допустимо.32
   * Вынесение чата: Если задача может быть решена без диалога (на основе контекста экрана, файла, кода), чат-интерфейс должен быть скрыт. Вместо него используются индикаторы состояния («Thinking...», «Working...») и результат (Ghost Text, измененный файл).
   * Вынесение личности: В системном промпте следует минимизировать «болтливость» (chattiness). Инструкция: «Do not explain what you are doing. Just do it. Output only the code/result».42 Это экономит токены и время пользователя.
________________


6. Заключение


Применение принципа ТРИЗ №2 «Вынесение» к архитектуре ИИ-агентов позволяет трансформировать их из вероятностных, шумных и контекстно-зависимых чат-ботов в надежные инженерные системы.
   1. Архитектурно: Мы переходим от монолита к модульной системе, где генерация отделена от верификации (CoVe), а правила отделены от исполнения (Constitutional AI).
   2. Ресурсно: Мы переходим от бесконечного накопления контекста к циклическому сбросу «балласта» (Drop Tanks, H2O), поддерживая высокую производительность внимания.
   3. Интерфейсно: Мы переходим от навязчивого диалога к невидимому содействию (Zero UI), где ИИ становится прозрачным слоем функциональности.
Интеграция этих паттернов формирует облик ИИ следующего поколения: строгого внутри, гибкого снаружи, невидимого в простое и абсолютно точного в действии. Это и есть инженерная реализация «Чистого Сигнала».
Цитируемые источники:


1
Источники
   1. Trustworthy Generative AI — Hallucination Detection Post-Processing | LivePerson Developer Center, дата последнего обращения: ноября 25, 2025, https://developers.liveperson.com/trustworthy-generative-ai-llm-gateway-hallucination-detection-post-processing.html
   2. Context Engineering in LLM-Based Agents | by Jin Tan Ruan, CSE Computer Science, дата последнего обращения: ноября 25, 2025, https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc
   3. TRIZ-02 Taking out（Sub-principle illustrated version）, дата последнего обращения: ноября 25, 2025, https://www.proengineer-institute.com/en_triz02.html
   4. TRIZ for Digital Systems Engineering: New Characteristics and Principles Redefined - MDPI, дата последнего обращения: ноября 25, 2025, https://www.mdpi.com/2079-8954/7/3/39
   5. 40 Inventive Principles for Business - The Triz Journal, дата последнего обращения: ноября 25, 2025, https://the-trizjournal.com/40-inventive-business-principles-examples/
   6. LLM Hallucinations 101: Why Do They Appear? Can We Avoid Them?, дата последнего обращения: ноября 25, 2025, https://neptune.ai/blog/llm-hallucinations
   7. Chain of Verification (CoVe) — Understanding & Implementation | by sourajit roy chowdhury | Medium, дата последнего обращения: ноября 25, 2025, https://sourajit16-02-93.medium.com/chain-of-verification-cove-understanding-implementation-e7338c7f4cb5
   8. Chain of Verification Implementation Using LangChain Expression Language and LLM, дата последнего обращения: ноября 25, 2025, https://www.analyticsvidhya.com/blog/2023/12/chain-of-verification-implementation-using-langchain-expression-language-and-llm/
   9. Chain-of-Verification (CoVe): Reduce LLM Hallucinations - Learn Prompting, дата последнего обращения: ноября 25, 2025, https://learnprompting.org/docs/advanced/self_criticism/chain_of_verification
   10. NVIDIA-NeMo/Guardrails: NeMo Guardrails is an open ... - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/NVIDIA-NeMo/Guardrails
   11. meta-llama/Llama-Guard-4-12B - Hugging Face, дата последнего обращения: ноября 25, 2025, https://huggingface.co/meta-llama/Llama-Guard-4-12B
   12. Llama Guard – Vertex AI - Google Cloud Console, дата последнего обращения: ноября 25, 2025, https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-guard
   13. NeMo Guardrails, the Ultimate Open-Source LLM Security Toolkit | Towards Data Science, дата последнего обращения: ноября 25, 2025, https://towardsdatascience.com/nemo-guardrails-the-ultimate-open-source-llm-security-toolkit-0a34648713ef/
   14. Strategies, Patterns, and Methods to Avoid Hallucination in Large Language Model Responses | by Frank Goortani | Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@FrankGoortani/strategies-patterns-and-methods-to-avoid-hallucination-in-large-language-model-responses-81a871987d96
   15. How to build a multi-agent system using Elasticsearch and LangGraph, дата последнего обращения: ноября 25, 2025, https://www.elastic.co/search-labs/blog/multi-agent-system-llm-agents-elasticsearch-langgraph
   16. Building a Self-Correcting AI: A Deep Dive into the Reflexion Agent with LangChain and LangGraph | by Vi Q. Ha | Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@vi.ha.engr/building-a-self-correcting-ai-a-deep-dive-into-the-reflexion-agent-with-langchain-and-langgraph-ae2b1ddb8c3b
   17. Stop Your AI Assistant from Hallucinating: A Grounded Workflow for LangGraph, дата последнего обращения: ноября 25, 2025, https://dev.to/boting_wang_9571e70af30b/stop-your-ai-assistant-from-hallucinating-a-grounded-workflow-for-langgraph-10na
   18. Ijrcm 2 Cvol 2 Issue 9 | PDF | Social Entrepreneurship - Scribd, дата последнего обращения: ноября 25, 2025, https://www.scribd.com/document/149985430/Ijrcm-2-Cvol-2-Issue-9
   19. Dwight D. Eisenhower National Security Conference 2002 - U.S. Army Center of Military History, дата последнего обращения: ноября 25, 2025, https://history.army.mil/portals/143/Images/Publications/catalog/70-82-1.pdf
   20. Understanding Memory Management in LangGraph: A Practical Guide for GenAI Students | by Jaime Lucena Pérez | Nov, 2025 | Towards AI, дата последнего обращения: ноября 25, 2025, https://pub.towardsai.net/understanding-memory-management-in-langgraph-a-practical-guide-for-genai-students-b3642c9ea7e1
   21. Memory overview - Docs by LangChain, дата последнего обращения: ноября 25, 2025, https://docs.langchain.com/oss/python/langgraph/memory
   22. ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking - arXiv, дата последнего обращения: ноября 25, 2025, https://www.arxiv.org/pdf/2510.13842
   23. CrewAi Docs 25-2-25 | PDF | Command Line Interface - Scribd, дата последнего обращения: ноября 25, 2025, https://www.scribd.com/document/841532683/CrewAi-docs-25-2-25
   24. Langgraph state reset #912 - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/langchain-ai/langgraph/discussions/912
   25. H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models - NIPS papers, дата последнего обращения: ноября 25, 2025, https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf
   26. Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache - MLSys Proceedings, дата последнего обращения: ноября 25, 2025, https://proceedings.mlsys.org/paper_files/paper/2024/file/bbb7506579431a85861a05fff048d3e1-Paper-Conference.pdf
   27. [2306.14048] H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/abs/2306.14048
   28. Arxiv Dives - Efficient Streaming Language Models with Attention Sinks - Oxen.ai, дата последнего обращения: ноября 25, 2025, https://ghost.oxen.ai/arxiv-dives-efficient-streaming-language-models-with-attention-sinks/
   29. ️ Attention Sinks in LLMs for endless fluency (related to StreamingLLM) : r/LocalLLaMA - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/LocalLLaMA/comments/173slpi/hugging_face_community_blogpost_attention_sinks/
   30. [2309.17453] Efficient Streaming Language Models with Attention Sinks - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/abs/2309.17453
   31. Efficient Streaming Language Models with Attention Sinks - OpenReview, дата последнего обращения: ноября 25, 2025, https://openreview.net/forum?id=NG7sS51zVF
   32. The Practical Guide to the Levels of AI Agent Autonomy | by Sean Falconer | Nov, 2025, дата последнего обращения: ноября 25, 2025, https://seanfalconer.medium.com/the-practical-guide-to-the-levels-of-ai-agent-autonomy-ac5115d3af26
   33. The 5 Levels of AI Autonomy: From Co-Pilots to AI Agents, дата последнего обращения: ноября 25, 2025, https://www.turian.ai/blog/the-5-levels-of-ai-autonomy
   34. Levels of Autonomy for AI Agents Working Paper - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2506.12469v1
   35. Zero UI: The invisible interface revolution - Microsoft Advertising, дата последнего обращения: ноября 25, 2025, https://about.ads.microsoft.com/en/blog/post/june-2025/zero-ui-the-invisible-interface-revolution
   36. Using GitHub Copilot's "Ghost Text" to Code Faster Than You Think - Authentic Jobs, дата последнего обращения: ноября 25, 2025, https://authenticjobs.com/using-github-copilots-ghost-text-to-code-faster-than-you-think/
   37. GitHub for Beginners: Essential features of GitHub Copilot, дата последнего обращения: ноября 25, 2025, https://github.blog/ai-and-ml/github-copilot/github-for-beginners-essential-features-of-github-copilot/
   38. Ambient Intelligence: The Next Frontier of Computing | Innatera, дата последнего обращения: ноября 25, 2025, https://innatera.com/blog/ambient-intelligence-the-next-frontier-of-computing
   39. AI is the new UI: How ambient AI makes smartphone experiences effortless, дата последнего обращения: ноября 25, 2025, https://www.micron.com/about/blog/applications/ai/ai-is-the-new-ui-how-ambient-ai-makes-smartphone-experiences-effortless
   40. System Prompt vs User Prompt in AI: What's the difference? - PromptLayer Blog, дата последнего обращения: ноября 25, 2025, https://blog.promptlayer.com/system-prompt-vs-user-prompt-a-comprehensive-guide-for-ai-prompts/
   41. LLM System Prompt vs. User Prompt - Nebuly, дата последнего обращения: ноября 25, 2025, https://www.nebuly.com/blog/llm-system-prompt-vs-user-prompt
   42. The Art of Writing Great System Prompts | by Saurabh Singh - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/towardsdev/the-art-of-writing-great-system-prompts-abb22f8b8f37
   43. What exactly is a system Prompt? How different is it from user prompt? - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1hfcgol/what_exactly_is_a_system_prompt_how_different_is/
   44. Constitutional AI: Principle-Based Alignment Through Self-Critique - Michael Brenndoerfer, дата последнего обращения: ноября 25, 2025, https://mbrenndoerfer.com/writing/constitutional-ai-principle-based-alignment-through-self-critique
   45. Constitutional AI: Harmlessness from AI Feedback - Anthropic, дата последнего обращения: ноября 25, 2025, https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback
   46. Collective Constitutional AI: Aligning a Language Model with Public Input - Anthropic, дата последнего обращения: ноября 25, 2025, https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input
   47. Constitutional AI explained - Toloka AI, дата последнего обращения: ноября 25, 2025, https://toloka.ai/blog/constitutional-ai-explained/
   48. When to Reason: Semantic Router for vLLM - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2510.08731v1
   49. Smarter LLMs: How the vLLM Semantic Router Delivers Fast, Efficient Inference, дата последнего обращения: ноября 25, 2025, https://joshuaberkowitz.us/blog/news-1/smarter-llms-how-the-vllm-semantic-router-delivers-fast-efficient-inference-1133
   50. vllm-project/semantic-router: Intelligent Router for Mixture-of-Models - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/vllm-project/semantic-router
   51. vLLM Semantic Router: Improving efficiency in AI reasoning | Red Hat Developer, дата последнего обращения: ноября 25, 2025, https://developers.redhat.com/articles/2025/09/11/vllm-semantic-router-improving-efficiency-ai-reasoning
   52. vLLM Semantic Router: Next Phase in LLM inference, дата последнего обращения: ноября 25, 2025, https://blog.vllm.ai/2025/09/11/semantic-router.html
   53. Context Window Limitations of LLMs - Perplexity, дата последнего обращения: ноября 25, 2025, https://www.perplexity.ai/page/context-window-limitations-of-FKpx7M_ITz2rKXLFG1kNiQ
   54. Claude 4 System Prompts : Operational Blueprint and Strategic Implications - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@tuhinsharma121/decoding-claude-4-system-prompts-operational-blueprint-and-strategic-implications-727294cf79c3
   55. When Words Cannot Describe: Designing For AI Beyond Conversational Interfaces, дата последнего обращения: ноября 25, 2025, https://www.smashingmagazine.com/2024/02/designing-ai-beyond-conversational-interfaces/
   56. 7 Key Design Patterns for AI Interfaces | by Fanny | UX Planet, дата последнего обращения: ноября 25, 2025, https://uxplanet.org/7-key-design-patterns-for-ai-interfaces-893ab96988f6