АРХИТЕКТУРА ПОРИСТОГО ИНТЕЛЛЕКТА: ИНТЕГРАЦИЯ ПРИНЦИПА №31 В ПРОЕКТИРОВАНИЕ АГЕНТНЫХ СИСТЕМ




Введение: От монолита к клеточной структуре


В современной парадигме генеративного искусственного интеллекта доминирует концепция «монолита». Большие языковые модели (LLM) традиционно воспринимаются и эксплуатируются как плотные, непроницаемые блоки вычислений, которые поглощают массивные потоки неструктурированного текста и экстраполируют столь же плотные, непрерывные блоки ответа. Этот подход, хотя и эффективен для задач креативного письма или базового диалога, демонстрирует критические ограничения при внедрении в сложные корпоративные среды, требующие высокой точности, интерактивности и верифицируемости. Монолитные агенты страдают от высокой латентности (аналог термической массы), избыточного потребления токенов (аналог лишнего веса конструкции) и склонности к галлюцинациям, возникающим, когда модель пытается заполнить пробелы в знаниях правдоподобной, но ложной соединительной тканью (перегрев).
Настоящий отчет исследует фундаментальный сдвиг парадигмы, вдохновленный принципом ТРИЗ №31: «Пористые материалы». Мы предлагаем переосмыслить архитектуру ИИ-агента не как сплошной кирпич интеллекта, а как пористую, ячеистую структуру. В материаловедении пористость не является дефектом; это инженерное решение для управления весом, теплообменом и проницаемостью. В контексте вычислительной лингвистики и проектирования агентов «поры» — это не отсутствие интеллекта, а специально спроектированные функциональные пустоты.
Эти пустоты служат пяти критическим целям, которые будут детально проанализированы в данном исследовании:
1. Интерактивность через заполнение слотов (Slot Filling): Переход от пассивного потребления контента пользователем к активному соавторству через заполнение оставленных агентом пробелов.
2. Селективная проницаемость (Active Filtering): Создание полупроницаемых мембран внимания, которые пропускают фактологический сигнал, но задерживают эмоциональный шум.
3. Структурная эффективность (Honeycomb Structures): Замена плотной прозы на «воздушные» структуры графов знаний, снижающие вес ответа при сохранении жесткости аргументации.
4. Терморегуляция (Transpiration Cooling): Управление «температурой» неопределенности через выделение сигнальных токенов («пота») вместо генерации ложных утверждений.
5. Динамическая адаптация (Impregnation): Использование пустот в фиксированной структуре системного промпта для инъекции контекста RAG (Retrieval-Augmented Generation).
Ниже представлен всесторонний анализ, синтезирующий данные последних исследований в области промпт-инжиниринга, графовых баз данных и когнитивных архитектур.
________________


Часть I. Архитектура «Заполняемых Шаблонов» (Slot Filling) и Каркасная Генерация


Переход от монолитной генерации к пористой, «каркасной» генерации решает одну из фундаментальных проблем взаимодействия человека и ИИ: асимметрию полосы пропускания. LLM генерирует текст быстрее, чем человек может его критически осмыслить, создавая когнитивную перегрузку. Принцип пористых материалов предписывает создание структур, где твердая фаза (сгенерированный контент) чередуется с пустотами (местами для участия человека), что меняет саму природу диалога.


1.1 Теоретические основы «Каркаса Мысли» (Skeleton-of-Thought)


Традиционные LLM работают в режиме последовательного декодирования, генерируя ответ токен за токеном. Это создает задержку и часто приводит к потере структурной целостности длинных ответов. Методология Skeleton-of-Thought (SoT), исследованная в ряде недавних работ 1, предлагает альтернативу: сначала генерируется скелет ответа, а затем происходит параллельное заполнение его частей. В контексте пористого агента мы адаптируем этот метод, оставляя часть «пор» незаполненными намеренно.


1.1.1 Декомпозиция и параллельная генерация


Метод SoT базируется на интуитивном понимании человеческого мышления: эксперты редко пишут сложные отчеты линейно от первого до последнего слова. Сначала формируется план (скелет), затем прорабатываются детали. В вычислительном плане это реализуется через двухэтапный промптинг.
На первом этапе агент получает инструкцию действовать как «организатор», выдавая только краткий скелет ответа. Исследования показывают, что скелет должен быть предельно лаконичным (3-5 слов на пункт) и содержать 3-10 пунктов.4 Это создает «жесткий каркас» диалога. На втором этапе происходит расширение пунктов (Point-Expanding Stage). Здесь проявляется ключевое преимущество пористой структуры: возможность параллельной обработки. Вместо того чтобы ждать завершения генерации первого пункта, система может инициировать генерацию контента для всех пунктов одновременно через параллельные API-вызовы.1
Такой подход не только ускоряет вывод (в экспериментах наблюдалось ускорение более чем в 2 раза на различных моделях 1), но и повышает качество ответа за счет четкого планирования. Скелет действует как направляющая структура, предотвращая дрейф темы и галлюцинации, свойственные длинным монолитным генерациям. Однако, исследования отмечают, что SoT может уступать в связности (coherence) и иммерсивности текста, так как части генерируются изолированно.5 Для технического агента это, скорее, преимущество: отсутствие лишней «воды» и связующих фраз делает структуру более жесткой и информационно плотной.


1.1.2 Инверсия контроля: Пользователь как заполнитель пор


В классическом понимании SoT агент сам заполняет свой скелет. В архитектуре «Пористого Агента» мы вводим инвертированное заполнение слотов (Inverse Slot Filling). Агент генерирует структуру, определяет зависимости, но критически важные переменные оставляет пустыми для ввода пользователем.
Это решение базируется на технологии Fill-in-the-Middle (FIM), изначально разработанной для автодополнения кода.6 Модели, обученные с использованием специальных токенов <PRE>, <MID>, <SUF>, понимают контекст не только слева направо, но и как «мост через пропасть». Мы используем эту способность для создания диалоговых шаблонов.
Рассмотрим сценарий планирования проекта.
* Монолитный подход: Агент пытается угадать все параметры и выдает готовый план: «Проект начнется 1 января, бюджет $10k...». Пользователь вынужден читать и исправлять ошибки агента, тратя когнитивный ресурс на опровержение галлюцинаций.
* Пористый подход: Агент генерирует структуру зависимостей: «Фаза 1 (Подготовка) требует [ВВОД: Бюджет] и займет [ВВОД: Срок]. Фаза 2 начнется после завершения Фазы 1».
Здесь агент выступает не как оракул, а как архитектор. Он создает «соты» логики, а «медом» данных их заполняет пользователь. Это превращает чтение ответа в интерактивную работу по сборке решения. Исследования подтверждают, что структурированный вывод (например, в формате JSON Schema) повышает надежность интеграции с внешними системами до 100% при использовании режима strict mode, в то время как обычный промптинг дает лишь около 36% надежности форматирования.9


1.2 Метод «Активной фильтрации»: Мембрана внимания


Второй аспект принципа пористости касается входящего потока данных. Пористый материал функционирует как фильтр, задерживая крупные частицы и пропуская мелкие (или наоборот). В контексте ИИ-агента «частицами» являются семантические единицы. Цель Активной фильтрации — создать мембрану внимания, проницаемую для фактов и технических терминов, но непроницаемую для эмоционального шума и риторической воды.


1.2.1 Проблема эмоциональной энтропии


Когнитивная перегрузка оператора часто усугубляется эмоционально окрашенным вводом.10 Фразы вроде «Я ненавижу эту систему!», «Это срочно!!!» не несут технической информации, но создают высокий уровень «шума». В стандартной архитектуре трансформера механизм Self-Attention (самовнимания) является «плотным»: каждый токен взаимодействует с каждым. Это означает, что слово «ненавижу» получает такой же вычислительный вес, как и слово «ошибка 404», и может сместить вектор ответа в сторону психологической поддержки, а не технического решения.
Эмоциональный язык часто обладает высокой энтропией, но низкой информационной плотностью для решения задачи. Если агент обучался на данных, где за эмоциональными всплесками следовали конфликты, он может перейти в режим «защиты», генерируя извинения, что является потерей ресурсов.11


1.2.2 Реализация семантического фильтра


Для реализации активной фильтрации используется предварительный слой обработки, основанный на методах переноса стиля (Style Transfer).13 Этот слой действует как «грубый фильтр», очищающий поток токенов перед подачей в основное ядро рассуждений.
Механизм работает следующим образом:
1. Входной сигнал: «Я в бешенстве! Опять этот чертов API упал с 503 ошибкой, у меня дедлайн горит!»
2. Фильтрация (Денойзинг): Специализированный промпт или малая модель (SLM) получает инструкцию извлечь сущности и действия, игнорируя сентимент.16 Паттерн промпта: "Extract facts. Ignore emotion. Output JSON."
3. Выходной сигнал: {"Событие": "Сбой API", "Код ошибки": "503", "Контекст": "Срочно"}.
Исследования в области Text Style Transfer (TST) показывают, что современные модели способны эффективно разделять семантическое содержание и стилистические атрибуты (вежливость, эмоциональность), переписывая текст с сохранением смысла, но изменением тональности.18 Применение такого фильтра позволяет основному агенту работать в режиме «чистой логики», не затрачивая токены на обработку эмоционального контекста пользователя. Это также стабилизирует персону агента, предотвращая дрейф в сторону излишней эмоциональности или агрессии в ответ на грубость пользователя.20


1.3 Сравнительный анализ эффективности


Для наглядности сравним эффективность монолитного и пористого подходов в различных метриках, основываясь на проанализированных данных.


Характеристика
	Монолитный Агент (Wall of Text)
	Пористый Агент (Skeleton + Filter)
	Обоснование и источники
	Латентность (Скорость)
	Высокая (Последовательная генерация)
	Низкая (Параллельная генерация SoT)
	SoT ускоряет вывод до 2x за счет параллельных API-вызовов.1
	Потребление токенов
	Высокое (Избыточная связность)
	Оптимизированное (Только суть)
	Каркасная структура исключает вводные слова и переходы.4
	Устойчивость к шуму
	Низкая (Attention захватывает эмоции)
	Высокая (Активная фильтрация)
	Фильтрация удаляет эмоциональные токены до попадания в контекст.12
	Вовлеченность
	Пассивная (Чтение)
	Активная (Соавторство)
	Заполнение слотов требует действий от пользователя.9
	Риск галлюцинаций
	Высокий (Модель додумывает факты)
	Низкий (Модель оставляет слоты)
	Инверсия слотов предотвращает генерацию непроверенных данных.21
	Вывод по Части I:
Применение принципа пористости к архитектуре генерации трансформирует агента из «писателя» в «конструктора». Использование методов Skeleton-of-Thought создает жесткий несущий каркас ответа, в то время как технология Inverse Slot Filling намеренно оставляет в этом каркасе функциональные пустоты для пользователя. Активная фильтрация на входе обеспечивает чистоту «строительного материала» (токенов), отсеивая эмоциональный шлак. Такая система не просто выдает информацию, она создает структурированное пространство для совместного решения задачи.
________________


Часть II. Аэрокосмическая Метафора: Сотовые Конструкции и Транспирационное Охлаждение


В аэрокосмической инженерии каждый грамм массы имеет критическое значение. Сплошной алюминиевый блок используется крайне редко; вместо него применяются сотовые панели (honeycomb), обеспечивающие ту же жесткость при 10% веса. В экономике LLM «весом» является количество токенов. Токены стоят денег при генерации, увеличивают задержку при передаче и потребляют ограниченный ресурс контекстного окна. Для создания «легких» агентов необходимо внедрить архитектуру Сотовых Конструкций.
Вторая критическая проблема — перегрев. В ракетных двигателях, когда температура стенок сопла превышает предел прочности материала, используется транспирационное охлаждение: охладитель просачивается через пористые стенки, создавая защитную пленку. В ИИ аналогом тепла является Неопределенность. Когда агент сталкивается с запросом за пределами своих знаний, его «температура» (энтропия) растет, что грозит «термическим разрушением» в виде галлюцинаций. Принцип №31 предлагает решение через механизм «потения» (Sweating).


2.1 Сотовые конструкции: Графы знаний как каркас


Традиционные системы RAG (Retrieval Augmented Generation) работают по принципу «векторного поиска», извлекая массивные куски (chunks) текста. Это эквивалентно использованию сплошного материала: вместе с нужным фактом в контекст попадает огромное количество «соединительной ткани» — предлогов, вводных конструкций, избыточных описаний.


2.1.1 Графовый RAG против Текстового RAG


Реализацией сотовой структуры в ИИ является GraphRAG (RAG на основе графов знаний).22 Граф знаний удаляет лингвистическую «воду» и оставляет только структурную суть: узлы (сущности) и ребра (отношения).
Сравним эффективность представления информации:
* Текстовое представление (Сплошное): «Двигатель F-1, который использовался на ракете Сатурн-5, работает по циклу газогенератора. Этот цикл отличается тем, что сжигает небольшую часть топлива для привода турбонасосов». (~35 токенов).
* Графовое представление (Сотовое):
(Двигатель F-1) --[часть]--> (Сатурн-5)
(Двигатель F-1) --[цикл]--> (Газогенератор)
(Газогенератор) --[функция]--> (Привод турбонасосов)
(~15 токенов).
Исследования эффективности токенов в системах типа TERAG (Token-Efficient RAG) показывают, что графовые методы поиска могут сократить потребление токенов на выходе на 89-97% по сравнению с методами, основанными на извлечении текстовых чанков.25 Граф предварительно сжимает информацию. Пустоты между узлами графа семантически пусты, но структурно жестки — отношение определено без необходимости многословного объяснения.


2.1.2 Структурная жесткость аргументации


В сотовых панелях жесткость обеспечивается геометрией ячеек. В диалоге с агентом аналогом жесткости является структурированный вывод. Прозаический ответ гибок и аморфен; он может скрывать логические противоречия в красивых оборотах. Структурированный ответ (JSON, таблица, схема Mermaid) не прощает ошибок.
Для реализации «жесткости» агент должен быть настроен на вывод в форматах схем (Schema-First):
   1. Диаграммы Mermaid: Вместо описания процесса словами, агент генерирует код блок-схемы. Текст минимален, структура передает смысл.
   2. Гиперссылки как распорки: Вместо пересказа концепции, агент предоставляет жесткую ссылку (цитату) на источник. Ссылка работает как силовая распорка в сотах — она несет нагрузку доказательства без добавления веса текста.28
Такой подход позволяет создавать ответы, которые «весят» в 10 раз меньше монолитных, но обладают большей аргументативной прочностью.


2.2 Транспирационное охлаждение: Механизм «Потения»


Принцип «потения» (выделения жидкости через поры для охлаждения) является мощнейшей метафорой для управления неопределенностью. Когда модель «перегревается» (сталкивается с неоднозначностью), она не должна плавиться (галлюцинировать). Она должна выделять сигнальные токены.


2.2.1 Термодинамика галлюцинаций


Галлюцинация в LLM — это следствие принудительной генерации в условиях высокой энтропии. Модель обязана предсказать следующий токен, даже если вероятность любого токена низка. В терминах физики это «термический разгон». Исследования показывают, что модели часто бывают самоуверенными (overconfident), даже когда ошибаются.29 Однако, внутренняя метрика неопределенности (например, энтропия распределения вероятностей или log-probabilities топ-токенов) часто коррелирует с фактической ошибкой.31


2.2.2 Реализация «потовых желез» (Uncertainty Quantification)


Чтобы предотвратить галлюцинацию, мы внедряем механизм Квантификации Неопределенности (Uncertainty Quantification - UQ) непосредственно в процесс генерации.33
Механизм работы:
   1. Детекция (Термостат): В процессе генерации монитор (внешний или внутренний слой) отслеживает логиты (log-probabilities) топ-токенов. Если уверенность падает ниже порога (например, $P < 0.6$) или энтропия превышает лимит, фиксируется «перегрев».36
   2. Активация (Пора): Вместо того чтобы сэмплировать низковероятный токен (угадывать), модель активирует фоллбэк-механизм 38 и выбрасывает специальный токен неопределенности (Signal Token).
   * Примеры токенов: , , , .
Двойной эффект охлаждения:
   1. Внутреннее охлаждение: Выбрасывая токен ``, модель избегает штрафа целевой функции за ложь. Она «вознаграждается» за честное признание незнания. Это сохраняет калибровку модели.29
   2. Внешнее охлаждение (Ожидания пользователя): Пользователь, получающий уверенный, но ложный ответ, находится в состоянии «высокой температуры» (высокое доверие, высокий риск). Увидев токены неопределенности, пользователь «остывает» — его доверие снижается до здорового скептицизма.40


2.2.3 Фазовый переход: От генерации к поиску


В физике испарение поглощает скрытую теплоту. В ИИ разрешение неопределенности требует «скрытых вычислений». Выделение токена `` должно триггерить фазовый переход в поведении системы:
   * Жидкая фаза (Генерация): Модель пишет текст.
   * Событие потения: Выброс токена ``.
   * Газовая фаза (Поиск/Рассуждение): Генерация приостанавливается. Токен неопределенности запускает вызов инструмента (Tool Call). Агент обращается к внешней базе знаний (RAG) или задает уточняющий вопрос пользователю.38
Таким образом, «пот» испаряется (неопределенность устраняется), поглощая «тепло» (вычислительные затраты) поискового запроса. Это превращает агента из закрытой системы в открытую термодинамическую систему, способную поддерживать гомеостаз правдивости.
________________


Часть III. Пропитка (Impregnation): Динамическая Инъекция Контекста


Принцип ТРИЗ №31б гласит: «Сделать поры пористыми или заполнить их другим веществом». В материаловедении это пропитка (импрегнация), например, маслонаполненные подшипники. В архитектуре ИИ это транслируется в Динамическую Инъекцию Контекста.


3.1 Матрица и Поры: Разделение структуры и содержания


Мы рассматриваем Системный Промпт как твердую матрицу агента. Она определяет его персону, ограничения, правила безопасности и формат вывода. В монолитной архитектуре промпт жестко сцеплен с контекстом. В пористой архитектуре мы используем шаблонизаторы (например, Jinja2, Mustache или шаблоны LangChain), чтобы создать жесткую структуру с явными пустотами.41
Архитектура шаблона (Jinja2):


Фрагмент кода




SYSTEM: Вы - структурный инженер.
RULES:
1. Всегда рассчитывайте коэффициенты запаса.
2. Используйте метрическую систему.
3. Используйте данные, предоставленные в блоке CONTEXT.

CONTEXT:
{{ dynamic_rag_context }}  <-- ПОРА (Место для инъекции)

QUERY:
{{ user_query }}           <-- ПОРА (Место для запроса)

В этом подходе матрица (инструкции) неизменна и «тверда», а поры предназначены для заполнения жидким «веществом» — данными.


3.2 Динамическая инъекция RAG как «Вещества»


«Пропитка» происходит в рантайме (время выполнения). В отличие от простого добавления контекста в конец (append), импрегнация подразумевает заполнение внутренних пустот структуры, что позволяет более точно управлять вниманием модели.
   1. Семантический поиск: Система RAG извлекает релевантные фрагменты (или подграфы) из базы знаний.
   2. Инъекция: Эти данные впрыскиваются в пору {{ dynamic_rag_context }}.45
Такой подход позволяет реализовать мульти-портовую инъекцию. Вместо одной свалки контекста, мы можем иметь разные поры для разных типов «веществ»:
   * {{ definitions_pore }}: Заполняется терминами из глоссария.
   * {{ history_pore }}: Заполняется кратким саммари предыдущего диалога.
   * {{ tools_pore }}: Заполняется спецификациями доступных API-инструментов.47
Это критически важно для контекстной гигиены. Если мы просто добавляем всё в конец, модель может «забыть» начальные инструкции (эффект lost-in-the-middle). Пористая структура удерживает инструкции (матрицу) вокруг данных, обеспечивая их соблюдение.


3.2.1 Безопасность и «Засорение пор»


Существует риск перенасыщения пор. Если впрыснуть слишком много данных, давление (количество токенов) превысит лимит контекстного окна, вытесняя системные инструкции. Кроме того, существует риск Prompt Injection через пользовательский ввод — аналог «коррозионной жидкости», разъедающей матрицу.
   * Решение: Перед пропиткой вещество (контекст и запрос) должно проходить через фильтры санации (см. Активную фильтрацию в Части I).48 Только очищенное вещество допускается в поры матрицы.
________________


Часть IV. Синтез: Агент как Дышащая Структура


Интеграция рассмотренных аспектов принципа №31 позволяет перейти от метафоры «Стены текста» к метафоре «Дышащего фильтра».
Компонент Пористости
	Техническая Реализация
	Функция (ТРИЗ)
	Эффект для Агента
	Пустоты (Skeletons)
	SoT + FIM (Inverse Slot Filling)
	Сделать объект пористым
	Интерактивность, снижение нагрузки, соавторство.
	Мембраны (Filters)
	Style Transfer / Semantic Filtering
	Использовать поры как фильтр
	Чистота сигнала, защита от эмоционального шума.
	Соты (Honeycomb)
	GraphRAG / Structured Output
	Снизить вес объекта
	Снижение токенов на 90%, жесткость логики.
	Поры (Sweating)
	Uncertainty Quantification (UQ)
	Использовать фазовый переход
	Охлаждение ожиданий, предотвращение галлюцинаций.
	Пропитка (Impregnation)
	Jinja2 Templates + Dynamic RAG
	Заполнить поры веществом
	Актуальность знаний при сохранении структуры.
	

4.1 Пример работы интегрированного агента


Представим работу такого агента в сценарии технической поддержки сложного оборудования.
   1. Вход (Поток): Пользователь пишет: «Срочно! Насос H-20 вибрирует как бешеный, мы боимся взрыва! Что делать?»
   2. Фильтрация (Мембрана): Слой активной фильтрации удаляет панику. В ядро поступает: {"Объект": "Насос H-20", "Симптом": "Сильная вибрация", "Риск": "Взрыв"}.
   3. Пропитка (Матрица): Шаблонизатор заполняет пору {{ tech_specs }} данными из GraphRAG о насосе H-20 (допустимые уровни вибрации, схемы).
   4. Оценка (Термостат): Агент анализирует базу знаний. Он находит протокол остановки, но не уверен в точной причине вибрации (энтропия высока).
   5. Потение (Охлаждение): Вместо выдумки причины, агент генерирует токен ``.
   6. Каркас (Скелет): Агент генерирует ответ-соту:
   * Действие 1: Немедленная остановка (ссылка на протокол безопасности).
   * Действие 2: Проверка центровки вала.
   * Слот для пользователя: [ВВОД: Текущая температура подшипника?] (Агент понимает, что для диагноза не хватает данных, и оставляет пору).


4.2 Заключение


Применение принципа «Пористые материалы» к архитектуре ИИ-агентов является не просто художественной метафорой, а строгим инженерным подходом. Оно позволяет решать ключевые проблемы современных LLM: галлюцинации, многословие, пассивность пользователя и уязвимость к шуму. Пористый агент — это легкая, прочная и «прохладная» конструкция, способная эффективно функционировать в условиях высокой неопределенности и информационного давления, характерных для реальных бизнес-задач. Дальнейшее развитие этой архитектуры лежит в области стандартизации протоколов обмена «сигнальными токенами» (потом) и создании эффективных алгоритмов построения «сотовых» графов знаний в реальном времени.
Источники
   1. Skeleton-of-Thought Prompting: Faster and Efficient Response Generation, дата последнего обращения: ноября 25, 2025, https://learnprompting.org/docs/advanced/decomposition/skeleton_of_thoughts
   2. Accelerating LLMs with Skeleton-of-Thought Prompting - Portkey, дата последнего обращения: ноября 25, 2025, https://portkey.ai/blog/skeleton-of-thought-prompting/
   3. Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2307.15337v3
   4. Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation, дата последнего обращения: ноября 25, 2025, https://arxiv.org/abs/2307.15337
   5. Reducing Latency with Skeleton of Thought Prompting - PromptHub, дата последнего обращения: ноября 25, 2025, https://www.prompthub.us/blog/reducing-latency-with-skeleton-of-thought-prompting
   6. Alignment with Fill-In-the-Middle for Enhancing Code Generation - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2508.19532v1
   7. Structure-Aware Fill-in-the-Middle Pretraining for Code - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2506.00204v1
   8. What is FIM and why does it matter in LLM-based AI | by Eva Thompson | Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@SymeCloud/what-is-fim-and-why-does-it-matter-in-llm-based-ai-53f33385585b
   9. From Free-Form to Structured: A Better Way to Use LLMs - Maruti Techlabs, дата последнего обращения: ноября 25, 2025, https://marutitech.com/structured-outputs-llms/
   10. The Science Behind Cognitive Overload - DEV Community, дата последнего обращения: ноября 25, 2025, https://dev.to/a_belova/the-science-behind-cognitive-overload-39ga
   11. How to Be a World-Class Coach, Engineering Human Systems - Jake Smolarek, дата последнего обращения: ноября 25, 2025, https://jakesmolarek.com/articles/coaching-for-coaches/
   12. The Agent's Memory Dilemma: Is Forgetting a Bug or a Feature? | by Tao An - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@tao-hpu/the-agents-memory-dilemma-is-forgetting-a-bug-or-a-feature-a7e8421793d4
   13. LLM-Based Text Style Transfer: Have We Taken a Step Forward? - IEEE Xplore, дата последнего обращения: ноября 25, 2025, https://ieeexplore.ieee.org/iel8/6287639/10820123/10915631.pdf
   14. Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2505.07888v1
   15. LLM-based Text Style Transfer: Have We Taken a Step Forward? - ResearchGate, дата последнего обращения: ноября 25, 2025, https://www.researchgate.net/publication/389643037_LLM-based_Text_Style_Transfer_Have_We_Taken_a_Step_Forward
   16. Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2504.03520v1
   17. Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles - ResearchGate, дата последнего обращения: ноября 25, 2025, https://www.researchgate.net/publication/390545640_Neutralizing_the_Narrative_AI-Powered_Debiasing_of_Online_News_Articles
   18. TSTBench: A Comprehensive Benchmark for Text Style Transfer - MDPI, дата последнего обращения: ноября 25, 2025, https://www.mdpi.com/1099-4300/27/6/575
   19. Text Style Transfer: An Introductory Overview - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2407.14822v1
   20. Step-by-Step: Controlling Arbitrary Style in Text with Large Language Models - ACL Anthology, дата последнего обращения: ноября 25, 2025, https://aclanthology.org/2024.lrec-main.1328.pdf
   21. Structured outputs can hurt the performance of LLMs : r/LocalLLaMA - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1hcj0ur/structured_outputs_can_hurt_the_performance_of/
   22. The AI-Native GraphDB + GraphRAG + Graph Memory Landscape & Market Catalog, дата последнего обращения: ноября 25, 2025, https://dev.to/yigit-konur/the-ai-native-graphdb-graphrag-graph-memory-landscape-market-catalog-2198
   23. GraphRAG Costs Explained: What You Need to Know | Microsoft Community Hub, дата последнего обращения: ноября 25, 2025, https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/graphrag-costs-explained-what-you-need-to-know/4207978
   24. RAG vs. GraphRAG: A Systematic Evaluation and Key Insights - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2502.11371v2
   25. TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2509.18667v2
   26. Token Efficient RAG - Jiaxin Bai, дата последнего обращения: ноября 25, 2025, https://bjx.fun/p/token-efficient-rag/
   27. TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2509.18667v1
   28. Traditional vs Graph RAG: Boosting AI Agent Intelligence - Ardor Cloud, дата последнего обращения: ноября 25, 2025, https://ardor.cloud/blog/traditional-vs-graph-rag
   29. GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2509.09438v1
   30. What is AI? Stephen Hanson in conversation with Richard Sutton - ΑΙhub - AI Hub, дата последнего обращения: ноября 25, 2025, https://aihub.org/2021/10/14/what-is-ai-stephen-hanson-in-conversation-with-richard-sutton/
   31. Token Entropy Predicts LLM Uncertainty in Knowledge Tasks but not Reasoning Tasks : r/artificial - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/artificial/comments/1j4t2uh/token_entropy_predicts_llm_uncertainty_in/
   32. FLUE: Streamlined Uncertainty Estimation for Large Language Models, дата последнего обращения: ноября 25, 2025, https://ojs.aaai.org/index.php/AAAI/article/view/33840/35995
   33. SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2511.16275v1
   34. Learning to Route LLMs with Confidence Tokens - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2410.13284v3
   35. Can LLMs Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner - ACL Anthology, дата последнего обращения: ноября 25, 2025, https://aclanthology.org/2024.emnlp-main.1205.pdf
   36. kashif/DeepConf - Hugging Face, дата последнего обращения: ноября 25, 2025, https://huggingface.co/kashif/DeepConf
   37. Building Confidence: A Case Study in How to Create Confidence Scores for GenAI Applications | Spotify Engineering, дата последнего обращения: ноября 25, 2025, https://engineering.atspotify.com/2024/12/building-confidence-a-case-study-in-how-to-create-confidence-scores-for-genai-applications
   38. monostate/weave-logprobs-reasoning-loop: A notebook that compares a reasoning model x a non reasoning model that runs a loop using logprobs found uncertainty - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/monostate/weave-logprobs-reasoning-loop
   39. Multi-Layered Framework for LLM Hallucination Mitigation in High-Stakes Applications: A Tutorial - MDPI, дата последнего обращения: ноября 25, 2025, https://www.mdpi.com/2073-431X/14/8/332
   40. How to Prevent LLM Hallucinations: 5 Proven Strategies - Voiceflow, дата последнего обращения: ноября 25, 2025, https://www.voiceflow.com/blog/prevent-llm-hallucinations
   41. FlexiDataGen: An Adaptive LLM Framework for Dynamic Semantic Dataset Generation in Sensitive Domains - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2510.19025v1
   42. Dynamic Prompts with LangChain Templates - Newline.co, дата последнего обращения: ноября 25, 2025, https://www.newline.co/@zaoyang/dynamic-prompts-with-langchain-templates--71d0c244
   43. Release 17.0.1.dev58+g9207b2554 Equinor & TNO - Everest™ documentation, дата последнего обращения: ноября 25, 2025, https://everest.readthedocs.io/_/downloads/en/latest/pdf/
   44. How to Prompt Mistral AI models, and Why | AWS Builder Center, дата последнего обращения: ноября 25, 2025, https://builder.aws.com/content/2dFNOnLVQRhyrOrMsloofnW0ckZ/how-to-prompt-mistral-ai-models-and-why
   45. LLM Engineering: Implementing RAG on LangChain | by Kushal V | TechHappily - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/techhappily/llm-engineering-implementing-rag-on-langchain-e7d8ddd895f1
   46. Dynamic Context Injection with Retrieval Augmented Generation - Newline.co, дата последнего обращения: ноября 25, 2025, https://www.newline.co/@zaoyang/dynamic-context-injection-with-retrieval-augmented-generation--68b80921
   47. [Feature Request]: Semantic tool auto-filtering @ MCP Client / Proxy #845 - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/modelcontextprotocol/modelcontextprotocol/discussions/845
   48. Library - PLOT4AI, дата последнего обращения: ноября 25, 2025, https://plot4.ai/library