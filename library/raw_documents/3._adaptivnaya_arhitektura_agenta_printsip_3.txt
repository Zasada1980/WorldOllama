Архитектура Локального Качества: Парадигмальный сдвиг к Функционально-Градиентному Искусственному Интеллекту




1. Введение: Кризис Монолитности и Императив Локального Качества


В современной ландшафте генеративного искусственного интеллекта (ИИ) доминирует парадигма монолитности. Стандартная модель развертывания Больших Языковых Моделей (LLM) подразумевает использование единой нейронной сети, работающей с фиксированным набором гиперпараметров (температура, top-p, штрафы за повторение) для обработки всего потока пользовательского запроса. Этот подход, напоминающий попытку использовать один инструмент для всех этапов строительства здания, неизбежно приводит к усреднению качества. Ответ модели, который должен сочетать в себе креативность, фактологическую точность, логическую строгость и программный код, генерируется в условиях компромисса, где параметры, идеальные для одной задачи (например, генерации стихов), становятся катастрофическими для другой (например, извлечения финансовых данных).1
Данный отчет предлагает фундаментальный пересмотр архитектуры агентов на основе Принципа №3 Теории Решения Изобретательских Задач (ТРИЗ) — «Местное качество» (Local Quality). Этот принцип постулирует отказ от однородных структур в пользу гетерогенных, требуя, чтобы каждая часть системы функционировала в условиях, наиболее благоприятных для её работы.3 В физическом мире этот принцип находит воплощение в функционально-градиентных материалах (FGM) и аэрокосмических конструкциях с изменяемой геометрией. В цифровом мире агентов это означает переход к Функционально-Градиентному Интеллекту, где когнитивный профиль, среда исполнения и методы верификации динамически изменяются в зависимости от локальной "турбулентности" задачи.
Целью данного исследования является разработка всеобъемлющей архитектурной модели, в которой Агент перестает быть монолитом и становится композитной сущностью. Мы детально рассмотрим три вектора этой трансформации:
1. Гетерогенная структура ответа: Техническая реализация динамического управления энтропией и температурой на уровне токенов и секций.
2. Аэрокосмическая метафора (Адаптация к среде): Использование механизмов переключения среды (текст против кода) как аналога крыла изменяемой стреловидности.
3. Локализация дефектов: Применение квантификации неопределенности для создания зон "повышенной надежности" и маркировки галлюцинаций.
________________


2. Гетерогенная структура ответа: Инженерия когнитивных материалов


Принцип местного качества требует перехода от однородной структуры к неоднородной.4 В контексте генерации текста это означает, что "температура" (параметр стохастичности) и методы декодирования не должны быть глобальными константами сессии, а должны стать локальными переменными, управляемыми контекстом.


2.1. Физика генерации: От статики к динамике


В основе работы LLM лежит предсказание следующего токена на основе распределения вероятностей, полученного после применения функции Softmax к логитам. Температура ($T$) является скалярным делителем логитов ($z_i$):


$$P(x_i) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}$$
Традиционный подход фиксирует $T$ (например, $T=0.7$) для всего ответа. Это создает неустранимое противоречие: высокая температура способствует разнообразию и креативности, но разрушает фактологическую точность и синтаксическую структуру кода. Низкая температура ($T \to 0$) обеспечивает детерминизм, но ведет к зацикливанию и "сухим", шаблонным ответам.1


2.1.1. Динамическое сэмплирование на уровне токенов (Per-Token Dynamics)


Исследования в области динамического температурного сэмплирования 6 показывают, что оптимальная температура коррелирует с энтропией распределения вероятностей.
* Зоны низкой энтропии (Уверенность): Когда модель предсказывает функциональные слова, синтаксические конструкции или общеизвестные факты, распределение вероятностей является "острым" (peaked). В этих зонах введение высокой температуры искусственно уплощает распределение, давая шанс маловероятным и ошибочным токенам. Принцип местного качества требует здесь агрессивного понижения температуры или перехода к жадному декодированию (Greedy Decoding).6
* Зоны высокой энтропии (Неопределенность/Креативность): В моменты перехода между идеями или начала новых абзацев энтропия возрастает. Здесь модель сталкивается с множеством валидных продолжений. Фиксация низкой температуры в этой точке приводит к выбору наиболее банального варианта ("platitude"). Локальное качество требует здесь повышения температуры, чтобы позволить модели исследовать пространство решений.6
Техническая реализация через Logits Processors:
Современные фреймворки инференса, такие как vLLM или Hugging Face Transformers, позволяют внедрять пользовательские обработчики логитов (Logits Processors).9 Мы предлагаем алгоритм Entropy-Guided Temperature Scaling:
1. На каждом шаге $t$ вычисляется энтропия $H(x_t)$ текущего распределения.
2. Определяется целевая температура $T_t$ как функция от $H(x_t)$:
   * Если $H(x_t) < \theta_{low}$ (модель уверена), то $T_t = T_{min}$ (например, 0.1).
   * Если $H(x_t) > \theta_{high}$ (высокая вариативность), то $T_t = T_{max}$ (например, 0.9).
3. Применяется $T_t$ к логитам перед сэмплированием.
Такой подход позволяет создавать ответы, которые являются "твердыми" в фактах и "жидкими" в идеях, имитируя свойства композитных материалов.6


2.2. Архитектура «Скелет Мысли» (Skeleton-of-Thought) как макроструктура


Если динамическое сэмплирование работает на уровне микроструктуры (токенов), то архитектура Skeleton-of-Thought (SoT) 11 реализует принцип местного качества на уровне макроструктуры (секций документа).
Монолитная генерация страдает от последовательной зависимости: ошибка в начале текста может распространиться на последующие части. SoT предлагает разделить процесс на две фазы:
1. Фаза Скелета: Генерация структуры ответа (плана).
2. Фаза Расширения: Параллельная генерация содержимого для каждого пункта плана.


2.2.1. Гетерогенная параметризация пунктов скелета


Главная инновация, которую мы вносим в стандартный SoT, заключается в гетерогенной классификации пунктов скелета. После генерации плана специализированный маршрутизатор (Router) анализирует семантический тип каждого пункта и назначает ему уникальный профиль генерации.13
Таблица 1: Профили локального качества для секций ответа
Тип Секции
	Описание Задачи
	Температура (T)
	Top-P
	Frequency Penalty
	Инструментарий
	Фактология
	Извлечение данных, дат, имен.
	$0.0 - 0.1$
	$0.1$
	$0.0$
	RAG (Strict), Citation Check
	Креатив
	Генерация идей, метафор, стихов.
	$0.8 - 1.1$
	$0.95$
	$0.5$ (высокий)
	Нет (Pure LLM)
	Аналитика
	Логические выводы, сравнения.
	$0.3 - 0.4$
	$0.5$
	$0.1$
	Chain-of-Thought (CoT)
	Код/Математика
	Вычисления, алгоритмы.
	$0.0$
	$0.05$
	$0.0$
	Code Interpreter (Python)
	Введение/Вывод
	Связность, summarization.
	$0.5$
	$0.8$
	$0.2$
	Context Awareness
	Техническая реализация:
Для реализации используется асинхронный вызов API (например, asyncio.gather в Python). Каждый пункт плана становится отдельным запросом к LLM (или инструменту) с собственным системным промптом и параметрами сэмплирования.13 Это позволяет преодолеть ограничения "универсального промпта", так как каждая часть ответа генерируется в изолированной, оптимизированной среде.


2.3. Скриптовая гетерогенность: LMQL и Outlines


Для сценариев, где параллельная генерация невозможна (например, требуется строгая последовательность повествования), принцип местного качества реализуется через Языки Запросов к Моделям (Language Model Query Languages), такие как LMQL 16 и библиотеки структурированной генерации Outlines.18
Эти инструменты позволяют внедрять управляющие конструкции непосредственно в поток генерации, переключая параметры "на лету".


2.3.1. Локальные ограничения (Local Constraints)


Используя LMQL, мы можем задавать ограничения для конкретных переменных внутри шаблона промпта.
* Пример: Генерация финансового отчета.
Python
# Псевдокод LMQL
"Анализ финансовых показателей компании X."
"Краткий обзор:" where temperature=0.6
"Выручка за Q3 составила:" where regex="\$[0-9]+(\.[0-9]{2})?" and temperature=0.0
"Основные риски:" where temperature=0.7 and len(RISKS) < 100

В переменной `` применяется не только нулевая температура, но и Regex-ограничение (Regular Expression). Это гарантирует, что даже если модель "захочет" написать "около пяти миллионов", ограничение принудит её сгенерировать точное числовое значение в заданном формате.19 Это форма "жесткого" локального качества, где мы подменяем вероятностную природу модели детерминированным правилом синтаксиса.
Библиотека Outlines использует подход, основанный на конечно-автоматном декодировании (Finite State Machine Decoding), который маскирует недопустимые токены на уровне логитов. Это позволяет гарантировать валидность JSON или SQL структур в ответе, устраняя необходимость в повторных попытках генерации (retries) и повышая надежность локальных зон.21


2.4. Смесь Промптов (Mixture of Prompts - MoP)


Развивая идею гетерогенности, мы приходим к концепции Mixture of Prompts (MoP).23 Вместо того чтобы пытаться создать один "идеальный" промпт, который будет одновременно строгим и креативным, MoP делит пространство задачи на регионы.
   * Механизм: Входящий запрос анализируется на предмет его компонент.
   * Исполнение:
   * Часть запроса, касающаяся фактов, направляется экспертному промпту, насыщенному few-shot примерами в стиле "Вопрос-Ответ" с жесткими требованиями к цитированию.
   * Часть запроса, касающаяся стиля, направляется промпту с примерами художественной литературы.
   * Синтез: Результаты объединяются.
Исследования показывают, что использование специализированных промптов (экспертов) для подзадач значительно превосходит производительность единого универсального промпта, так как позволяет избежать "интерференции задач" (task interference), когда инструкции по креативности подавляют инструкции по точности.25
________________


3. Аэрокосмическая Метафора: Адаптация к когнитивной турбулентности


ТРИЗ Принцип №3 требует адаптации структуры к внешним условиям.4 В аэрокосмической инженерии это блестяще реализовано в концепции крыла изменяемой стреловидности (Variable Sweep Wing).27
   * Прямое крыло (Unswept): Обеспечивает максимальную подъемную силу на низких скоростях (взлет/посадка), но создает огромное сопротивление на высоких скоростях.
   * Стреловидное крыло (Swept): Минимизирует сопротивление при сверхзвуковом полете, но неэффективно на малых скоростях.
Для ИИ Агента аналогом "скорости" и "сопротивления" является Сложность Запроса и Когнитивная Нагрузка (или "Турбулентность").
   * Низкая турбулентность (Low Turbulence): Простые запросы (болтовня, базовые факты). Здесь "прямое крыло" (стандартная текстовая генерация LLM) наиболее эффективно — быстро и дешево.
   * Высокая турбулентность (High Turbulence): Сложные вычисления, алгоритмическая логика, многоступенчатый анализ данных. Здесь текстовая генерация создает "когнитивное сопротивление" — галлюцинации и логические сбои. Агент должен "изменить стреловидность", переключившись в режим, минимизирующий ошибки.


3.1. Переключение среды: От Текста к Коду (Hard Switch)


Самым радикальным применением локального качества является признание того, что для определенных классов задач (математика, символьная логика, обработка таблиц) текстовая среда LLM является враждебной. Авторегрессионная природа модели, предсказывающей следующий токен, фундаментально не приспособлена для арифметики (проблема токенизации чисел, отсутствие логического АЛУ).29
Принцип: "Каждая часть должна работать в наиболее благоприятных условиях" означает, что математическая часть ответа не имеет права быть сгенерирована как текст.


3.1.1. Парадигма CodeAct и Toolformer


Агент должен обладать способностью динамически переключаться из среды "Natural Language" в среду "Formal Language" (Python/Code Interpreter).
   * CodeAct (Code as Action): Вместо того чтобы генерировать текстовый ответ "Ответ равен 42", модель генерирует исполняемый блок кода Python. Этот код выполняется в изолированной среде (sandbox), и результат выполнения возвращается в контекст модели.31
   * Преимущество: Устранение галлюцинаций в вычислениях. Локальное качество вычисления повышается с вероятностного ($P < 1.0$) до детерминированного ($P = 1.0$, при условии верного кода).
   * Toolformer: Этот подход идет дальше, обучая модель самостоятельно вставлять вызовы API (калькулятор, календарь, поисковик) прямо в текст в процессе генерации.32 Это делает использование инструментов "бесшовным" и локальным.
Механизм «Code Steer»:
Недавние исследования предлагают системы типа Code Steer 34, которые не просто выполняют код, но и используют LLM для итеративной отладки. Если код выдает ошибку, модель "читает" traceback, корректирует скрипт и перезапускает его. Это создает замкнутый контур качества внутри локальной задачи, недоступный при чистой текстовой генерации.


3.2. Композитные архитектуры: Смесь Агентов (Mixture of Agents)


Развивая метафору композитных материалов, мы переходим к архитектуре Mixture of Agents (MoA).35 Композиты (например, углепластик) сочетают в себе матрицу (связующее вещество) и армирующие волокна.
   * Матрица (Matrix): Обеспечивает форму и целостность. В Агенте это роль "Генералиста" (например, GPT-4o), который ведет диалог, понимает контекст и форматирует ответ.
   * Волокна (Fibers): Несут основную нагрузку. В Агенте это специализированные модели.


3.2.1. Гетерогенный ансамбль моделей


MoA позволяет использовать разные модели для разных частей ответа в рамках одного запроса.
   * Синтаксическое волокно (Coder): Если часть запроса требует написания SQL-запроса, эта подзадача маршрутизируется в модель, специализированную на коде (например, DeepSeek-Coder или StarCoder), так как её "локальное качество" в этой области выше.37
   * Медицинское волокно (Medical Expert): Для анализа симптомов подключается Med-PaLM.
   * Скоростное волокно (Router/Chit-chat): Для простых утверждений и связок используется Llama-3-8B (низкая задержка, низкая стоимость).
Layered Architecture (Слоистая архитектура):
MoA часто реализуется послойно. Первый слой агентов генерирует черновики ответов с разных точек зрения (разные промпты или модели). Второй слой агентов-агрегаторов синтезирует эти ответы в единый, высококачественный результат. Это напоминает процесс ламинирования композитных слоев для достижения свойств, недоступных ни одному слою по отдельности.36


3.3. Семантическая маршрутизация (Semantic Routing) как датчик турбулентности


Для того чтобы система работала автономно, необходим "датчик", определяющий, когда нужно "изменить стреловидность крыла" (переключиться на код или другую модель). Эту роль выполняет Семантический Маршрутизатор (Semantic Router).39
Маршрутизатор анализирует векторное представление (embedding) входящего запроса или его части.
   * Если вектор запроса близок к кластеру "Математика/Логика", активируется маршрут Code Interpreter.
   * Если вектор близок к кластеру "Творческое письмо", активируется маршрут High-Temp LLM.
   * Если вектор близок к кластеру "Справочная информация", активируется маршрут RAG.
Исследования показывают, что использование семантической маршрутизации вместо постоянных обращений к самой мощной модели позволяет сократить затраты и время отклика (Latency) до 50 раз, при этом повышая локальное качество ответов за счет специализации.40
________________


4. Устранение недостатков через локализацию: Зоны доверия


ТРИЗ учит нас: если невозможно устранить вредный фактор во всей системе, нужно изолировать его или вытеснить в безопасную зону.42 Галлюцинации LLM являются таким вредным фактором. Создать модель, которая никогда не галлюцинирует, на данном этапе невозможно. Однако мы можем создать архитектуру, которая локализует галлюцинации и маркирует их.


4.1. Квантификация неопределенности (Uncertainty Quantification - UQ)


Для локализации дефектов необходим инструмент "неразрушающего контроля" (Non-Destructive Testing) текста. Этим инструментом является Квантификация Неопределенности.43


4.1.1. Токеновая энтропия vs Семантическая энтропия


   * Токеновая энтропия (Token Entropy): Простой анализ логарифмических вероятностей (logprobs) токенов. Если модель присваивает низкую вероятность выбранному токену, это сигнал тревоги. Однако, часто модель может быть "уверенно неправой" (overconfident), имея низкую энтропию на ошибочном факте.45
   * Семантическая энтропия (Semantic Entropy): Более надежный метод.47 Мы генерируем несколько вариантов ответа (сэмплирование) на один и тот же вопрос. Затем мы кластеризуем ответы по смыслу (используя модель NLI - Natural Language Inference).
   * Если 5 из 5 ответов звучат по-разному, но имеют один смысл (например, "Париж", "Столица Франции", "Город огней") → Высокое доверие.
   * Если ответы противоречат друг другу ("Париж", "Лондон", "Берлин") → Высокая семантическая энтропия (Низкое доверие).


4.2. Конформное предсказание (Conformal Prediction)


Для критических зон ответа (например, медицинский диагноз или юридическая консультация) простого "score" недостаточно. Нам нужны статистические гарантии. Конформное предсказание 49 позволяет преобразовать точечный прогноз модели в множество прогнозов с гарантированным уровнем покрытия (например, 95%).
   * Принцип: Вместо того чтобы заставлять агента выбирать один вариант ("Диагноз: Грипп"), конформный предиктор заставляет его выдавать набор ("Диагноз: {Грипп, COVID-19, ОРВИ}"), если уверенность в одном варианте недостаточна.
   * Применение: Это создает зону Local Quality High, где риск ошибки (error rate) жестко ограничен заданным порогом $\alpha$, ценой снижения информативности (более широкое множество ответов).


4.3. Self-RAG и Токены Рефлексии


Механизм Self-RAG (Self-Reflective Retrieval-Augmented Generation) 52 внедряет локальный контроль качества непосредственно в процесс генерации через специальные Токены Рефлексии.
В процессе обучения модель учится генерировать служебные токены:
   1. ``: Модель сама решает, что ей не хватает знаний для продолжения, и вызывает поиск (RAG).
   2. `` (Is Relevant): После получения контекста модель оценивает, релевантен ли он запросу.
   3. `` (Is Supported): После генерации предложения модель проверяет, подтверждается ли оно найденным контекстом.
   4. [IsUse] (Is Useful): Оценка полезности ответа для пользователя.
Сценарий работы:
Агент пишет: "В 2023 году ВВП вырос на 3%." -> Генерирует токен [Check]. -> Запускается проверка фактов. -> Если факт не подтвержден, срабатывает механизм коррекции (rewriting), и предложение переписывается или помечается как ненадежное. Это создает "петлю обратной связи" внутри генерации, обеспечивая локальную очистку данных.54


4.4. Интерфейс «Слой Доверия» (The Trust Layer UI)


Локализация дефектов бессмысленна, если пользователь о ней не знает. Принцип местного качества требует изменения пользовательского интерфейса. Мы переходим от "плоского текста" к тексту с аннотацией надежности.56
   * Тепловые карты доверия (Confidence Heatmaps): Токены с высокой энтропией или низкой семантической согласованностью подсвечиваются (например, оттенками красного).58
   * Маркировка зон:
   * Зона Фактов (Green): Подтверждена RAG, есть ссылки на источники.
   * Зона Логики (Blue): Подтверждена Code Interpreter.
   * Зона Гипотез (Gray/Italic): Текст сгенерирован с высокой температурой, помечен как "Идея" или "Мнение ИИ".
Такой UI позволяет пользователю "настроить оптику" восприятия: доверять зеленым зонам и скептически относиться к серым, что снижает риск негативных последствий галлюцинаций.60
________________


5. Синтез и Будущее: Композитный Агент


Интеграция рассмотренных технологий приводит нас к новой архитектурной модели Агента, построенной на принципах ТРИЗ.


5.1. Архитектура Композитного Агента


Мы предлагаем следующую структуру, заменяющую монолитный LLM pipeline:
   1. Слой Восприятия (The Sentinel):
   * Входной запрос проходит через Семантический Маршрутизатор, который декомпозирует задачу на подзадачи (Sub-tasks) и оценивает "турбулентность" (сложность/неопределенность).
   2. Слой Планирования (The Architect):
   * Генерируется Skeleton-of-Thought. Каждому пункту плана назначается Исполнительный Профиль:
   * Профиль: Модель (GPT-4 vs Llama-3), Температура (0.0 vs 1.0), Инструмент (Code vs Text).
   3. Слой Исполнения (The Fabricator):
   * Параллельное исполнение подзадач.
   * Ветка А (Анализ данных): Выполняется через Code Interpreter.
   * Ветка B (Креатив): Выполняется через LLM с $T=0.9$.
   * Ветка C (Факты): Выполняется через Self-RAG с жесткой верификацией (``).
   4. Слой Инспекции (The Inspector):
   * Агрегация результатов. Применение Conformal Prediction для оценки уверенности. Если уверенность ниже порога, ветка перезапускается или маркируется как ненадежная.
   5. Слой Интеграции (The Integrator):
   * Сборка гетерогенных частей в единый ответ. Финальное "сглаживание" (Smoothing) переходов между стилями, чтобы избежать ощущения "лоскутного одеяла" (Uncanny Valley of Consistency).


5.2. Экономический и Инженерный Прогноз


Переход к локальному качеству имеет глубокие экономические последствия — Арбитраж Стоимости/Качества (Cost-Quality Arbitrage). Вместо того чтобы обрабатывать 100% токенов дорогой моделью (GPT-4), мы можем обрабатывать 80% (простые связки, общие фразы) дешевой моделью или кэшированными шаблонами, и тратить бюджет вычислительной мощности (Compute Budget) только на 20% критически важных токенов (сложная логика, верификация).62
В будущем роль "Prompt Engineer" (инженера промптов) трансформируется в "Cognitive Architect" (когнитивного архитектора) — специалиста, который проектирует не текст запроса, а топологию маршрутизации, выбирает "материалы" (модели) для разных узлов графа и настраивает параметры "термической обработки" (температуры) для каждого компонента системы.64


5.3. Заключение


Применение Принципа ТРИЗ №3 «Местное качество» к архитектуре ИИ Агентов знаменует собой конец эры "Универсальных Ответчиков". Будущее за Адаптивными Композитными Системами, которые, подобно совершенным аэрокосмическим конструкциям, меняют свою форму, структуру и свойства в ответ на меняющиеся условия среды. Отказ от монолитности в пользу функциональной градиентности — это единственный путь к созданию ИИ, который будет одновременно творческим, точным и надежным.
Источники
   1. Understanding LLM Temperature: Creativity vs. Consistency | by Tahir | Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@tahirbalarabe2/%EF%B8%8Funderstanding-llm-temperature-creativity-vs-consistency-ce2e8194ed7c
   2. LLM Temperature Setting: Control Randomness & Creativity - PromptLayer Blog, дата последнего обращения: ноября 25, 2025, https://blog.promptlayer.com/temperature-setting-in-llms/
   3. 40 TRIZ Principles, дата последнего обращения: ноября 25, 2025, https://www.triz40.com/aff_Principles_TRIZ.php
   4. 40 Principles – Home - triz.org, дата последнего обращения: ноября 25, 2025, https://triz.org/principles/
   5. What is LLM Temperature? - IBM, дата последнего обращения: ноября 25, 2025, https://www.ibm.com/think/topics/llm-temperature
   6. Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2510.08892v1
   7. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2403.14541v1
   8. I need people to test my experiment - Dynamic Temperature : r/LocalLLaMA - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/LocalLLaMA/comments/180b673/i_need_people_to_test_my_experiment_dynamic/
   9. Logits Processors - vLLM, дата последнего обращения: ноября 25, 2025, https://docs.vllm.ai/en/latest/design/logits_processors/
   10. Utilities for Generation - Hugging Face, дата последнего обращения: ноября 25, 2025, https://huggingface.co/docs/transformers/en/internal/generation_utils
   11. Skeleton-of-Thought - Google Sites, дата последнего обращения: ноября 25, 2025, https://sites.google.com/view/sot-llm
   12. Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2307.15337v3
   13. Skeleton-of-Thought: Parallel decoding speeds up and improves LLM output - Microsoft, дата последнего обращения: ноября 25, 2025, https://www.microsoft.com/en-us/research/blog/skeleton-of-thought-parallel-decoding-speeds-up-and-improves-llm-output/
   14. Skeleton-of-Thought Prompting: Faster and Efficient Response Generation, дата последнего обращения: ноября 25, 2025, https://learnprompting.org/docs/advanced/decomposition/skeleton_of_thoughts
   15. Accelerating LLMs with Skeleton-of-Thought Prompting - Portkey, дата последнего обращения: ноября 25, 2025, https://portkey.ai/blog/skeleton-of-thought-prompting/
   16. LLM Temperature: How It Works and When You Should Use It - Vellum AI, дата последнего обращения: ноября 25, 2025, https://www.vellum.ai/llm-parameters/temperature
   17. Language Reference - LMQL, дата последнего обращения: ноября 25, 2025, https://lmql.ai/docs/latest/language/reference.html
   18. outlines 0.0.4 - PyPI, дата последнего обращения: ноября 25, 2025, https://pypi.org/project/outlines/0.0.4/
   19. Welcome to Outlines!, дата последнего обращения: ноября 25, 2025, https://dottxt-ai.github.io/outlines/welcome/
   20. Best way to do guided generation in your experience? : r/LocalLLaMA - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/LocalLLaMA/comments/18iajgp/best_way_to_do_guided_generation_in_your/
   21. dottxt-ai/outlines: Structured Outputs - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/dottxt-ai/outlines
   22. Getting Structured LLM Output - DeepLearning.AI, дата последнего обращения: ноября 25, 2025, https://learn.deeplearning.ai/courses/getting-structured-llm-output/lesson/zv9cy/structured-generation-with-outlines
   23. [ICML 2024] One Prompt is Not Enough: Automated Construction of a Mixture-of-Expert Prompts - TurningPoint AI - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/ruocwang/mixture-of-prompts
   24. One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2407.00256v1
   25. Multiple Prompt Engineering - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@zbabar/multiple-prompt-engineering-dd24eead4143
   26. Beyond the Single Prompt: A Layered Framework for Consistent & Nuanced AI Personas (Seeking Peer Feedback!) : r/PromptEngineering - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/PromptEngineering/comments/1mdkkkx/beyond_the_single_prompt_a_layered_framework_for/
   27. What Is a Variable-Sweep Wing? How Swing Wings Work - Pilot Institute, дата последнего обращения: ноября 25, 2025, https://pilotinstitute.com/variable-sweep-wing/
   28. Variable-sweep wing - Wikipedia, дата последнего обращения: ноября 25, 2025, https://en.wikipedia.org/wiki/Variable-sweep_wing
   29. Code Interpreter: Traditional vs. LLM Use Cases & Top 5 Tools | Obot AI, дата последнего обращения: ноября 25, 2025, https://obot.ai/resources/learning-center/code-interpreter/
   30. Tutorial: Use code interpreter sessions in Semantic Kernel with Azure Container Apps, дата последнего обращения: ноября 25, 2025, https://learn.microsoft.com/en-us/azure/container-apps/sessions-tutorial-semantic-kernel
   31. Executable Code Actions Elicit Better LLM Agents - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2402.01030v1
   32. Vinija's Notes • Models • Toolformer, дата последнего обращения: ноября 25, 2025, https://vinija.ai/models/Toolformer/
   33. Arxiv Dives — Toolformer: Language models can teach themselves to use tools - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@oxenai/arxiv-dives-toolformer-language-models-can-teach-themselves-to-use-tools-b50c9312c2a9
   34. This “smart coach” helps LLMs switch between text and code | MIT News, дата последнего обращения: ноября 25, 2025, https://news.mit.edu/2025/smart-coach-helps-llms-switch-between-text-and-code-0717
   35. Stop building on single LLMs - start using a mixture of expert models instead - Ema, дата последнего обращения: ноября 25, 2025, https://www.ema.co/additional-blogs/addition-blogs/mixture-of-agents-enhancing-large-language-model-capabilities
   36. Mixture-of-Agents Enhances Large Language Model Capabilities - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2406.04692v1
   37. Applying Mixture of Experts in LLM Architectures | NVIDIA Technical Blog, дата последнего обращения: ноября 25, 2025, https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/
   38. Mixture of Agents (MoA): a better MoE? | by Abdulrahman Hesham | Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@abdulrahmanrihan/mixture-of-agents-moa-a-better-moe-33683151beec
   39. AI Agent Routing: Tutorial & Examples - FME by Safe Software, дата последнего обращения: ноября 25, 2025, https://fme.safe.com/guides/ai-agent-architecture/ai-agent-routing/
   40. Semantic Routing for Enhanced Performance of LLM-Assisted Intent-Based 5G Core Network Management and Orchestration - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2404.15869v1
   41. How to Build an AI Agent With Semantic Router and LLM Tools - The New Stack, дата последнего обращения: ноября 25, 2025, https://thenewstack.io/how-to-build-an-ai-agent-with-semantic-router-and-llm-tools/
   42. TRIZ - The Theory of Inventive Problem Solving - Six Sigma Study Guide, дата последнего обращения: ноября 25, 2025, https://sixsigmastudyguide.com/theory-of-inventive-problem-solving-triz/
   43. Efficient and Effective Uncertainty Quantification in LLMs - Apple Machine Learning Research, дата последнего обращения: ноября 25, 2025, https://machinelearning.apple.com/research/efficient-and-effective
   44. Uncertainty Quantification for Large Language Models - ACL Anthology, дата последнего обращения: ноября 25, 2025, https://aclanthology.org/2025.acl-tutorials.3/
   45. Using logprobs | OpenAI Cookbook, дата последнего обращения: ноября 25, 2025, https://cookbook.openai.com/examples/using_logprobs
   46. Confidence Unlocked: A Method to Measure Certainty in LLM Outputs - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@vatvenger/confidence-unlocked-a-method-to-measure-certainty-in-llm-outputs-1d921a4ca43c
   47. Efficient and Effective Uncertainty Quantification for LLMs - OpenReview, дата последнего обращения: ноября 25, 2025, https://openreview.net/forum?id=QKRLH57ATT
   48. Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph | Transactions of the Association for Computational Linguistics - MIT Press Direct, дата последнего обращения: ноября 25, 2025, https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00737/128713/Benchmarking-Uncertainty-Quantification-Methods
   49. [2502.12601] COPU: Conformal Prediction for Uncertainty Quantification in Natural Language Generation - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/abs/2502.12601
   50. Conformal Prediction for Natural Language Processing: A Survey - ACL Anthology, дата последнего обращения: ноября 25, 2025, https://aclanthology.org/2024.tacl-1.82/
   51. Conformal Prediction for Natural Language Processing: A Survey - MIT Press Direct, дата последнего обращения: ноября 25, 2025, https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00715/125278/Conformal-Prediction-for-Natural-Language
   52. Self RAG (Retrieval Augmented Generation) - GeeksforGeeks, дата последнего обращения: ноября 25, 2025, https://www.geeksforgeeks.org/artificial-intelligence/self-rag-retrieval-augmented-generation/
   53. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/abs/2310.11511
   54. The Self-RAG Shortcut Every AI Expert Wishes They Knew - ProjectPro, дата последнего обращения: ноября 25, 2025, https://www.projectpro.io/article/self-rag/1176
   55. Advanced RAG Techniques — The Self-RAG strategy | by Gabriel Gomes, PhD - Medium, дата последнего обращения: ноября 25, 2025, https://gabrielgomes61320.medium.com/advanced-rag-techniques-the-self-rag-strategy-7f3b367cb657
   56. Top 10 Agentic AI Design Patterns | Enterprise Guide - Aufait UX, дата последнего обращения: ноября 25, 2025, https://www.aufaitux.com/blog/agentic-ai-design-patterns-enterprise-guide/
   57. 5 Ways To Build a Trustworthy AI Agent - Salesforce, дата последнего обращения: ноября 25, 2025, https://www.salesforce.com/blog/trustworthy-ai-agent/
   58. Learning to Route LLMs with Confidence Tokens - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2410.13284v2
   59. Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2511.09980v1
   60. Detect hallucinations for RAG-based systems | Artificial Intelligence - AWS, дата последнего обращения: ноября 25, 2025, https://aws.amazon.com/blogs/machine-learning/detect-hallucinations-for-rag-based-systems/
   61. What are AI Hallucinations? How to Test? - testRigor AI-Based Automated Testing Tool, дата последнего обращения: ноября 25, 2025, https://testrigor.com/blog/ai-hallucinations/
   62. Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning | OpenReview, дата последнего обращения: ноября 25, 2025, https://openreview.net/forum?id=4FWAwZtd2n
   63. TAO: Using test-time compute to train efficient LLMs without labeled data | Databricks Blog, дата последнего обращения: ноября 25, 2025, https://www.databricks.com/blog/tao-using-test-time-compute-train-efficient-llms-without-labeled-data
   64. The Router Pattern: A Smarter Way to Build AI Agents, дата последнего обращения: ноября 25, 2025, https://brianjenney.medium.com/the-router-pattern-a-smarter-way-to-build-ai-agents-dbdd2ee12656
   65. Agentic AI Workflows Design Patterns, Examples, and what to watch in 2025 | by Shanding P. G | CodeX, дата последнего обращения: ноября 25, 2025, https://medium.com/codex/agentic-ai-workflows-design-patterns-examples-and-what-to-watch-in-2025-a3602b19b7e8