АРХИТЕКТУРА НЕПОСТОЯНСТВА: ГЛУБОКОЕ ИССЛЕДОВАНИЕ ПРИМЕНЕНИЯ ПРИНЦИПА TRIZ №27 В ПРОЕКТИРОВАНИИ АВТОНОМНЫХ ИНТЕЛЛЕКТУАЛЬНЫХ АГЕНТОВ




Аннотация


В современной парадигме разработки больших языковых моделей (LLM) и агентных систем доминирует подход, ориентированный на персистентность: сохранение истории диалога, накопление знаний в векторных базах данных и использование монолитных системных промптов. Данный отчет бросает вызов этой ортодоксии, предлагая архитектурный сдвиг, основанный на Принципе ТРИЗ №27 «Дешевые кратковременные объекты» (Cheap Short-Living Objects). Исследование детально рассматривает механизмы замены дорогих, долговечных вычислительных ресурсов и контекстов множеством специализированных, эфемерных сущностей. Анализируются три ключевых направления: (1) Эфемерные цепочки мышления (Disposable Chain of Thought) для снижения когнитивной нагрузки и стоимости, (2) Метафора «Подвесных баков» (Drop Tanks) для управления внешней памятью и приватностью данных, и (3) Абляционная защита (Ablative Shielding) и каскадные архитектуры (FrugalGPT) для оптимизации безопасности и баланса «цена-качество». В работе демонстрируется, как переход от стратегии «хранить вечно» к стратегии «использовать и выбросить» позволяет преодолеть ограничения контекстного окна, устранить эффект «потери в середине» (Lost-in-the-Middle) и создать экономически эффективные, масштабируемые агентные системы.
________________


1. Введение: Кризис Цифрового Бессмертия и Необходимость Эфемерности




1.1. Бремя бесконечного контекста в эпоху Generative AI


Развитие агентных систем искусственного интеллекта достигло критической точки, характеризующейся парадоксом памяти. С одной стороны, мы стремимся к тому, чтобы агенты «помнили все» — предпочтения пользователя, детали предыдущих сессий, корпоративные базы знаний. С другой стороны, техническая и экономическая реальность накладывает жесткие ограничения на этот идеал. Увеличение контекстного окна до 100 000 или даже 1 000 000 токенов, хотя и является инженерным достижением, создает новые проблемы: экспоненциальный рост стоимости инференса, линейное увеличение задержки (latency) и, что наиболее критично, деградацию качества рассуждений, известную как феномен «Lost-in-the-Middle».1
Традиционная архитектура агента напоминает «керамическую чашку» из классической теории решения изобретательских задач (ТРИЗ): это дорогой, долговечный объект, который необходимо «мыть» (чистить контекст), «хранить» (платить за базы данных) и оберегать от повреждений (защищать от джейлбрейков). В программной инженерии этот подход проявляется в создании монолитных агентов с огромными системными промптами, которые пытаются быть универсальными специалистами во всем — от написания кода до поэзии. Такой подход становится экономически несостоятельным при масштабировании.


1.2. Принцип ТРИЗ №27 как архитектурный фундамент


Принцип №27 «Дешевые кратковременные объекты» предлагает радикальную альтернативу: заменить дорогой объект набором дешевых, одноразовых объектов, пожертвовав некоторыми качествами (например, долговечностью) ради функциональности и эффективности.2 В контексте AI-агентов это означает переход к Архитектуре Непостоянства (Architecture of Impermanence).
Данная архитектура базируется на следующих постулатах:
1. Вычисления эфемерны: Процесс мышления не должен засорять память результата.
2. Данные ситуативны: Информация полезна только в момент решения задачи; ее долгосрочное хранение создает риски приватности и когнитивный шум.
3. Агенты расходные: Личность и функции агента должны инстанцироваться под задачу и уничтожаться после ее выполнения.
Этот отчет исследует, как именно принцип №27 трансформирует каждый слой стека AI-агентов — от управления памятью и безопасностью до оркестрации моделей. Мы переходим от статических, персистентных систем к динамическим, потоковым архитектурам, где объекты существуют ровно столько, сколько необходимо для выполнения микро-операции.
________________


2. Эфемерные Цепочки Мышления (Disposable Chain of Thought)


Центральной проблемой современных LLM является «загрязнение контекста» (Context Pollution). Методология Chain of Thought (CoT) — пошаговое рассуждение — значительно улучшает качество ответов, но генерирует огромный объем промежуточных токенов. Эти токены, являясь по сути «черновиком», попадают в историю диалога, размывая внимание модели при последующих запросах и увеличивая стоимость каждого следующего токена.


2.1. Техническая реализация механизма «Одноразовых блокнотов»


Концепция «Одноразового блокнота» (Disposable Notebook) подразумевает, что агент должен иметь пространство для «грязных» вычислений, которое невидимо для пользователя и не сохраняется в долгосрочной памяти диалога. Это прямой аналог использования бумажных салфеток для промежуточных расчетов — они выбрасываются сразу после получения ответа.


2.1.1. Архитектура Fork-Join-Prune


Реализация данного механизма требует перехода от линейной цепочки сообщений к графовой структуре, поддерживаемой такими инструментами, как LangGraph или Maisa's Knowledge Processing Unit (KPU).5
Архитектурный паттерн выглядит следующим образом:
1. Ветвление (Fork): При поступлении сложного запроса (например, «Рассчитай оптимальную стратегию инвестирования»), основной агент (Orchestrator) не генерирует ответ напрямую. Он инициирует параллельный процесс — «мыслительный поток».
2. Изолированное исполнение: В этом потоке создается временный объект состояния. Агент генерирует серию промежуточных шагов (CoT), вызывает инструменты (калькулятор, поиск), получает ошибки, исправляет их (Self-Correction). Вся эта активность генерирует тысячи токенов.7
3. Синтез результата: Как только решение найдено, специальный узел графа (Synthesizer) извлекает финальный ответ и формирует чистое, лаконичное сообщение.
4. Абляционное удаление (Prune): Это критический шаг, реализующий принцип №27. Система выполняет принудительную очистку состояния. В терминологии LangChain/LangGraph это реализуется через механизм RemoveMessage. Все сообщения, помеченные как «промежуточные мысли» (intermediate thoughts), удаляются из графа истории. Для системы они перестают существовать.9
Этап процесса
	Статус данных
	Принцип ТРИЗ №27
	Инициация
	Персистентный
	Объект создан
	Рассуждение (CoT)
	Эфемерный
	Дешевый, короткоживущий объект
	Результат
	Персистентный
	Полезный остаток
	История диалога
	Очищенный
	Долговечность принесена в жертву чистоте
	

2.1.2. Параллелизм и Tree of Thoughts


Использование дешевых, короткоживущих объектов позволяет реализовать стратегию «Дерева мыслей» (Tree of Thoughts) без экспоненциального роста затрат памяти. Агент может одновременно запустить 5-10 параллельных «одноразовых мыслителей», каждый из которых пробует свою стратегию решения задачи.
* Мыслитель А пытается решить задачу аналитически.
* Мыслитель Б пишет код на Python.
* Мыслитель В ищет прецеденты в базе знаний.
Победитель определяется узлом-валидатором (Judge). После выбора лучшего решения, все ветви, включая победную (в ее сыром виде), уничтожаются. В основную память попадает только «золотой» ответ. Это радикально отличается от линейного сохранения всех попыток, которое быстро переполняет контекстное окно модели.11


2.2. От «Дорогого универсала» к «Рою дешевых специалистов»


Традиционный подход к созданию персоны агента заключается в написании «Божественного Промпта» (God Prompt) — огромной системной инструкции, описывающей все возможные сценарии поведения, тон голоса, ограничения и знания. Такой промпт является «дорогим и долговечным» объектом. Он потребляет ресурсы при каждом запросе и сложен в поддержке.


2.2.1. Динамическая инстанциация микро-агентов


Принцип №27 диктует замену этого монолита на рой микро-агентов. Вместо одного агента, который умеет быть и юристом, и программистом, и поэтом, система содержит библиотеку из сотен микро-промптов (специалистов).
Алгоритм работы:
1. Маршрутизация (Router): На входе стоит сверхлегкая модель (например, классификатор на базе BERT или дистиллированная LLM), которая определяет интент пользователя.
2. Инстанциация (Spawn): Если пользователь просит написать код, система на лету создает агента «Программист», загружая в контекст только специфические инструкции для кодинга (200-300 токенов). Если следующий запрос касается юридических рисков, агент «Программист» уничтожается, и создается агент «Юрист».12
3. Утилизация: После завершения микро-диалога специализированный агент удаляется.
Это снижает «налог на входные токены» (Input Token Tax). Мы не платим за инструкции по написанию стихов, когда решаем математическую задачу.


2.2.2. Синтезируемые промпты (Synthesized Prompts)


Еще более радикальное применение принципа — отказ от заранее написанных промптов вообще. Использование мета-промптинга позволяет генерировать системную инструкцию в момент запроса.
* Запрос: «Помоги мне починить карбюратор от ВАЗ-2107».
* Действие: Оркестратор генерирует одноразовый системный промпт: «Ты — эксперт по советской автомеханике 80-х годов, специализирующийся на карбюраторных двигателях. Твой тон — прагматичный и технический...».
* Этот промпт живет ровно одну сессию и исчезает. Он идеально подогнан под задачу (High Fit), дешев в создании (Low Cost) и не требует хранения (Short-Living).15


2.3. Maisa KPU и разделение вычислений


Интересным примером индустриальной реализации данного подхода является Knowledge Processing Unit (KPU) компании Maisa.5 Их архитектура явно отделяет «рассуждение» (reasoning) от «исполнения» (execution). В отличие от текстовых CoT, которые смешиваются с данными, KPU оркестрирует шаги ИИ как исполняемый код. Это позволяет создавать детерминированные, проверяемые цепочки действий, которые не засоряют контекст нарративом, а существуют как временные вычислительные графы. Это технологическое воплощение идеи о том, что процесс мышления должен быть отделен от результата и утилизирован после использования.
________________


3. Аэрокосмическая Метафора: Подвесные баки (Drop Tanks) и Абляция


Аэрокосмическая инженерия — идеальный домен для поиска аналогий принципу №27. Для достижения орбиты ракета сбрасывает отработанные ступени (Drop Tanks), чтобы не тащить мертвый груз. Для возвращения на Землю используется абляционная защита (Ablative Shielding), которая сгорает (разрушается), поглощая тепло и спасая капсулу.


3.1. Drop Tanks: Внешняя память как одноразовый ресурс


В стандартной архитектуре RAG (Retrieval Augmented Generation) пользовательские данные часто индексируются в персистентные векторные базы данных (Pinecone, Weaviate). Это создает эффект «цифрового накопительства»: данные лежат годами, требуя оплаты за хранение и создавая риски утечки.


3.1.1. Паттерн InMemoryVectorStore


Применение принципа «подвесных баков» подразумевает использование In-Memory Vector Stores (эфемерных векторных хранилищ), живущих только в оперативной памяти (RAM) во время сессии.16
Сценарий использования:
1. Загрузка (Взлет): Пользователь загружает PDF с квартальным отчетом.
2. Индексация (Подключение бака): Файл мгновенно чанкуется и векторизуется в локальный индекс (Chroma или FAISS), развернутый прямо в контейнере инференса.
3. Маневрирование: Агент использует этот индекс для ответов на вопросы пользователя. Данные доступны с микросекундной задержкой (так как они в RAM), что значительно быстрее сетевых запросов к облачным БД.19
4. Сброс бака: Как только пользователь меняет тему или закрывает сессию, объект InMemoryVectorStore уничтожается сборщиком мусора Python. Данные физически исчезают из памяти.
Преимущества:
* Приватность (Goldfish Effect): Данные существуют только в моменте. Нет риска, что конфиденциальный отчет, загруженный сегодня, «всплывет» в галлюцинации агента через месяц в разговоре с другим пользователем. Эффект «Золотой рыбки» (короткая память) становится фичей безопасности, а не багом.21
* Экономия: Отсутствуют затраты на долгосрочное хранение векторов.


3.1.2. Just-in-Time Knowledge Graphs


Для более сложных задач можно создавать одноразовые графы знаний. Агент сканирует документ, строит временный граф сущностей и связей для решения конкретной аналитической задачи, а затем удаляет его. Это позволяет использовать мощь графовых алгоритмов без необходимости поддерживать гигантскую онтологию всего мира.22


3.2. Абляционная защита: Слой одноразовых ответов


Агенты постоянно подвергаются атакам: промпт-инъекции, попытки джейлбрейка, токсичный контент. Если основной агент (Core Agent) будет обрабатывать каждую такую атаку, пытаясь «аргументированно отказать», это приведет к:
1. Трате дорогих токенов GPT-4.
2. Риску «пробоя» системного промпта (чем больше агент говорит с атакующим, тем выше шанс, что он поддастся на манипуляцию).
Решение — Абляционный слой защиты (Ablative Shielding Layer).


3.2.1. Архитектура «Жертвенных Агентов» (Sacrificial Agents)


Мы создаем внешний периметр из дешевых, «глупых» моделей (например, Llama-Guard, BERT-классификаторов или даже regex-скриптов), задача которых — принять удар на себя.23
Механизм работы:
1. Перехват: Входящее сообщение сначала попадает в абляционный слой.
2. Анализ и Сгорание: Если модель-защитник (Guardrail) обнаруживает атаку (jailbreak, toxicity), она не передает сообщение основному агенту. Вместо этого она генерирует дешевый шаблонный ответ («Запрос отклонен») и блокирует поток.
3. Изоляция: Основной агент даже не знает, что атака произошла. Его контекстное окно остается чистым. Взаимодействие с атакующим происходит через «одноразового» бота, которого не жалко.25
Это отличается от встроенного отказа (refusal), когда сама модель отказывается отвечать. Здесь отказ происходит до модели, на уровне архитектуры. Это похоже на «Векторную абляцию отказа» (Refusal-Vector Ablation), но реализованную не через изменение весов модели, а через архитектурный паттерн.27


3.2.2. Yaw Damper: Демпфер рыскания для тем диалога


В авиации демпфер рыскания (Yaw Damper) автоматически гасит нежелательные колебания курса. В диалоговых системах мы можем использовать аналогичный механизм для предотвращения «дрейфа темы» (Topic Drift).29
Специализированный, короткоживущий микро-агент мониторит соответствие текущего запроса заявленной цели сессии. Если обнаруживается отклонение (дрейф), этот агент вмешивается, мягко возвращая разговор в русло («Это интересно, но давайте вернемся к оформлению ипотеки»), и затем отключается. Это предотвращает ситуацию, когда банковский бот после 20 сообщений начинает обсуждать рецепты пиццы.30
________________


4. Снижение качества ради скорости: FrugalGPT и Экономика Компромисса


Принцип ТРИЗ №27 явно допускает «компромисс с некоторыми качествами (например, долговечностью)». В контексте генеративного ИИ это означает отказ от идеи, что каждый ответ должен быть сгенерирован самой мощной и дорогой моделью (SOTA). Мы можем торговать качеством и глубиной ради скорости и цены.


4.1. Парадигма FrugalGPT: Каскадная маршрутизация


Концепция FrugalGPT 12 — это прямое воплощение идеи использования дешевых объектов. Вместо того чтобы отправлять все запросы в GPT-4o, система использует каскад моделей.
Алгоритм каскада:
1. Уровень 1 (Дешевый объект): Запрос обрабатывается сверхлегкой моделью (например, Llama-3-8B или GPT-4o-mini).
2. Оценка (Scorer): Специализированный легковесный ревокер (или скоринговая функция) оценивает уверенность ответа.
3. Решение:
   * Если уверенность высока (простой фактологический вопрос), ответ возвращается пользователю. Дорогая модель даже не просыпалась.
   * Если уверенность низка, запрос эскалируется на Уровень 2 (Средняя модель) или сразу на Уровень 3 (SOTA-модель).
Такой подход позволяет снизить стоимость инференса на 60-80% без существенной потери качества для конечного пользователя, так как большинство запросов (приветствия, простые вопросы) не требуют гениальности.33


4.2. RouteLLM и обучение маршрутизатора


Эффективность каскада зависит от маршрутизатора (Router). Исследования RouteLLM 14 показывают, что можно обучить небольшой классификатор предсказывать, какая модель справится с запросом, используя данные о предпочтениях людей. Это позволяет динамически выбирать «инструмент» (модель) под задачу, максимизируя использование «дешевых объектов».


4.3. Экспресс-ответы и управление ожиданиями


Мы можем пойти дальше и сделать этот компромисс явным для пользователя (UX-паттерн).
Агент может предложить два режима:
* «Экспресс» (Short-Living): Ответ генерируется мгновенно дешевой моделью. Он маркируется как «Предварительный» или «Черновой». Подразумевается, что этот ответ не долговечен — он нужен здесь и сейчас для быстрой оценки.
* «Глубокий анализ» (Long-Living): Агент запускает полноценный процесс CoT, использует поиск, проверку фактов. Ответ будет дорогим, медленным, но «долговечным» (надежным).
Это внедряет концепцию «Адаптивной глубины верификации» (Adaptive Verification Depth).36 Для критических задач (код, медицина) мы используем дорогие объекты. Для болтовни — дешевые.


4.4. Стратегии Think-at-Hard (TaH)


Помимо выбора модели, можно динамически управлять объемом вычислений внутри одной модели. Стратегии Think-at-Hard 38 подразумевают аллокацию вычислительных ресурсов в зависимости от сложности токена или запроса. Если система детектирует сложный логический переход, она активирует дополнительные слои обработки или генерирует больше токенов рассуждения. Если переход тривиален — вычисления минимизируются. Это адаптивная архитектура, где «глубина мышления» является переменной величиной, регулируемой в реальном времени.
________________


5. Архитектурные Паттерны и Реализация


Для внедрения этих принципов необходимы конкретные программные паттерны. Ниже представлены ключевые механизмы реализации на базе современных фреймворков (LangChain, LangGraph).


5.1. Паттерн «Сборщик Мусора» (Garbage Collector Agent)


В персистентных чатах накапливается «мусор»: приветствия, обсуждение погоды, устаревшие данные. Вместо того чтобы хранить это вечно, можно запустить фоновый процесс (микро-агент), который периодически сканирует историю и выполняет семантическую сборку мусора.
Он не просто удаляет старые сообщения, а «сжимает» их в краткие самари (Summary), удаляя исходные многословные диалоги. Таким образом, «дорогие» токены истории заменяются «дешевым» сжатым представлением.39


5.2. Динамическая генерация инструментов (Dynamic Tool Generation)


Вместо того чтобы загружать агента сотней статических инструментов (калькулятор, погода, поиск акций), мы можем использовать паттерн LLM-Generated Tools.41
1. Агент понимает, что ему нужно выполнить специфическую трансформацию данных (например, конвертировать CSV определенного формата в JSON).
2. Вместо поиска готового инструмента, агент пишет Python-скрипт для этой задачи прямо в рантайме.
3. Скрипт исполняется в песочнице.
4. Скрипт удаляется.
Инструмент (скрипт) был «дешевым, кратковременным объектом». Он существовал ровно 2 секунды, выполнил функцию и исчез. Это дает агенту бесконечную гибкость без раздувания кодовой базы.


5.3. Использование Conditional Edges для Абляции


В LangGraph можно использовать условные ребра (conditional_edges) для реализации логики сброса состояния.


Python




def route_and_prune(state):
   # Логика завершения задачи
   if state["status"] == "ready_to_answer":
       # ПРИНЦИП 27: Удаляем всю "кухню" (intermediate_steps) перед финальным ответом
       return Command(
           goto="END",
           update={"scratchpad": RemoveMessage(id="ALL")} 
       )
   return "continue_reasoning"

Этот код демонстрирует, как программно реализовать «забывание» ненужной информации перед переходом к следующему шагу.9


5.4. Адаптивный RAG (Adaptive RAG)


Использование классификатора сложности запроса для выбора стратегии поиска (No Retrieval, Single-step, Multi-step).44
* Простой вопрос: «Столица Франции?» -> Прямой ответ LLM (No Retrieval).
* Средний: «Кто выиграл ЧМ 2022?» -> Одиночный поиск (Single-step).
* Сложный: «Сравни экономические показатели компаний А и Б за 2023 год» -> Мульти-поиск с планированием (Multi-step).
Это позволяет не расходовать ресурсы RAG (время, деньги на эмбеддинг) там, где они не нужны.
________________


6. Будущее: Парадигма Одноразового ПО (Disposable Software)


Применение Принципа №27 в ИИ — это предвестник более глобального сдвига в разработке ПО. Мы движемся к эре Disposable Software («Одноразового программного обеспечения»).46
В традиционной разработке мы пишем код, чтобы поддерживать его годами (Refactoring, CI/CD, Legacy). В эре мощных агентов стоимость написания кода стремится к нулю. Становится выгоднее попросить ИИ написать микро-приложение для конкретной задачи здесь и сейчас, использовать его один раз и выбросить, чем пытаться создать и поддерживать универсальный «швейцарский нож».


6.1. Vibe Coding и Эфемерность


Феномен «Vibe Coding» (интуитивное программирование с помощью LLM) поддерживает этот тренд. Разработчики все чаще создают скрипты-однодневки, которые решают текущую проблему и не предназначены для репозитория. Агенты, построенные на принципе №27, будут архитектурно готовы к такому миру: они будут создавать свои собственные инструменты, свои собственные интерфейсы и свои собственные структуры данных на лету, оптимизируя их под текущий контекст, а не под будущее наследие.


6.2. Приватность как побочный эффект


Переход к короткоживущим объектам имеет колоссальные последствия для кибербезопасности и приватности. Если данные (Drop Tanks) и мыслительные процессы (Disposable CoT) живут только в оперативной памяти и уничтожаются после сессии, поверхность атаки для утечек данных радикально сокращается. «Хранение» становится уязвимостью, а «Поток» — защитой.
________________


7. Заключение


Интеграция принципа ТРИЗ №27 «Дешевые кратковременные объекты» в архитектуру ИИ-агентов — это не просто оптимизация ресурсов, это фундаментальная смена философии проектирования. Мы отказываемся от антропоморфной иллюзии «памяти» как ценности и переходим к инженерной реальности «процессинга» как функции.
Ключевые выводы:
   1. Забудьте, чтобы помнить: Агенты должны активно забывать промежуточные шаги (CoT Ablation), чтобы сохранять фокус на главном.
   2. Данные — это топливо, а не багаж: Используйте эфемерные векторные индексы (Drop Tanks) для работы с документами.
   3. Защита через самопожертвование: Используйте дешевые модели-щиты для фильтрации токсичности, оберегая ядро системы.
   4. Каскады решают: Не стреляйте из пушки по воробьям. FrugalGPT и адаптивная маршрутизация — ключ к экономике масштаба.
Будущее за системами, которые умеют быть легкими, быстрыми и, что самое важное, непостоянными.
________________


Сравнительная таблица: Традиционная архитектура vs. Архитектура непостоянства


Характеристика
	Традиционная Архитектура (Persistent)
	Архитектура Непостоянства (Ephemeral / TRIZ 27)
	Память (Context)
	Append-only (Добавляем всё)
	Prune-heavy (Удаляем всё лишнее)
	Векторная БД
	Персистентная (Pinecone, Weaviate)
	Эфемерная (In-Memory, Chroma, FAISS)
	Цепочка мыслей
	Сохраняется в истории
	Уничтожается после получения ответа
	Выбор модели
	Одна мощная модель (GPT-4)
	Каскад моделей (FrugalGPT)
	Безопасность
	Встроена в системный промпт
	Внешний слой «сгорающих» гардраилов
	Инструменты
	Статическая библиотека
	Динамическая генерация (LLM-generated tools)
	Стоимость
	Растет с длиной диалога
	Стабильна / Оптимизирована
	Конец отчета
Источники
   1. Steering Conversational Large Language Models for Long Emotional Support Conversations - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2402.10453v2
   2. DIKWP-TRIZ: A Revolution on Traditional TRIZ Towards Invention for Artificial Consciousness - MDPI, дата последнего обращения: ноября 25, 2025, https://www.mdpi.com/2076-3417/14/23/10865
   3. 40 TRIZ Principles, дата последнего обращения: ноября 25, 2025, https://www.triz40.com/aff_Principles_TRIZ.php
   4. 40 Inventive Principles of TRIZ: A Practical Guide for Process Innovation, дата последнего обращения: ноября 25, 2025, https://leanoutsidethebox.com/40-inventive-principles-of-triz/
   5. What It Really Takes To Make AI Agents Work - NFX, дата последнего обращения: ноября 25, 2025, https://www.nfx.com/post/onboarding-ai-agents
   6. How agent-oriented design patterns transform system development - Outshift | Cisco, дата последнего обращения: ноября 25, 2025, https://outshift.cisco.com/blog/how-agent-oriented-design-patterns-transform-system-development
   7. Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2504.07952v1
   8. A Self-Improving Coding Agent - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2504.15228v1
   9. How to manage conversation history in a ReAct Agent - GitHub Pages, дата последнего обращения: ноября 25, 2025, https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-manage-message-history/
   10. Message flow - CopilotKit Docs, дата последнего обращения: ноября 25, 2025, https://docs.copilotkit.ai/langgraph/concepts/message-management
   11. Reflection Agents - LangChain Blog, дата последнего обращения: ноября 25, 2025, https://blog.langchain.com/reflection-agents/
   12. Cascadia: An Efficient Cascade Serving System for Large Language Models - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2506.04203v2
   13. Router-R1 and LLM routing research - Champaign Magazine, дата последнего обращения: ноября 25, 2025, https://champaignmagazine.com/2025/10/16/router-r1-and-llm-routing-research/
   14. RouteLLM: Balancing Cost and Quality in LLM Deployments - Zilliz Learn, дата последнего обращения: ноября 25, 2025, https://zilliz.com/learn/routellm-open-source-framework-for-navigate-cost-quality-trade-offs-in-llm-deployment
   15. Synthesized Prompts (SHIP) Overview - Emergent Mind, дата последнего обращения: ноября 25, 2025, https://www.emergentmind.com/topics/synthesized-prompts-ship
   16. LangGraph RAG: Build Agentic Retrieval‑Augmented Generation - Leanware, дата последнего обращения: ноября 25, 2025, https://www.leanware.co/insights/langgraph-rag-agentic
   17. Build a RAG agent with LangChain, дата последнего обращения: ноября 25, 2025, https://docs.langchain.com/oss/python/langchain/rag
   18. Vector Databases Guide: RAG Applications 2025 - DEV Community, дата последнего обращения: ноября 25, 2025, https://dev.to/klement_gunndu_e16216829c/vector-databases-guide-rag-applications-2025-55oj
   19. Vector Databases vs. In-Memory Databases - Zilliz blog, дата последнего обращения: ноября 25, 2025, https://zilliz.com/blog/vector-database-vs-in-memory-databases
   20. Guide to Chroma DB: A Vector Store for Your Generative AI LLMs - Analytics Vidhya, дата последнего обращения: ноября 25, 2025, https://www.analyticsvidhya.com/blog/2023/07/guide-to-chroma-db-a-vector-store-for-your-generative-ai-llms/
   21. RAG Models: The “Goldfish Effect” of Enterprise AI | by Inclusion Cloud | Nov, 2025 | Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@inclusion.cloud/rag-models-the-goldfish-effect-of-enterprise-ai-cf65e583d649
   22. Space Exploration Systems, Strategies and Solutions - Politecnico di Torino, дата последнего обращения: ноября 25, 2025, https://iris.polito.it/retrieve/e384c42e-316a-d4b2-e053-9f05fe0a1d67/Space_exploration_Systems_Strategies_and_Solutions.pdf
   23. Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2510.13351v1
   24. Andrei Kucharavy · Octave Plancherel · Valentin Mulder · Alain Mermoud · Vincent Lenders Editors Threats, Expo, дата последнего обращения: ноября 25, 2025, https://sec.cafe/handbook/pdf/LLM_in_cyber_security__1730205794.pdf
   25. Fortifying the Future: A Comprehensive Blueprint for Safeguarding AI Agents Against Cyber Threats | by Dr. Armando Fandango - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/secure-agentic-ai/fortifying-the-future-a-comprehensive-blueprint-for-safeguarding-ai-agents-against-cyber-threats-b798a3ff5b5d
   26. KV Cache Reuse for NemoGuard NIM - NVIDIA Docs Hub, дата последнего обращения: ноября 25, 2025, https://docs.nvidia.com/nemo/guardrails/latest/user-guides/advanced/kv-cache-reuse.html
   27. Applying Refusal-Vector Ablation to Llama 3.1 70B Agents - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2410.10871v1
   28. Refusal Vector Ablation in LLMs - Medium, дата последнего обращения: ноября 25, 2025, https://kaushiksp.medium.com/refusal-vector-ablation-in-llms-35aa646ff4a9
   29. Flying Qualities Flight Testing of Digital Flight Control Systems - DTIC, дата последнего обращения: ноября 25, 2025, https://apps.dtic.mil/sti/tr/pdf/ADA398738.pdf
   30. Examining Identity Drift in Conversations of LLM Agents - OpenReview, дата последнего обращения: ноября 25, 2025, https://openreview.net/forum?id=hXkXv81Vmh
   31. How can chatbots perform long-term performance monitoring and model drift detection?, дата последнего обращения: ноября 25, 2025, https://www.tencentcloud.com/techpedia/127730
   32. FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance - Lingjiao Chen's, дата последнего обращения: ноября 25, 2025, http://lingjiaochen.com/papers/2024_FrugalGPT_TMLR.pdf
   33. FRUGALGPT: HOW TO USE LARGE LANGUAGE MOD- ELS WHILE REDUCING COST AND IMPROVING PER - OpenReview, дата последнего обращения: ноября 25, 2025, https://openreview.net/pdf?id=XUZ2S0JVJP
   34. RouteLLM: Learning to Route LLMs with Preference Data - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/pdf/2406.18665
   35. RouteLLM: Learning to Route LLMs with Preference Data - Weaviate, дата последнего обращения: ноября 25, 2025, https://weaviate.io/papers/routellm
   36. VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation - arXiv, дата последнего обращения: ноября 25, 2025, https://arxiv.org/html/2510.05156v1
   37. Adaptive Draft-Verification for Efficient Large Language Model Decoding, дата последнего обращения: ноября 25, 2025, https://ojs.aaai.org/index.php/AAAI/article/view/34647/36802
   38. Think-at-Hard: Adaptive Computation for LLMs - Emergent Mind, дата последнего обращения: ноября 25, 2025, https://www.emergentmind.com/topics/think-at-hard-tah
   39. Securely launch and scale your agents and tools on Amazon Bedrock AgentCore Runtime, дата последнего обращения: ноября 25, 2025, https://aws.amazon.com/blogs/machine-learning/securely-launch-and-scale-your-agents-and-tools-on-amazon-bedrock-agentcore-runtime/
   40. Embeddings and Vector Databases With ChromaDB - Real Python, дата последнего обращения: ноября 25, 2025, https://realpython.com/chromadb-vector-database/
   41. How to Build LangGraph Agents on the Fly with LLMs - Artificial Intelligence in Plain English, дата последнего обращения: ноября 25, 2025, https://ai.plainenglish.io/can-ai-agents-build-themselves-creating-workflows-on-the-fly-with-llms-f9d5807340bc
   42. Nodes, Edges, States & Graph in LangGraph — Basics | by Jayasree M | Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@official.hardcodeconcepts/nodes-edges-states-graph-in-langgraph-basics-3bdc7e9954b6
   43. How to route to a specific node based on tool return in LangGraph #5113 - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/langchain-ai/langgraph/discussions/5113
   44. Understanding Adaptive-RAG: Smarter, Faster, and More Efficient Retrieval-Augmented Generation | by Tuhin Sharma | Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@tuhinsharma121/understanding-adaptive-rag-smarter-faster-and-more-efficient-retrieval-augmented-generation-38490b6acf88
   45. Adaptive RAG explained: What to know in 2025 - Meilisearch, дата последнего обращения: ноября 25, 2025, https://www.meilisearch.com/blog/adaptive-rag
   46. Software Becomes Disposable? How AI Is Changing the Way We Architect Code - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/google-cloud/software-becomes-disposable-how-ai-is-changing-the-way-we-architect-code-1bfb50356b98
   47. The Disposable Software Revolution in the AI Era — The Future of Hybrid Development, дата последнего обращения: ноября 25, 2025, https://medium.com/@poola.vii/the-disposable-software-revolution-in-the-ai-era-the-future-of-hybrid-development-d62f673af90a