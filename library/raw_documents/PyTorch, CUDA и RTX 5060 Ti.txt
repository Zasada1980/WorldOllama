Исследовательский отчет: Состояние совместимости архитектуры NVIDIA Blackwell (sm_120) с экосистемой PyTorch и анализ альтернативных решений для RTX 5060 Ti




Введение


По состоянию на ноябрь 2025 года, ландшафт машинного обучения (ML) и искусственного интеллекта (AI) переживает значительную трансформацию, обусловленную выходом на рынок потребительских видеокарт серии NVIDIA GeForce RTX 50. В центре этого перехода находится архитектура Blackwell, представляющая собой следующее поколение вычислительных возможностей после архитектур Hopper и Ada Lovelace. Видеокарта RTX 5060 Ti, позиционируемая как решение среднего сегмента, стала доступна широкому кругу исследователей и разработчиков, однако ее внедрение сопровождается существенными программными барьерами.
Основная проблема, с которой сталкиваются пользователи, заключается в разрыве между аппаратными возможностями новой архитектуры (Compute Capability 12.0, или sm_120) и готовностью основного программного стека, в частности библиотеки PyTorch. Стандартные дистрибутивы PyTorch, доступные через менеджеры пакетов pip и conda, исторически ориентированы на стабильность и широкую совместимость с предыдущими поколениями оборудования, что создает временной лаг в поддержке новейших инструкций. Это приводит к критическим ошибкам при попытке запуска вычислений на RTX 5060 Ti, делая карту фактически неработоспособной для задач глубокого обучения в стандартных средах.
В данном отчете проводится исчерпывающий анализ текущей ситуации с поддержкой sm_120 в PyTorch, детально разбираются технические причины несовместимости (отсутствие бинарных ядер SASS), и предлагается развернутая стратегия действий. Особое внимание уделяется альтернативным инструментам и фреймворкам, таким как Unsloth, LLaMA-Factory и Kolo, которые позволяют обойти ограничения "ванильного" PyTorch и эффективно использовать вычислительную мощь RTX 5060 Ti уже сегодня. Отчет структурирован таким образом, чтобы предоставить как теоретическое понимание проблемы, так и практические руководства по ее решению.
________________


1. Архитектурный контекст: Blackwell и специфика sm_120




1.1 Эволюция вычислительных возможностей NVIDIA


Для понимания природы текущих проблем с PyTorch необходимо углубиться в классификацию вычислительных возможностей (Compute Capability), используемую NVIDIA. Compute Capability — это версионная спецификация аппаратных функций GPU, определяющая набор поддерживаемых инструкций, организацию памяти и возможности тензорных ядер.1
Архитектура Blackwell, лежащая в основе серии RTX 50, получила обозначение Compute Capability 12.0 (sm_120).2 Это знаменует собой фундаментальный скачок по сравнению с предыдущими поколениями:
* Ada Lovelace (RTX 40-series): sm_89.3
* Hopper (H100): sm_90.4
* Ampere (RTX 30-series): sm_86.3
Переход от версии 8.x и 9.0 к версии 12.0 подразумевает радикальные изменения в микроархитектуре. В частности, Blackwell внедряет новые форматы данных (например, нативная поддержка микросхемных форматов с плавающей запятой) и обновленные инструкции управления потоками, которые несовместимы с бинарным кодом, скомпилированным для старых архитектур.4 Видеокарта RTX 5060 Ti, являясь представителем этой новой волны, требует от программного обеспечения наличия специфических ядер, оптимизированных именно под sm_120.


1.2 Механизм компиляции CUDA: PTX против SASS


Корень проблемы несовместимости PyTorch с RTX 5060 Ti кроется в двухуровневой системе компиляции кода CUDA. Когда разработчики PyTorch создают дистрибутив (wheel), они компилируют код C++/CUDA в два типа файлов, которые упаковываются в библиотеку:
1. SASS (Streaming Assembler): Это машинный код, специфичный для конкретной архитектуры. Например, код SASS для sm_80 будет работать на картах Ampere, но физически не сможет исполниться на карте Blackwell, если архитектура претерпела изменения в наборе инструкций (ISA).5
2. PTX (Parallel Thread Execution): Это промежуточное представление, своего рода виртуальный ассемблер. Драйвер видеокарты может взять PTX-код и скомпилировать его "на лету" (JIT-компиляция) в SASS для конкретного установленного GPU. Это обеспечивает совместимость вперед.6
Проблема ноября 2025 года заключается в том, что "стабильные" релизы PyTorch (версии 2.4.x, 2.5.x и ранние 2.6) собираются с поддержкой SASS для старых архитектур (от sm_50 до sm_90) и часто не включают PTX-код версии, достаточной для поддержки Blackwell (Compute Capability 12.0).7 Без соответствующего SASS или совместимого PTX драйвер не может создать исполняемый код для RTX 5060 Ti, что приводит к фатальной ошибке при инициализации тензоров.
________________


2. Диагностика проблемы: PyTorch и RTX 5060 Ti в ноябре 2025 года




2.1 Симптоматика отказа (RuntimeError)


Пользователи, установившие стандартную версию PyTorch на систему с RTX 5060 Ti с помощью команды pip install torch, сталкиваются с немедленным сбоем при попытке перенести данные на GPU. Ошибка выглядит следующим образом:
RuntimeError: CUDA error: no kernel image is available for execution on the device. 7
Сопровождающее предупреждение часто уточняет список поддерживаемых архитектур текущей сборкой:
The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90. 7
Это сообщение недвусмысленно указывает на то, что бинарный файл библиотеки PyTorch не содержит инструкций для sm_120. Видеокарта RTX 5060 Ti, обладая ID архитектуры 12.0, не находит совместимого образа ядра для исполнения операций.2


2.2 Статус поддержки в стабильных релизах


На текущий момент (ноябрь 2025 года) официальная стабильная ветка PyTorch отстает от графика выпуска потребительского оборудования NVIDIA. Хотя поддержка Blackwell была анонсирована для будущих релизов (PyTorch 2.7 и выше), текущие стабильные пакеты (например, 2.5.1 или 2.6.0) собираются с использованием CUDA Toolkit версий 12.1 или 12.4.11
CUDA 12.1 и 12.4 вышли до финализации спецификаций потребительских карт Blackwell и не содержат компилятора nvcc, способного генерировать код sm_120. Следовательно, даже если бы разработчики PyTorch захотели включить поддержку в старые версии, инструментарий сборки этого не позволял. Полная поддержка sm_120 требует перехода на CUDA Toolkit 12.8 или новее, который стал доступен только недавно.5


2.3 Роль драйверов NVIDIA


Дополнительным слоем сложности является версия драйвера. Для работы с CUDA 12.8 и архитектурой sm_120 требуется драйвер серии R580 (например, 581.xx для Windows) или новее.10 Если пользователь обновил PyTorch до версии, поддерживающей Blackwell, но оставил старый драйвер, система не сможет инициализировать контекст CUDA. В отчетах пользователей отмечается, что утилита nvidia-smi может корректно отображать видеокарту и версию CUDA, но приложения всё равно не получают доступ к GPU из-за рассогласования версий драйвера и runtime-библиотек PyTorch.14
________________


3. Стратегия решения: Нативные методы восстановления совместимости


Если использование PyTorch является обязательным требованием и переход на альтернативные фреймворки невозможен, существует два основных пути решения проблемы: использование "ночных" сборок (Nightly Builds) или компиляция из исходного кода.


3.1 Путь А: Установка Nightly-сборок (Рекомендуемый метод)


Наиболее прагматичным решением для большинства исследователей является переход на канал обновлений "Preview" (Nightly). Разработчики PyTorch и инженеры NVIDIA активно интегрируют поддержку sm_120 именно в эту ветку разработки, опережая стабильные релизы на несколько месяцев.


Процедура установки


Ключевым требованием является выбор сборки, скомпилированной с CUDA 12.8 или новее. Сборки с CUDA 12.4 или 12.6, даже из ветки Nightly, не будут работать на RTX 5060 Ti.15
1. Удаление текущих версий:
Критических важно полностью очистить среду от старых пакетов, чтобы избежать конфликтов библиотек DLL (на Windows) или shared objects (на Linux).
Bash
pip uninstall torch torchvision torchaudio

2. Установка Nightly с CUDA 12.8:
Необходимо использовать специальный URL индекса пакетов.
Bash
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128

Эта команда загружает версии вида torch-2.8.0.dev20251120+cu128, которые содержат необходимые ядра SASS для sm_120.16


Особенности для разных ОС


   * Linux: Ситуация наиболее благоприятная. Сборки cu128 для Linux x86_64 регулярно тестируются и демонстрируют стабильную работу с Blackwell.18
   * Windows: Ситуация более сложная. Хотя колеса (wheels) для Windows существуют, пользователи часто сообщают о проблемах с зависимостями (например, torchvision может требовать старую версию torch) или о том, что даже с правильной версией возникают ошибки линковки.10 В связи с этим, для пользователей Windows настоятельно рекомендуется использование подсистемы WSL2 (см. раздел 4).


3.2 Путь Б: Компиляция из исходного кода (Для экспертов)


Если nightly-сборки недоступны или нестабильны, единственным способом получить нативный PyTorch является самостоятельная компиляция. Этот процесс позволяет вручную задать список поддерживаемых архитектур.


Требования к сборке


   * Компилятор: Требуется установленный CUDA Toolkit 12.8 (не просто runtime, а полный набор инструментов разработчика) и совместимый компилятор C++ (Visual Studio 2022 для Windows или GCC 11+ для Linux).
   * Переменные окружения: Критически важным флагом является TORCH_CUDA_ARCH_LIST.
Bash
export TORCH_CUDA_ARCH_LIST="12.0"

Этот флаг прямо указывает скрипту сборки PyTorch генерировать код для архитектуры Blackwell.19


Риски и сложности


Компиляция PyTorch — ресурсоемкий процесс, занимающий от 30 минут до нескольких часов. Исследования пользовательских отчетов показывают, что даже при успешной компиляции на Windows часто возникают проблемы взаимодействия драйвера и скомпилированных библиотек, приводящие к тому, что torch.cuda.is_available() возвращает False.14 В одном из описанных случаев пользователь успешно скомпилировал PyTorch с флагом 12.0 на Windows, но столкнулся с тем, что WSL-драйвер не мог корректно передать контекст устройству, несмотря на рабочий nvidia-smi.14 Это подчеркивает, что компиляция из исходников на новой архитектуре — это путь с высоким риском и неопределенным результатом.
________________


4. Виртуализация и контейнеризация как надежная альтернатива


Учитывая нестабильность нативных сборок PyTorch для Windows в ноябре 2025 года, наиболее надежной стратегией для владельцев RTX 5060 Ti является абстрагирование среды выполнения через контейнеризацию.


4.1 Windows Subsystem for Linux 2 (WSL2)


WSL2 является де-факто стандартом для ML-разработки на Windows. Она позволяет запускать полноценное ядро Linux параллельно с Windows, используя технологию GPU-Paravirtualization (GPU-PV).
Преимущества для RTX 5060 Ti:
      * Драйвер NVIDIA для Windows пробрасывает доступ к GPU внутрь Linux-среды.
      * Внутри WSL2 можно использовать Linux-версии PyTorch Nightly, которые, как отмечалось выше, значительно стабильнее и быстрее получают поддержку sm_120, чем их Windows-аналоги.11
      * Изоляция среды предотвращает конфликты с системными библиотеками Windows.


4.2 NVIDIA NGC Контейнеры


NVIDIA поддерживает собственный реестр контейнеров (NVIDIA GPU Cloud), где публикуются оптимизированные образы для глубокого обучения.
      * Образ: Рекомендуется использовать тег 25.10-py3 или новее.
      * Совместимость: Инженеры NVIDIA гарантируют, что в этих образах версии PyTorch, CUDA Toolkit (12.8+) и драйверов (внутри контейнера) согласованы и протестированы на архитектуре Blackwell.21
      * Механизм: Запуск Docker-контейнера из NGC снимает с пользователя необходимость подбирать версии библиотек. Если хост-драйвер обновлен, контейнер автоматически подхватывает sm_120 возможности.
________________


5. Альтернативные экосистемы и инструменты (без настройки PyTorch)


Отвечая на вторую часть запроса пользователя ("Найти варианты альтернатив"), важно отметить, что для решения прикладных задач (обучение, дообучение, инференс) не обязательно вручную настраивать "голый" PyTorch. Существует класс инструментов ("Meta-Frameworks"), которые инкапсулируют сложности настройки и предоставляют готовые решения для RTX 5060 Ti.


5.1 Kolo: "Все в одном" для локального LLM


Одним из наиболее перспективных решений конца 2025 года является инструмент Kolo. Это обертка над Docker, созданная специально для упрощения работы с локальными LLM на потребительском оборудовании.23
Архитектура решения:
Kolo объединяет в одном Docker-образе несколько ключевых компонентов:
      1. Unsloth: Для эффективного дообучения.
      2. Ollama: Для инференса и управления моделями.
      3. Open WebUI: Для визуального взаимодействия.
      4. Llama.cpp: Для квантования моделей.
Преимущества для RTX 5060 Ti:
Главная ценность Kolo заключается в преднастроенной совместимости. Скрипты сборки Kolo (build_image.ps1) автоматически подтягивают версии библиотек, совместимые с текущим оборудованием. Для пользователя процесс запуска сводится к выполнению скрипта create_and_run_container.ps1, который разворачивает полностью готовую среду, где PyTorch уже корректно работает с sm_120.23 Это избавляет от необходимости вручную решать проблему "no kernel image".


5.2 Unsloth: Оптимизация памяти для 5060 Ti


Видеокарта RTX 5060 Ti, как правило, оснащается 8 ГБ или 16 ГБ видеопамяти, что является "узким местом" для обучения современных языковых моделей (LLM). Стандартный PyTorch неэффективно расходует память, делая дообучение моделей класса Llama-3-8B практически невозможным на 8 ГБ VRAM без сильного квантования.
Unsloth — это библиотека, которая переписывает критические пути вычисления градиентов (backpropagation) вручную, минуя стандартные механизмы PyTorch autograd.
      * Экономия памяти: Unsloth снижает потребление VRAM на 30-40% и ускоряет обучение в 2 раза.25
      * Поддержка Blackwell: Разработчики Unsloth одними из первых внедрили поддержку sm_120. Для корректной работы требуется установить Triton версии 3.3.1+ и, при необходимости, указать TORCH_CUDA_ARCH_LIST="12.0" при установке зависимостей, таких как xformers.19
      * Интеграция: Unsloth является "движком" внутри Kolo, но может использоваться и отдельно в Google Colab или локальной среде Conda.


5.3 LLaMA-Factory: Визуальный подход к Fine-Tuning


Для пользователей, предпочитающих графический интерфейс (WebUI) командной строке, LLaMA-Factory предлагает мощную альтернативу.
      * Функциональность: Этот инструмент предоставляет веб-интерфейс для настройки всех параметров обучения (LoRA, QLoRA, full fine-tuning).
      * Экспорт: Одной из ключевых функций является возможность экспорта обученной модели (или адаптеров LoRA) напрямую в формат, совместимый с Ollama (GGUF), или слияние адаптеров с базовой моделью.26
      * Решение проблемы совместимости: Как и в случае с Kolo, использование LLaMA-Factory через Docker является предпочтительным способом запуска на RTX 5060 Ti. Официальный Docker-образ LLaMA-Factory базируется на актуальных версиях PyTorch/CUDA, поддерживающих новые GPU.28 Вкладка "Export" в интерфейсе позволяет без написания кода конвертировать результаты обучения для дальнейшего использования.29
________________


6. Инференс и развертывание моделей


Если задача пользователя ограничивается запуском уже обученных моделей (inference), прямая зависимость от PyTorch вообще отсутствует. Экосистема инференса развивалась параллельно и часто быстрее адаптируется к новому железу.


6.1 Ollama и llama.cpp


Ollama — это инструмент для запуска LLM, который под капотом использует библиотеку llama.cpp. Эта библиотека написана на чистом C/C++ и CUDA, не зависит от Python/PyTorch и имеет крайне легкий вес.
      * Поддержка 5060 Ti: Поддержка новых архитектур в llama.cpp появляется практически мгновенно после выхода драйверов. Для работы Ollama на RTX 5060 Ti достаточно иметь актуальный драйвер NVIDIA; сама библиотека определит GPU и будет использовать его вычислительные мощности.30
      * Производительность: Благодаря отсутствию оверхеда Python, Ollama обеспечивает максимальную производительность генерации токенов (tokens per second).


6.2 Open WebUI и DeepSeek R1


Open WebUI — это фронтенд, который подключается к API Ollama (или другим бэкендам) и предоставляет интерфейс, аналогичный ChatGPT.
      * Специфика конца 2025 года: В ноябре 2025 года особую популярность набрали модели DeepSeek-R1 с динамическим квантованием (например, 1.58-bit). Open WebUI и Ollama поддерживают эти экстремально сжатые форматы, что позволяет запускать огромные модели (которые раньше требовали 24+ ГБ VRAM) на картах среднего сегмента, таких как RTX 5060 Ti, с высокой скоростью.32
      * RAG (Retrieval Augmented Generation): Open WebUI имеет встроенную поддержку RAG, позволяя загружать документы и общаться с ними, используя локальные мощности 5060 Ti для эмбеддинга и генерации.34
________________


7. Сравнительный анализ производительности и рекомендации


В таблице ниже представлен сравнительный анализ доступных инструментов для владельца RTX 5060 Ti по состоянию на ноябрь 2025 года.
Инструмент
	Основная функция
	Совместимость с sm_120 (Blackwell)
	Сложность настройки
	Рекомендуемый сценарий использования
	PyTorch (Stable)
	Фреймворк DL
	Нет (Требует CUDA 12.1/12.4)
	Высокая (не работает)
	Не использовать до релиза 2.7+ Stable
	PyTorch (Nightly)
	Фреймворк DL
	Да (Только Linux/WSL2)
	Средняя (pip install --pre)
	Разработка новых архитектур нейросетей
	Kolo
	Обучение + Тесты
	Да (Через Docker)
	Низкая (Автоматизация)
	Быстрый старт, дообучение LLM "под ключ"
	Unsloth
	Эффективное обучение
	Да
	Средняя
	Fine-tuning на карте с ограниченной памятью (8/16 ГБ)
	LLaMA-Factory
	Обучение (GUI)
	Да (Через Docker)
	Низкая
	Визуальная настройка обучения, экспорт в Ollama
	Ollama
	Инференс
	Да (Нативная поддержка)
	Очень низкая
	Чат с моделями, использование готовых GGUF
	

Итоговые рекомендации


Для исследователя или инженера, обладающего видеокартой RTX 5060 Ti в ноябре 2025 года, попытка заставить работать стандартный pip install torch на Windows является тупиковым путем, ведущим к потере времени. Архитектура sm_120 требует обновления всего инструментария.
      1. Для задач обучения (Fine-Tuning):
Настоятельно рекомендуется использовать инструмент Kolo или Unsloth внутри контейнера Docker. Это решает сразу две проблемы: программную несовместимость (Docker предоставляет корректное окружение Linux/CUDA 12.8) и аппаратное ограничение памяти (Unsloth позволяет вместить процесс обучения в VRAM 5060 Ti).
      2. Для задач инференса (Chat/Generation):
Используйте связку Ollama + Open WebUI. Это обеспечит наилучшую производительность и поддержку современных квантованных моделей (DeepSeek R1) без необходимости борьбы с зависимостями Python.
      3. Для разработки на низком уровне:
Если необходимо писать кастомный код на PyTorch, единственным рабочим вариантом является установка WSL2 (Ubuntu 24.04) и использование PyTorch Nightly (cu128). Нативная разработка на Windows с использованием sm_120 пока остается экспериментальной и нестабильной зоной.


Прогноз развития


Ожидается, что ситуация стабилизируется в первом квартале 2026 года с выходом официального стабильного релиза PyTorch 2.7 (или 2.8), который сделает CUDA 12.8 стандартом по умолчанию. До этого момента использование Docker-контейнеров и специализированных инструментов вроде Kolo остается наиболее эффективной стратегией выживания для "ранних последователей" архитектуры Blackwell.
Источники
         1. CUDA GPU Compute Capability - NVIDIA Developer, дата последнего обращения: ноября 25, 2025, https://developer.nvidia.com/cuda-gpus
         2. RTX 5060 = sm_120 an unusual CUDA co | NVIDIA GeForce Forums, дата последнего обращения: ноября 25, 2025, https://www.nvidia.com/en-us/geforce/forums/geforce-graphics-cards/5/578477/rtx-5060-sm120-an-unusual-cuda-compute-capability/
         3. CUDA - Wikipedia, дата последнего обращения: ноября 25, 2025, https://en.wikipedia.org/wiki/CUDA
         4. nvidia gpu compute capability reference - mike bommarito, дата последнего обращения: ноября 25, 2025, https://michaelbommarito.com/wiki/programming/tools/gpu-compute-capability/
         5. 1. Blackwell Architecture Compatibility - NVIDIA Docs Hub, дата последнего обращения: ноября 25, 2025, https://docs.nvidia.com/cuda/blackwell-compatibility-guide/
         6. NVIDIA Blackwell and NVIDIA CUDA 12.9 Introduce Family-Specific Architecture Features, дата последнего обращения: ноября 25, 2025, https://developer.nvidia.com/blog/nvidia-blackwell-and-nvidia-cuda-12-9-introduce-family-specific-architecture-features/
         7. NVIDIA GeForce RTX 5060 Ti with CUDA capability sm_120 is not compatible with the current PyTorch installation. : r/StableDiffusion - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/StableDiffusion/comments/1l6yymn/nvidia_geforce_rtx_5060_ti_with_cuda_capability/
         8. RTX 5090 Training Issues - PyTorch Doesn't Support Blackwell Architecture Yet? - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1law1go/rtx_5090_training_issues_pytorch_doesnt_support/
         9. Add official support for CUDA sm_120 (RTX 5090 / Blackwell ..., дата последнего обращения: ноября 25, 2025, https://github.com/pytorch/pytorch/issues/159207
         10. NVIDIA GeForce RTX 5070 Ti with CUDA capability sm_120 - PyTorch Forums, дата последнего обращения: ноября 25, 2025, https://discuss.pytorch.org/t/nvidia-geforce-rtx-5070-ti-with-cuda-capability-sm-120/221509
         11. PyTorch support for sm_120: NVIDIA GeForce RTX 5060, дата последнего обращения: ноября 25, 2025, https://discuss.pytorch.org/t/pytorch-support-for-sm-120-nvidia-geforce-rtx-5060/220941
         12. Upgrading to Blackwell GPU: PyTorch Compatibility, CUDA Support, and Real-ESRGAN Benchmark | by Allen Kuo (kwyshell), дата последнего обращения: ноября 25, 2025, https://allenkuo.medium.com/upgrading-to-blackwell-gpu-pytorch-compatibility-cuda-support-and-real-esrgan-benchmark-0ebb363e4e9c
         13. PyTorch 2.7 Release, дата последнего обращения: ноября 25, 2025, https://pytorch.org/blog/pytorch-2-7/
         14. Blackwell GPU (RTX 5060 Ti) - CUDA Not Available to Applications in WSL Despite Working nvidia-smi, дата последнего обращения: ноября 25, 2025, https://forums.developer.nvidia.com/t/blackwell-gpu-rtx-5060-ti-cuda-not-available-to-applications-in-wsl-despite-working-nvidia-smi/344426
         15. Pytorch support for sm120 - deployment, дата последнего обращения: ноября 25, 2025, https://discuss.pytorch.org/t/pytorch-support-for-sm120/216099
         16. RTX 5090 not working with PyTorch and Stable Diffusion (sm_120 unsupported), дата последнего обращения: ноября 25, 2025, https://forums.developer.nvidia.com/t/rtx-5090-not-working-with-pytorch-and-stable-diffusion-sm-120-unsupported/338015
         17. RTX 5090 Help with CUDA error: "No kernel image is available for execution on the device" : r/StableDiffusion - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/StableDiffusion/comments/1jco4kw/rtx_5090_help_with_cuda_error_no_kernel_image_is/
         18. New AI SDKs and Tools Released for NVIDIA Blackwell GeForce RTX 50 Series GPUs, дата последнего обращения: ноября 25, 2025, https://developer.nvidia.com/blog/new-ai-sdks-and-tools-released-for-nvidia-blackwell-geforce-rtx-50-series-gpus/
         19. Fine-tuning LLMs with Blackwell, RTX 50 series & Unsloth, дата последнего обращения: ноября 25, 2025, https://docs.unsloth.ai/basics/fine-tuning-llms-with-blackwell-rtx-50-series-and-unsloth
         20. CUDA not available on WSL with custom-built PyTorch for Blackwell architecture (Compute Capability 12.0)" · Issue #162403 - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/pytorch/pytorch/issues/162403
         21. PyTorch - NGC Catalog - NVIDIA, дата последнего обращения: ноября 25, 2025, https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch
         22. PyTorch Release 25.09 - NVIDIA Docs Hub, дата последнего обращения: ноября 25, 2025, https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-25-09.html
         23. MaxHastings/Kolo: The Fastest Way to Fine-Tune LLMs ... - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/MaxHastings/Kolo
         24. I built a docker tool that has Unsloth + Ollama + OpenWebUI bundled together so you can begin fine tuning and testing in minutes not hours! - Reddit, дата последнего обращения: ноября 25, 2025, https://www.reddit.com/r/OpenWebUI/comments/1iggbx1/i_built_a_docker_tool_that_has_unsloth_ollama/
         25. Fine-Tuning Ollama Models with Unsloth | by Xiaojian Yu - Medium, дата последнего обращения: ноября 25, 2025, https://medium.com/@yuxiaojian/fine-tuning-ollama-models-with-unsloth-a504ff9e8002
         26. Mastering LLM Fine-Tuning: A Practical Guide with LLaMA-Factory and LoRA, дата последнего обращения: ноября 25, 2025, https://programmer.ie/post/fine_tuning/
         27. LLaMA-Factory - Qwen, дата последнего обращения: ноября 25, 2025, https://qwen.readthedocs.io/en/v2.0/training/SFT/llama_factory.html
         28. hiyouga/LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024) - GitHub, дата последнего обращения: ноября 25, 2025, https://github.com/hiyouga/LLaMA-Factory
         29. LLaMA Factory: A Feature-Rich Toolkit for Accessible LLM Customization - FPT Smart Cloud, дата последнего обращения: ноября 25, 2025, https://fptcloud.com/en/llama-factory-a-feature-rich-toolkit-for-accessible-llm-customization/
         30. Running Generative AI Models Locally with Ollama and Open WebUI - Fedora Magazine, дата последнего обращения: ноября 25, 2025, https://fedoramagazine.org/running-generative-ai-models-locally-with-ollama-and-open-webui/
         31. AI: Introduction to Ollama for local LLM launch | by Arseny Zinchenko (setevoy) - ITNEXT, дата последнего обращения: ноября 25, 2025, https://itnext.io/ai-introduction-to-ollama-for-local-llm-launch-a95e5200c3e7
         32. Starting with Llama.cpp - Open WebUI, дата последнего обращения: ноября 25, 2025, https://docs.openwebui.com/getting-started/quick-start/starting-with-llama-cpp/
         33. Run DeepSeek-R1 Dynamic 1.58-bit - Unsloth AI, дата последнего обращения: ноября 25, 2025, https://unsloth.ai/blog/deepseekr1-dynamic
         34. Key Features of Open WebUI, дата последнего обращения: ноября 25, 2025, https://open-webui.com/key-features-of-open-webui/