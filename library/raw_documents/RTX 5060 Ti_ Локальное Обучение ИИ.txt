Технический Отчёт: Локальное Обучение и Инференс ИИ на NVIDIA RTX 5060 Ti (16 GB VRAM) в 2025 году




1. Введение: Роль RTX 5060 Ti в Экосистеме Локального ИИ


NVIDIA GeForce RTX 5060 Ti с 16 GB видеопамяти (VRAM) позиционируется в 2025 году как критически важный компонент для локальных исследований и разработки (R&D) в области генеративного искусственного интеллекта. Стоимость карты, запущенной с рекомендованной розничной ценой $429 1, в сочетании с 16 GB VRAM, создает оптимальный баланс между доступностью, энергоэффективностью и вычислительной мощностью, необходимой для работы с крупными языковыми моделями (LLM).


1.1. Стратегическое Позиционирование 5060 Ti


Объем VRAM в 16 GB является стратегическим "водоразделом" для локального ИИ в 2025 году. Этот объем позволяет разработчику размещать и обучать параметрически-эффективными методами (LoRA/QLoRA) ключевые модели среднего размера (до 8-15 миллиардов параметров), такие как Llama 3.1, Gemma 2 или Phi-4.3 Для LLM-задач критически важно, что карта предлагает 16 GB VRAM, в отличие от карт с более высокой чистой вычислительной мощностью, но меньшим объемом памяти, например, RTX 4070 с 12 GB.4 Наличие достаточного объема VRAM является определяющим фактором для выбора модели, длины контекста и, следовательно, качества локально развернутых RAG-систем.
Ключевым технологическим улучшением архитектуры Blackwell (Blackwell sm_120) является внедрение памяти GDDR7. Этот тип памяти обеспечивает значительно более высокую пропускную способность (Bandwidth, BW), что для LLM-операций часто является более важным фактором, чем увеличение количества CUDA-ядер.2 Поскольку большинство LLM-задач, особенно инференс, являются memory-bound (ограниченными пропускной способностью памяти), высокий BW напрямую конвертируется в более высокую скорость генерации токенов.


2. Аппаратный Анализ: Архитектура Blackwell и Вычислительные Возможности


RTX 5060 Ti основана на архитектуре Blackwell (чип GB206) и демонстрирует значительный прирост производительности по сравнению со своим предшественником, RTX 4060 Ti, особенно в задачах, критичных к памяти и точности.


2.1. Ключевые Спецификации и Улучшения Tensor Cores


Видеокарта 5060 Ti оснащена 16 GB памяти GDDR7 со скоростью 28 Gbps.2 При 128-битной шине это обеспечивает общую пропускную способность памяти в 448 GB/s.2 Это увеличение на 56% по сравнению с 288 GB/s у RTX 4060 Ti 16GB 2 является наиболее важным аспектом для задач локального ИИ, поскольку оно позволяет более эффективно загружать большие веса моделей и обслуживать крупный KV Cache при длинном контексте.
Архитектура Blackwell в 5060 Ti включает 5-е поколение Tensor Cores, которые предлагают нативную поддержку числовых форматов FP4 и FP6, в дополнение к уже существующему FP8.2 Теоретическая пиковая производительность в формате FP4 достигает 759 TFLOPS.2 Этот уровень производительности указывает на высокую степень готовности 5060 Ti к будущим стандартам LLM-инференса. Ожидается, что FP4-квантизация, обеспечивающая максимальное сокращение потребления VRAM при минимальном снижении качества, станет общепринятой практикой, и 5060 Ti технологически готова к этому переходу.2


2.2. Сравнительный Анализ GPU для LLM-Задач


При выборе аппаратного обеспечения для локального ИИ VRAM и пропускная способность часто важнее, чем чистая вычислительная мощность (TFLOPS) в формате FP32.


Сравнение с RTX 4060 Ti (16 GB)


RTX 5060 Ti 16GB имеет явное техническое превосходство над RTX 4060 Ti 16GB.2 Прирост пропускной способности с 288 GB/s до 448 GB/s и более чем двукратное увеличение производительности Tensor Cores в низких точностях (759 TFLOPS FP4 против 353 TFLOPS FP8) делает 5060 Ti однозначно предпочтительной для любых рабочих нагрузок, связанных с LLM.2 Кроме того, 5060 Ti обеспечивает примерно 24% прирост производительности в общих AI-нагрузках по сравнению с 4060 Ti.2


Сравнение с RTX 4070 (12 GB)


Несмотря на то, что RTX 4070 может иметь больше CUDA-ядер и более высокую общую FP32 производительность (4070 Ti, как более мощная карта в этом сегменте, имеет около 7680 CUDA-ядер и 40.09 FP32 TFLOPS 5), для задач LLM объем VRAM является лимитирующим фактором.4 Дополнительные 4 GB VRAM у 5060 Ti позволяют загружать более крупные модели или, что критически важно, обеспечивать значительно более длинный контекст для RAG-систем.6 В сообществе разработчиков ИИ преобладает мнение, что "VRAM — король", поэтому 16 GB VRAM 5060 Ti предпочтительнее 12 GB VRAM 4070.4


Сравнение с A100 (Профессиональный сегмент)


Профессиональные GPU, такие как A100 (80 GB VRAM, 2039 GB/s BW, 1248 INT8 TFLOPS) 5, остаются стандартом для полномасштабного обучения (full fine-tuning) и крупного развертывания. Однако при использовании оптимизированных методов, таких как QLoRA, потребительские карты, такие как RTX 5060 Ti, могут обеспечить значительную долю этой производительности (например, RTX 5070 Ti обеспечивает 30–40% производительности A100 при дроби стоимости).7
Таблица 1: Сводная Таблица Технических Характеристик и AI-Производительности


Характеристика
	RTX 5060 Ti (16 GB)
	RTX 4060 Ti (16 GB)
	RTX 4070 (12 GB)
	A100 (80 GB)
	Архитектура
	Blackwell (sm_120)
	Ada Lovelace (sm_89)
	Ada Lovelace
	Ampere
	VRAM (GB)
	16 / GDDR7
	16 / GDDR6
	12 / GDDR6X
	80 / HBM2e
	Пропускная Способность (GB/s)
	448 2
	288 2
	~504 (4070 Ti)
	2039 5
	FP16 TFLOPS (Boost)
	~190 2
	~177 2
	N/A
	624
	INT8/FP4 TFLOPS (Tensor)
	759 (FP4) 2
	353 (FP8) 2
	400+ (FP8 est.)
	1248
	TBP (W)
	180 2
	165 2
	200
	300/400
	

2.3. Энергоэффективность и TBP


Total Board Power (TBP) RTX 5060 Ti составляет 180W 2, что лишь немного выше, чем у 4060 Ti (165W).2 Поддержание высокой вычислительной производительности при относительно низком TBP и тепловыделении делает эту карту идеальной для продолжительной локальной работы, не требующей специализированного охлаждения, и для развертывания в малых, энергоэффективных рабочих станциях. Эффективность охлаждения и низкое энергопотребление особенно важны для разработчиков, проводящих длительные циклы QLoRA-обучения.


3. Программный Стек и Оптимизация Памяти (VRAM)


Максимальное использование потенциала RTX 5060 Ti в 2025 году требует использования передовых оптимизаций и преодоления проблем совместимости, связанных с новизной архитектуры Blackwell (sm_120).


3.1. Базовые Требования и Проблемы Совместимости Blackwell


Для работы с архитектурой sm_120 необходимы последние версии CUDA 12.8+ и PyTorch 2.9+.8
Из-за того, что Blackwell является новой архитектурой, стандартные стабильные релизы программного обеспечения, такие как PyTorch и vLLM, в середине 2025 года могут не включать нативную поддержку sm_120. Это требует от инженера установки PyTorch Nightly Build и компиляции высокопроизводительных фреймворков, таких как vLLM, из исходников. В процессе сборки необходимо явно указывать архитектуру (TORCH_CUDA_ARCH_LIST="12.0") для успешной компиляции специфических ядер.8 Отсутствие такой ручной настройки приводит к ошибкам, связанным с отсутствием поддержки новой архитектуры, что является типичной проблемой раннего адоптера.4
Существует и технологическое ограничение: высокоэффективный механизм Flash Attention 3 (FA3) не поддерживается на Blackwell в текущих сборках vLLM, и приходится использовать предыдущую версию Flash Attention 2 (FA2).8


3.2. Стратегии VRAM-Оптимизации для Обучения: QLoRA — Обязательный Минимум


Полный Fine-tuning LLM с 16 GB VRAM невозможен. Например, для обучения 7B модели в половинной точности (FP16) с использованием 8-битного оптимизатора требуется около 70 GB VRAM.9 Следовательно, для RTX 5060 Ti единственным реалистичным методом обучения является QLoRA (Quantized Low-Rank Adaptation).10
* bitsandbytes и QLoRA: Библиотека bitsandbytes обеспечивает поддержку QLoRA (4-битная квантизация весов модели) и 8-битных оптимизаторов.11 Она полностью совместима с современными архитектурами NVIDIA (SM60+ / CUDA 11.8+).11 QLoRA сначала квантует базовую модель в 4-бит, что радикально сокращает фиксированные затраты VRAM, а затем обучает только небольшой набор адаптеров (LoRA weights).12
* 8-bit Optimizers: Использование оптимизаторов, таких как adamw_8bit, значительно сокращает объем памяти, необходимый для хранения состояний оптимизатора (optimizer states), что является вторым по величине потребителем VRAM после весов модели и градиентов.12
* Gradient Checkpointing: Эта техника обмена производительности на память (time-for-memory tradeoff) позволяет сократить использование VRAM для хранения активаций, что крайне важно для предотвращения ошибок Out-of-Memory (OOM) при работе с длинными последовательностями.12


3.3. Фреймворки Инференса и Их Оптимизации


Локальное развертывание и обслуживание (Serving) LLM на 5060 Ti может быть реализовано через несколько ключевых фреймворков, каждый со своими преимуществами в оптимизации 16 GB VRAM.
* vLLM: Предназначен для высокоскоростного инференса с высокой пропускной способностью (throughput). Ключевой оптимизацией является PagedAttention, который эффективно управляет KV Cache, предотвращая фрагментацию памяти.14 vLLM также использует torch.compile для ускорения "горячих" путей кода.14 Однако, как упоминалось ранее, для Blackwell требуется сборка из исходников.8
* TensorRT-LLM: Это решение от NVIDIA обеспечивает абсолютную пиковую производительность инференса на GPU NVIDIA за счет слияния графов CUDA (CUDA graph fusion) и максимальной оптимизации Tensor Cores, включая будущий потенциал FP4 Blackwell.15 Это оптимальный выбор для долгосрочного развертывания, где требуется максимальная скорость генерации токенов.
* Ollama / LM Studio: Эти инструменты используют формат GGUF (на базе llama.cpp), который обеспечивает самый низкий VRAM-след. Они идеальны для быстрого R&D, однопользовательского инференса и сервиса тонко настроенных LoRA-адаптеров.16 GGUF позволяет размещать веса модели, а при необходимости выполнять частичное offloading в системную RAM (хотя это снижает скорость).
Таблица 2: Сводная Таблица Фреймворков и Оптимизаций для RTX 5060 Ti


Фреймворк/Инструмент
	Основная Функция
	Ключевая Оптимизация VRAM
	Совместимость с Blackwell (sm_120) (2025)
	PyTorch 2.9+
	База для обучения LLM
	torch.compile, BF16/FP16
	Требуется Nightly Build для полной совместимости 8
	bitsandbytes
	Квантизация/Обучение
	QLoRA (4-bit), 8-bit Optimizers
	Полная (SM60+ / CUDA 11.8+) 11
	vLLM (0.10+)
	Высокоскоростной Инференс
	PagedAttention, Flash Attention 2 (FA2)
	Требует ручной сборки, FA3 не поддерживается 8
	Hugging Face PEFT/Axolotl
	LoRA/QLoRA Обучение
	Gradient Checkpointing, LORA target selection 12
	Полная совместимость через PyTorch
	Ollama (0.4+)
	Локальное Развертывание/Сервинг
	GGUF/GGML, Загрузка GGUF-адаптеров 17
	Полная совместимость, низкий порог входа
	

4. Реалистичные Сценарии Локального Обучения (Fine-Tuning/QLoRA)


RTX 5060 Ti с 16 GB VRAM позволяет инженерам проводить ценное и сложное тонкое обучение, но только в рамках параметрически-эффективных методов.


4.1. Максимальные Размеры Моделей для Обучения (QLoRA)


16 GB VRAM определяют границы возможных экспериментов.
* Оптимальный Сценарий: Идеальным выбором для QLoRA обучения являются модели с параметрами 7B–8B (например, Llama 3.1-8B 3 или Gemma 3-8B). Этот размер позволяет использовать комфортные настройки микро-батчей (micro_batch_size 2-4) и достаточную длину последовательности без немедленного исчерпания памяти.18 8B модели с длинным контекстом (Llama 3.1-8B поддерживает 128K 3) становятся мощными базовыми моделями для тонкой настройки.
* Граничный Сценарий: Модели с параметрами 13B–15B (например, Gemma 3-12B, Phi-4-14B) теоретически могут быть обучены с QLoRA, но это требует крайне агрессивной оптимизации. Необходимо снизить micro_batch_size до 1 и максимально увеличить gradient_accumulation_steps 12, а также ограничить длину последовательности (sequence_len), чтобы предотвратить ошибки OOM.12


4.2. Практическая Реализация: Обучение AGI-Агентов и RAG-Модулей


Возможности 5060 Ti по VRAM и пропускной способности идеально подходят для обучения специализированных агентов и RAG-модулей.
* Обучение Агентов: 8B модели, которые стабильно работают на 16 GB, обладают достаточной сложностью для тонкой настройки специализированных навыков, таких как управление инструментами (Tool Use), сложная логика планирования или специализированные языковые манеры.
* Приоритет Длины Контекста для RAG: Для систем, основанных на генерации с использованием извлеченных данных (RAG), возможность обслуживать большой контекст является ключевой. Объем VRAM, необходимый для RAG-инференса, складывается из весов модели и KV Cache, который растет линейно с длиной контекста.6 16 GB VRAM обеспечивают достаточный буфер, чтобы 8B QLoRA-модель могла эффективно использовать контекст длиной 128K токенов.3
Разработчикам рекомендуется использовать высокоуровневые библиотеки, такие как PyTorch torchtune или Axolotl 19, которые упрощают управление сложным циклом QLoRA-обучения на одиночных потребительских GPU.


4.3. Оптимизация Гиперпараметров (VRAM Management)


При QLoRA-обучении на 16 GB VRAM управление гиперпараметрами напрямую влияет на стабильность и скорость обучения.
1. Размер Батча (Batch Size): Снижение micro_batch_size — это самый эффективный и первый шаг для предотвращения ошибок CUDA Out-of-Memory.12
2. Накопление Градиентов (Gradient Accumulation): Этот параметр позволяет компенсировать снижение размера батча, симулируя больший эффективный размер батча без увеличения мгновенного потребления VRAM. Например, использование micro_batch_size: 2 с gradient_accumulation_steps: 4 дает эффективный батч 8.12
3. Выбор Целевых Модулей LoRA (Target Modules): Для максимальной экономии VRAM (за счет потенциального изменения качества адаптации) рекомендуется тренировать только критически важные слои трансформатора, такие как q_proj и v_proj.12


5. Инференс и Развертывание (GGUF, Ollama, LM Studio)


После тонкой настройки модели необходимо развернуть ее для инференса, где 16 GB VRAM раскрывают свой потенциал.


5.1. Доступные Форматы Моделей и Требования к VRAM для Инференса


Стандартным форматом для эффективного локального инференса в 2025 году является GGUF (GPT-Genominated Unified Format), который позволяет использовать высокоэффективную квантизацию, такую как Q4_K_M (приблизительно 0.57 байта на параметр).3
16 GB VRAM позволяют комфортно инференсировать модели до 14B–15B параметров в формате Q4_K_M. Это включает в себя запас памяти для операционной системы, служебных данных CUDA (фиксированная стоимость $\sim 0.55$ GB) и, что наиболее важно, для KV Cache.6
Крупные модели, такие как Qwen3-32B, которые в квантизации Q4_K_M требуют около 18.5 GB VRAM, уже не вписываются в 16 GB и требуют либо частичного offloading в системную RAM, либо использования мульти-GPU решений.3 5060 Ti является идеальным хостом для моделей, не требующих offloading.
Таблица 3: Максимальный Размер Моделей для Инференса (RTX 5060 Ti 16 GB VRAM, GGUF Q4_K_M)


Модель (LLM)
	Параметры (B)
	Квантизация
	Требуемая VRAM (GB) (Только Веса)
	Практическое Ограничение (16 GB)
	Llama 3.1-8B
	8
	Q4_K_M (GGUF)
	$\sim 5.0$ 3
	Идеально для 128K контекста
	Gemma 3-12B
	12
	Q4_K_M (GGUF)
	$\sim 7.5$ 16
	Комфортно для 64K контекста
	Phi-4
	14
	Q4_K_M (GGUF)
	$\sim 8.5$ 3
	Максимальный размер для стабильного инференса
	Qwen3-32B
	32
	Q4_K_M (GGUF)
	$\sim 18.5$ 3
	Требует CPU Offloading / Multi-GPU
	

5.2. Кейс Сообщества: Развертывание LoRA через Ollama


Сообщество локальных разработчиков активно использует Ollama для сервиса адаптированных моделей. Рабочий процесс включает обучение LoRA-адаптера с помощью PEFT/Axolotl, а затем его конвертацию в формат GGUF.17
Для этого используется скрипт convert_lora_to_gguf.py, входящий в проект llama.cpp. После конвертации адаптер (например, merchant-parser-adapter.gguf) можно легко интегрировать с базовой моделью (например, llama3.2:1b) с помощью простого Modelfile. Команда ADAPTER./merchant-parser-adapter.gguf внутри Modelfile позволяет Ollama комбинировать базовую квантованную модель с тонко настроенным адаптером, обеспечивая быстрый, легко масштабируемый и управляемый сервис на 5060 Ti.17


6. Практическая Секция: Мини-Гайд по Настройке Среды


Для достижения оптимальной производительности на Blackwell (sm_120) необходима тщательная подготовка программной среды.


6.1. Настройка WSL2/Docker для CUDA/Blackwell


Наиболее стабильной средой для разработки на NVIDIA GPU, включая Blackwell, является Linux (или WSL2/Docker на Windows), поскольку она обеспечивает наилучшую поддержку CUDA и инструментов компиляции. Разработчик должен убедиться, что установлены драйверы NVIDIA, поддерживающие CUDA 12.8.
Критически важный шаг — это установка предварительной сборки PyTorch, так как стандартные стабильные версии могут не содержать необходимых ядер для sm_120.8


6.2. Примеры Команд для Оптимальной Установки и Обучения


Для инженера, стремящегося к максимальной производительности, необходима ручная сборка vLLM, поскольку готовые колеса (wheels) не гарантируют поддержку новой архитектуры Blackwell.8
Таблица 4: Примеры Команд и Конфигураций для Оптимальной Настройки


Инструмент
	Задача
	Пример Команды/Конфигурации
	Цель
	PyTorch (Установка)
	Поддержка Blackwell (sm_120)
	pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128 8
	Установка bleeding-edge PyTorch для совместимости с sm_120
	vLLM (Сборка)
	Ускорение инференса (FA2)
	export TORCH_CUDA_ARCH_LIST="12.0"; MAX_JOBS=6 pip install --no-build-isolation -e. 8
	Компиляция с поддержкой Blackwell и отключение Flash Attention 3
	Axolotl/QLoRA (VRAM)
	Агрессивная экономия VRAM (Llama 3.1-8B)
	gradient_checkpointing: true; optimizer: adamw_8bit; load_in_4bit: true 12
	Включение всех ключевых VRAM-сберегающих техник
	Ollama (Сервинг LoRA)
	Развертывание адаптированной модели
	FROM llama3.2:1b; ADAPTER./my_lora_adapter.gguf; ollama create parser -f Modelfile 17
	Создание и локальный сервис модели с LoRA-адаптером
	

7. Выводы и Рекомендации




7.1. Потенциал и Ограничения


RTX 5060 Ti 16 GB VRAM является наиболее оптимальным потребительским GPU для разработчиков, работающих с LLM в 2025 году. Карта обеспечивает необходимый объем VRAM (16 GB) и критически важную пропускную способность (448 GB/s GDDR7) для стабильного QLoRA обучения моделей среднего размера (8B–15B) и высокоскоростного инференса. Поддержка FP4 в архитектуре Blackwell обеспечивает технологический задел для будущего.
Однако карта имеет два ключевых ограничения:
1. Невозможность обучения крупных моделей: 16 GB VRAM физически ограничивают обучение (даже QLoRA) моделей крупнее 15B, не говоря уже о полном Fine-Tuning.
2. Проблемы раннего адоптера: В середине 2025 года использование архитектуры Blackwell требует значительных инженерных усилий для настройки программной среды (PyTorch Nightly, ручная сборка vLLM), чтобы добиться максимальной производительности и совместимости.


7.2. Окончательные Рекомендации для Разработчика


1. Приоритет VRAM: При выборе GPU в этом ценовом диапазоне для LLM-задач всегда следует отдавать предпочтение 5060 Ti (16 GB) перед конкурирующими картами, предлагающими меньший объем VRAM, даже если их FP32 TFLOPS выше. 16 GB — это минимальный стандарт для серьезной локальной LLM-разработки.
2. Стратегия Обучения: Фокусироваться исключительно на QLoRA для обучения 7B–8B моделей, используя 8-битные оптимизаторы и Gradient Checkpointing для стабилизации процесса.
3. Развертывание Инференса: Для высокопроизводительного, многозадачного развертывания использовать vLLM (собранный из исходников). Для быстрого прототипирования, тестирования и сервиса LoRA-адаптеров использовать Ollama в связке с GGUF.
4. Будущие Направления: В долгосрочной перспективе необходимо отслеживать развитие фреймворков TensorRT-LLM (версии 0.5+), чтобы в полной мере использовать потенциал FP4 Tensor Cores Blackwell, который обеспечит дальнейшее сокращение VRAM и повышение скорости инференса.
Источники
1. GeForce RTX 5060 Ti 16GB - Price performance comparison - Video Card Benchmarks, дата последнего обращения: ноября 27, 2025, https://www.videocardbenchmark.net/gpu.php?gpu=GeForce+RTX+5060+Ti+16GB&id=6160
2. Nvidia GeForce RTX 5060 Ti 16GB vs RTX 4060 Ti 16GB: Blackwell ..., дата последнего обращения: ноября 27, 2025, https://www.tomshardware.com/pc-components/gpus/rtx-5060-ti-16gb-vs-rtx-4060-ti-16gb-gpu-faceoff
3. Top 10 Local LLMs (2025): Context Windows, VRAM Targets, and Licenses Compared, дата последнего обращения: ноября 27, 2025, https://www.marktechpost.com/2025/09/27/top-10-local-llms-2025-context-windows-vram-targets-and-licenses-compared/
4. Rtx 4070 vs rtx 5060ti 16gb for deep learning : r/nvidia - Reddit, дата последнего обращения: ноября 27, 2025, https://www.reddit.com/r/nvidia/comments/1m2rzkl/rtx_4070_vs_rtx_5060ti_16gb_for_deep_learning/
5. How come 4070 ti outperform 5060 ti in stable diffusion benchmarks by over 60% with only 12 GB VRAM. Is it because they are testing with a smaller model that could fit in a 12GB VRAM? : r/StableDiffusion - Reddit, дата последнего обращения: ноября 27, 2025, https://www.reddit.com/r/StableDiffusion/comments/1l85rxp/how_come_4070_ti_outperform_5060_ti_in_stable/
6. The Best GPUs for Local LLM Inference in 2025, дата последнего обращения: ноября 27, 2025, https://localllm.in/blog/best-gpus-llm-inference-2025
7. Analyzing the NVIDIA GeForce RTX 5070 Ti for AI Model Training: Performance Insights, дата последнего обращения: ноября 27, 2025, https://dev.to/berkan_baerbuilder_85/analyzing-the-nvidia-geforce-rtx-5070-ti-for-ai-model-training-performance-insights-4mb9
8. vLLM on RTX5090: Working GPU setup with torch 2.9.0 cu128 ..., дата последнего обращения: ноября 27, 2025, https://discuss.vllm.ai/t/vllm-on-rtx5090-working-gpu-setup-with-torch-2-9-0-cu128/1492
9. How much VRAM do I need for LLM model fine-tuning? | Modal Blog, дата последнего обращения: ноября 27, 2025, https://modal.com/blog/how-much-vram-need-fine-tuning
10. Top 5 AI Fine-Tuning Tools 2025: LoRA vs QLoRA vs Full - Index.dev, дата последнего обращения: ноября 27, 2025, https://www.index.dev/blog/top-ai-fine-tuning-tools-lora-vs-qlora-vs-full
11. bitsandbytes-foundation/bitsandbytes: Accessible large language models via k-bit quantization for PyTorch. - GitHub, дата последнего обращения: ноября 27, 2025, https://github.com/bitsandbytes-foundation/bitsandbytes
12. LoRA/QLoRA: The most significant training parameters that affect the VRAM (Axolotl) : r/LocalLLaMA - Reddit, дата последнего обращения: ноября 27, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1od0gw9/loraqlora_the_most_significant_training/
13. Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study - arXiv, дата последнего обращения: ноября 27, 2025, https://arxiv.org/html/2509.12229v1
14. PyTorch + vLLM = ♥️, дата последнего обращения: ноября 27, 2025, https://pytorch.org/blog/pytorch-vllm-%E2%99%A5%EF%B8%8F/
15. vLLM vs TensorRT-LLM: Key differences, performance, and how to run them - Northflank, дата последнего обращения: ноября 27, 2025, https://northflank.com/blog/vllm-vs-tensorrt-llm-and-how-to-run-them
16. ollama/ollama: Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models. - GitHub, дата последнего обращения: ноября 27, 2025, https://github.com/ollama/ollama
17. Fine Tuning LLM for Parsing and Serving Through Ollama | by Kaushik Holla - Towards AI, дата последнего обращения: ноября 27, 2025, https://pub.towardsai.net/fine-tuning-llm-for-parsing-and-serving-through-ollama-e224a8a5636a
18. Fine-tuning Llama-3–8B-Instruct QLORA using low cost resources | by Avishek Paul, дата последнего обращения: ноября 27, 2025, https://medium.com/@avishekpaul31/fine-tuning-llama-3-8b-instruct-qlora-using-low-cost-resources-89075e0dfa04
19. Fine-tuning | How-to guides - Llama, дата последнего обращения: ноября 27, 2025, https://www.llama.com/docs/how-to-guides/fine-tuning/
20. How to fine-tune a model using Axolotl | Runpod Blog, дата последнего обращения: ноября 27, 2025, https://www.runpod.io/blog/how-to-fine-tune-a-model-using-axolotl